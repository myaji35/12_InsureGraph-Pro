# ì„¸ì…˜ ìš”ì•½: ë¬¸ì„œ í¬ë¡¤ë§ ê¸°ëŠ¥ êµ¬í˜„ (2025-12-02)

## ğŸ“‹ ì„¸ì…˜ ëª©í‘œ

ì‚¬ìš©ì ìš”ì²­: "í¬ë¡¤ë§ ì„¤ì • ìš°ì¸¡ ì˜†ì— [ë¬¸ì„œ ì—…ë°ì´íŠ¸] ë²„íŠ¼ì„ ë‘ê³  í¬ë¡¤ë§ ì •ë³´ì— ë”°ë¼ ë¬¸ì„œ ëª©ë¡ì„ ì—…ë°ì´íŠ¸. Playwright + AIë¥¼ ì´ìš©í•œ HTML íŒŒì¼ ë¶„ì„ìœ¼ë¡œ ì•½ê´€/íŠ¹ì•½ì— ëŒ€í•œ PDF íŒŒì¼ ë§í¬ ëª©ë¡ ìˆ˜ì§‘"

## âœ… ì™„ë£Œëœ ì‘ì—…

### 1. í”„ë¡ íŠ¸ì—”ë“œ UI ìˆ˜ì •

**íŒŒì¼**: `frontend/src/components/dashboard/InsurerDetailView.tsx`

- "ë¬¸ì„œ ì—…ë°ì´íŠ¸" ë²„íŠ¼ ì¶”ê°€ (í¬ë¡¤ë§ ì„¤ì • ë²„íŠ¼ ì˜†)
- í¬ë¡¤ë§ ì¤‘ ìƒíƒœ í‘œì‹œ (`isCrawling` state)
- API í˜¸ì¶œ: `POST /api/v1/crawler/crawl-documents?insurer={ë³´í—˜ì‚¬ëª…}`
- ì„±ê³µ/ì‹¤íŒ¨ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ í‘œì‹œ

**ë³€ê²½ ì‚¬í•­**:
```typescript
// Import ì¶”ê°€
import { RefreshCw } from "lucide-react";
import { showSuccess, showError, showInfo } from "@/lib/toast-config";

// ë²„íŠ¼ ì¶”ê°€
<Button
  variant="default"
  size="sm"
  onClick={handleUpdateDocuments}
  disabled={isCrawling}
  className="bg-blue-600 hover:bg-blue-700"
>
  <RefreshCw className={`h-4 w-4 mr-1 ${isCrawling ? "animate-spin" : ""}`} />
  {isCrawling ? "í¬ë¡¤ë§ ì¤‘..." : "ë¬¸ì„œ ì—…ë°ì´íŠ¸"}
</Button>
```

### 2. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

**íŒŒì¼**: `backend/alembic/versions/005_add_crawler_documents_table.sql`

**í…Œì´ë¸”**: `crawler_documents`

| ì»¬ëŸ¼ëª… | íƒ€ì… | ì„¤ëª… |
|--------|------|------|
| id | UUID | ë¬¸ì„œ ê³ ìœ  ID (Primary Key) |
| insurer | VARCHAR(255) | ë³´í—˜ì‚¬ëª… |
| title | TEXT | ë¬¸ì„œ ì œëª© |
| pdf_url | TEXT | PDF íŒŒì¼ URL (UNIQUE) |
| category | VARCHAR(50) | ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ (ì•½ê´€/íŠ¹ì•½) |
| product_type | VARCHAR(100) | ìƒí’ˆ ìœ í˜• (ì¢…ì‹ ë³´í—˜, ì •ê¸°ë³´í—˜ ë“±) |
| source_url | TEXT | í¬ë¡¤ë§í•œ ì›ë³¸ í˜ì´ì§€ URL |
| status | VARCHAR(50) | ì²˜ë¦¬ ìƒíƒœ (pending, downloaded, processed, failed) |
| file_path | TEXT | ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ ê²½ë¡œ |
| error_message | TEXT | ì—ëŸ¬ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°) |
| metadata | JSONB | ì¶”ê°€ ë©”íƒ€ë°ì´í„° |
| created_at | TIMESTAMP | ìƒì„± ì‹œê°„ |
| updated_at | TIMESTAMP | ìˆ˜ì • ì‹œê°„ |

**ì¸ë±ìŠ¤**:
- `idx_crawler_documents_insurer` (insurer)
- `idx_crawler_documents_status` (status)
- `idx_crawler_documents_category` (category)
- `idx_crawler_documents_product_type` (product_type)
- `idx_crawler_documents_created_at` (created_at DESC)

**ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰**:
```bash
psql -h localhost -U gangseungsig -d insuregraph -f backend/alembic/versions/005_add_crawler_documents_table.sql
```
ìƒíƒœ: âœ… ì™„ë£Œ

### 3. AI ê¸°ë°˜ PDF ë§í¬ ì¶”ì¶œ ì„œë¹„ìŠ¤

**íŒŒì¼**: `backend/app/services/ai_pdf_extractor.py`

**í´ë˜ìŠ¤**: `AIPdfExtractor`

**ì£¼ìš” ê¸°ëŠ¥**:
1. **HTML íŒŒì‹±**: BeautifulSoup4ë¡œ HTMLì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œ
2. **PDF ë§í¬ í•„í„°ë§**: `.pdf` í¬í•¨ ë˜ëŠ” `download` í‚¤ì›Œë“œê°€ ìˆëŠ” ë§í¬ ì„ ë³„
3. **AI ë¶„ë¥˜**: OpenAI GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ë§í¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¥˜
   - ë¬¸ì„œ ì œëª© (í•œêµ­ì–´, ê°„ê²°)
   - ì¹´í…Œê³ ë¦¬ (ì•½ê´€/íŠ¹ì•½)
   - ìƒí’ˆ ìœ í˜• (ì¢…ì‹ ë³´í—˜, ì •ê¸°ë³´í—˜, ì—°ê¸ˆë³´í—˜, CIë³´í—˜, ê±´ê°•ë³´í—˜, ì €ì¶•ë³´í—˜ ë“±)
   - ê´€ë ¨ì„± íŒë‹¨ (ë³´í—˜ì•½ê´€/íŠ¹ì•½ ë¬¸ì„œì¸ì§€ ì—¬ë¶€)
4. **Fallback**: AI ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜

**ì˜ì¡´ì„±**:
- `beautifulsoup4` âœ… ì„¤ì¹˜ë¨
- `openai` âœ… ì„¤ì¹˜ë¨

**AI í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ**:
```
ë‹¹ì‹ ì€ {ë³´í—˜ì‚¬} ë³´í—˜ì‚¬ì˜ ì•½ê´€ ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ê° ë§í¬ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

{
  "documents": [
    {
      "index": ë§í¬ ë²ˆí˜¸,
      "title": "ë¬¸ì„œ ì œëª©",
      "category": "ì•½ê´€" ë˜ëŠ” "íŠ¹ì•½",
      "product_type": "ìƒí’ˆ ìœ í˜•",
      "is_relevant": true/false
    }
  ]
}
```

### 4. Playwright + AI í†µí•© í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤

**íŒŒì¼**: `backend/app/services/document_crawler_service.py`

**í´ë˜ìŠ¤**: `DocumentCrawlerService`

**ì£¼ìš” ë©”ì„œë“œ**:

1. **`crawl_insurer_documents(insurer, urls=None)`**
   - ë³´í—˜ì‚¬ì˜ ë¬¸ì„œë¥¼ í¬ë¡¤ë§
   - `crawler_urls` í…Œì´ë¸”ì—ì„œ í™œì„±í™”ëœ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
   - ê° URLì— ëŒ€í•´ Playwrightë¡œ í˜ì´ì§€ í¬ë¡¤ë§
   - AI PDF ì¶”ì¶œê¸°ë¡œ PDF ë§í¬ ìˆ˜ì§‘
   - ê²°ê³¼ ë°˜í™˜: `{total_urls, total_documents, documents}`

2. **`_get_crawler_urls(insurer)`**
   - DBì—ì„œ í•´ë‹¹ ë³´í—˜ì‚¬ì˜ í™œì„±í™”ëœ í¬ë¡¤ë§ URL ëª©ë¡ ì¡°íšŒ

3. **`save_crawled_documents(documents)`**
   - í¬ë¡¤ë§í•œ ë¬¸ì„œë¥¼ `crawler_documents` í…Œì´ë¸”ì— ì €ì¥
   - `ON CONFLICT (pdf_url) DO UPDATE` - ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸

**ì›Œí¬í”Œë¡œìš°**:
```
1. ì‚¬ìš©ìê°€ "ë¬¸ì„œ ì—…ë°ì´íŠ¸" ë²„íŠ¼ í´ë¦­
2. Frontend â†’ POST /api/v1/crawler/crawl-documents?insurer=ë©”íŠ¸ë¼ì´í”„ìƒëª…
3. Backend:
   a. DocumentCrawlerService ì´ˆê¸°í™”
   b. crawler_urls í…Œì´ë¸”ì—ì„œ URL ëª©ë¡ ì¡°íšŒ
   c. ê° URLì— ëŒ€í•´:
      - PlaywrightCrawlerë¡œ HTML ë‹¤ìš´ë¡œë“œ (3ì´ˆ ëŒ€ê¸°)
      - AIPdfExtractorë¡œ PDF ë§í¬ ì¶”ì¶œ ë° AI ë¶„ë¥˜
   d. crawler_documents í…Œì´ë¸”ì— ì €ì¥
4. Frontend: ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ (Nê°œ ë¬¸ì„œ ë°œê²¬)
```

### 5. API ì—”ë“œí¬ì¸íŠ¸

**íŒŒì¼**: `backend/app/api/v1/endpoints/crawler_documents.py`

**Router Prefix**: `/crawler`

**ì—”ë“œí¬ì¸íŠ¸**:

#### 1. POST `/crawler/crawl-documents`
- **ì„¤ëª…**: íŠ¹ì • ë³´í—˜ì‚¬ì˜ ë¬¸ì„œë¥¼ í¬ë¡¤ë§
- **Query Params**: `insurer` (í•„ìˆ˜) - ë³´í—˜ì‚¬ëª…
- **ì‘ë‹µ ëª¨ë¸**: `CrawlResultResponse`
  ```json
  {
    "message": "ë©”íŠ¸ë¼ì´í”„ìƒëª…ì˜ ë¬¸ì„œ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
    "total_urls": 1,
    "total_documents": 45,
    "saved_documents": 45,
    "documents": [...]
  }
  ```

#### 2. GET `/crawler/documents`
- **ì„¤ëª…**: í¬ë¡¤ë§í•œ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
- **Query Params**:
  - `insurer` (ì„ íƒ) - ë³´í—˜ì‚¬ëª…
  - `category` (ì„ íƒ) - ì¹´í…Œê³ ë¦¬ (ì•½ê´€/íŠ¹ì•½)
  - `status` (ì„ íƒ) - ìƒíƒœ (pending/downloaded/processed/failed)
  - `limit` (ê¸°ë³¸ 100) - ìµœëŒ€ ê²°ê³¼ ìˆ˜
  - `offset` (ê¸°ë³¸ 0) - ì˜¤í”„ì…‹
- **ì‘ë‹µ ëª¨ë¸**: `DocumentListResponse`
  ```json
  {
    "total": 100,
    "items": [
      {
        "id": "uuid",
        "insurer": "ë©”íŠ¸ë¼ì´í”„ìƒëª…",
        "title": "ë¬´ë°°ë‹¹ í•˜ì´ë¼ì´í”„ì¢…ì‹ ë³´í—˜ ì•½ê´€",
        "pdf_url": "https://...",
        "category": "ì•½ê´€",
        "product_type": "ì¢…ì‹ ë³´í—˜",
        "source_url": "https://...",
        "status": "pending",
        "created_at": "2025-12-02T...",
        "updated_at": "2025-12-02T..."
      }
    ]
  }
  ```

**Router ë“±ë¡**:
```python
# backend/app/api/v1/router.py
from app.api.v1.endpoints import crawler_documents
api_router.include_router(crawler_documents.router)
```

## âš ï¸ í˜„ì¬ ì´ìŠˆ

### ì„œë²„ ì‹œì‘ ì—ëŸ¬

**ë¬¸ì œ**: ì„œë²„ ì‹œì‘ ì‹œ import ì—ëŸ¬ ë°œìƒ

**ì—ëŸ¬ ë©”ì‹œì§€**:
```
File "/Users/gangseungsig/Documents/02_GitHub/12_InsureGraph Pro/backend/app/services/document_crawler_service.py", line 11, in <module>
```

**ì›ì¸ ì¶”ì •**:
1. `get_pg_connection()` í•¨ìˆ˜ í˜¸ì¶œ ë°©ì‹ì´ ì˜ëª»ë˜ì—ˆì„ ê°€ëŠ¥ì„±
2. AsyncPG connectionì„ ë™ê¸°ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë ¤ëŠ” ì‹œë„

**í•´ê²° ë°©ë²• (ë‹¤ìŒ ì„¸ì…˜)**:

1. **Option 1: Dependency Injection ì‚¬ìš©**
   ```python
   # crawler_documents.py
   from app.core.database import get_db
   from fastapi import Depends
   from sqlalchemy.ext.asyncio import AsyncSession

   @router.post("/crawl-documents")
   async def crawl_documents(
       insurer: str,
       db: AsyncSession = Depends(get_db)
   ):
       # db ì‚¬ìš©
   ```

2. **Option 2: AsyncPG Pool ì§ì ‘ ì‚¬ìš©**
   ```python
   # document_crawler_service.py
   from app.core.database import pg_pool

   async def _get_crawler_urls(self, insurer: str):
       async with pg_pool.acquire() as conn:
           rows = await conn.fetch(...)
   ```

3. **Option 3: ì „ì—­ connection ëŒ€ì‹  ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬**
   ```python
   async def crawl_insurer_documents(
       self,
       insurer: str,
       conn  # connectionì„ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê¸°
   ):
       # ...
   ```

## ğŸ“‚ ìƒì„±ëœ íŒŒì¼ ëª©ë¡

### Backend
1. `backend/alembic/versions/005_add_crawler_documents_table.sql` - ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
2. `backend/app/services/ai_pdf_extractor.py` - AI ê¸°ë°˜ PDF ë§í¬ ì¶”ì¶œ ì„œë¹„ìŠ¤
3. `backend/app/services/document_crawler_service.py` - Playwright + AI í†µí•© í¬ë¡¤ëŸ¬
4. `backend/app/api/v1/endpoints/crawler_documents.py` - API ì—”ë“œí¬ì¸íŠ¸

### Frontend
- `frontend/src/components/dashboard/InsurerDetailView.tsx` - ìˆ˜ì •ë¨ (ë¬¸ì„œ ì—…ë°ì´íŠ¸ ë²„íŠ¼ ì¶”ê°€)

## ğŸ”§ ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡

### Backend
1. `backend/app/api/v1/router.py` - crawler_documents router ì¶”ê°€
2. `backend/app/services/document_crawler_service.py` - `get_pg_pool` â†’ `get_pg_connection` ë³€ê²½ ì‹œë„

### Frontend
1. `frontend/src/components/dashboard/InsurerDetailView.tsx` - ì™„ì „ ìˆ˜ì •

## ğŸ¯ ë‹¤ìŒ ì„¸ì…˜ ì‘ì—…

### ìš°ì„ ìˆœìœ„ 1: ì„œë²„ ì—ëŸ¬ ìˆ˜ì •
1. database.pyì˜ connection ê´€ë¦¬ ë°©ì‹ í™•ì¸
2. DocumentCrawlerServiceì˜ DB ì ‘ê·¼ ë°©ì‹ ìˆ˜ì •
3. ì„œë²„ ì‹œì‘ í…ŒìŠ¤íŠ¸

### ìš°ì„ ìˆœìœ„ 2: í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
1. ë©”íŠ¸ë¼ì´í”„ìƒëª… í¬ë¡¤ë§ URL ë“±ë¡
   ```sql
   INSERT INTO crawler_urls (insurer, url, description, enabled) VALUES
   ('ë©”íŠ¸ë¼ì´í”„ìƒëª…', 'https://brand.metlife.co.kr/pn/mcvrgProd/retrieveMcvrgProdMain.do', 'ì•½ê´€ì •ë³´', true);
   ```
2. "ë¬¸ì„œ ì—…ë°ì´íŠ¸" ë²„íŠ¼ í´ë¦­ í…ŒìŠ¤íŠ¸
3. í¬ë¡¤ë§ ê²°ê³¼ í™•ì¸
4. `crawler_documents` í…Œì´ë¸” ë°ì´í„° í™•ì¸

### ìš°ì„ ìˆœìœ„ 3: í”„ë¡ íŠ¸ì—”ë“œ ë¬¸ì„œ ëª©ë¡ ì—°ë™
1. InsurerDetailViewì—ì„œ ì‹¤ì œ DB ë°ì´í„° ë¡œë“œ
2. í•™ìŠµ ì™„ë£Œ / ë¯¸í•™ìŠµ ë¬¸ì„œ ì†ŒíŒ…
3. ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§ (ì•½ê´€/íŠ¹ì•½)

### ìš°ì„ ìˆœìœ„ 4: PDF ë‹¤ìš´ë¡œë“œ ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸
1. í¬ë¡¤ë§ëœ PDF URLì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
2. ê¸°ì¡´ ingestion workflowì™€ ì—°ë™
3. ë¬¸ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸ (pending â†’ downloaded â†’ processed)

## ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€

ì´ë¯¸ ì„¤ì¹˜ë¨:
- âœ… `beautifulsoup4` - HTML íŒŒì‹±
- âœ… `openai` - AI PDF ë¶„ë¥˜
- âœ… `playwright` - ì›¹ í¬ë¡¤ë§ (chromium ë¸Œë¼ìš°ì € í¬í•¨)

## ğŸ’¡ ì°¸ê³ ì‚¬í•­

### í¬ë¡¤ë§ URL ì˜ˆì‹œ
```sql
-- ë©”íŠ¸ë¼ì´í”„ìƒëª…
INSERT INTO crawler_urls (insurer, url, description, enabled) VALUES
('ë©”íŠ¸ë¼ì´í”„ìƒëª…', 'https://brand.metlife.co.kr/pn/mcvrgProd/retrieveMcvrgProdMain.do', 'ì•½ê´€ì •ë³´', true);

-- ì‚¼ì„±ìƒëª…
INSERT INTO crawler_urls (insurer, url, description, enabled) VALUES
('ì‚¼ì„±ìƒëª…', 'https://www.samsunglife.com/customer/info/custerms/retrieveTermsList.do', 'ì•½ê´€ ëª©ë¡', true);
```

### í™˜ê²½ ë³€ìˆ˜ í•„ìš”
```env
# .env
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1  # ì„ íƒì‚¬í•­
```

### Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í™•ì¸
```bash
playwright install chromium
```

## ğŸ” ë””ë²„ê¹… íŒ

### ì„œë²„ ì—ëŸ¬ ìƒì„¸ í™•ì¸
```bash
cd backend
python -c "from app.services.document_crawler_service import DocumentCrawlerService"
```

### ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
```bash
psql -h localhost -U gangseungsig -d insuregraph
\d crawler_documents
SELECT * FROM crawler_documents;
```

### API í…ŒìŠ¤íŠ¸
```bash
# í¬ë¡¤ë§ ì‹œì‘
curl -X POST "http://localhost:3030/api/v1/crawler/crawl-documents?insurer=ë©”íŠ¸ë¼ì´í”„ìƒëª…"

# ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
curl "http://localhost:3030/api/v1/crawler/documents?insurer=ë©”íŠ¸ë¼ì´í”„ìƒëª…&limit=10"
```

## ğŸ“Š ì§„í–‰ ìƒí™© ìš”ì•½

| ì‘ì—… | ìƒíƒœ | ë¹„ê³  |
|------|------|------|
| í”„ë¡ íŠ¸ì—”ë“œ UI | âœ… ì™„ë£Œ | ë¬¸ì„œ ì—…ë°ì´íŠ¸ ë²„íŠ¼ ì¶”ê°€ |
| ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ | âœ… ì™„ë£Œ | crawler_documents í…Œì´ë¸” ìƒì„± |
| AI PDF ì¶”ì¶œ ì„œë¹„ìŠ¤ | âœ… ì™„ë£Œ | ai_pdf_extractor.py |
| Playwright í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ | âœ… ì™„ë£Œ | document_crawler_service.py |
| API ì—”ë“œí¬ì¸íŠ¸ | âœ… ì™„ë£Œ | crawler_documents.py |
| Router ë“±ë¡ | âœ… ì™„ë£Œ | router.py ìˆ˜ì • |
| ì„œë²„ ì‹œì‘ í…ŒìŠ¤íŠ¸ | âŒ ì‹¤íŒ¨ | Import ì—ëŸ¬ |
| í¬ë¡¤ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ | â³ ëŒ€ê¸° | ì„œë²„ ì—ëŸ¬ í•´ê²° í›„ |
| í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ì—°ë™ | â³ ëŒ€ê¸° | |
| ë¬¸ì„œ ì†ŒíŒ… ê¸°ëŠ¥ | â³ ëŒ€ê¸° | |

---

**ì„¸ì…˜ ì¢…ë£Œ ì‹œê°„**: 2025-12-02 10:38 (KST)
**ë‹¤ìŒ ì„¸ì…˜ ì‹œì‘ì **: ì„œë²„ import ì—ëŸ¬ ìˆ˜ì • (database connection ë°©ì‹ ë³€ê²½)
