"""
Hybrid LLM Entity Extractor - Targeted Processing for Specific Documents

ë©”íŠ¸ë¼ì´í”„ 1ê°œ + ì‚¼ì„±í™”ì¬ 1ê°œ ë¬¸ì„œë¥¼ LLMìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íƒ€ê²Ÿ ì›Œì»¤
ì˜ˆì‚°: $11 (ë¬¸ì„œë‹¹ ~$9, ì´ 2ê°œ)
"""
import sys
import os
import asyncio
from typing import List, Dict, Optional
from pathlib import Path

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import requests
import pdfplumber
from loguru import logger
from sqlalchemy import text as sql_text

from app.core.database import AsyncSessionLocal
from app.services.llm_entity_extractor import LLMEntityExtractor

# PDF ì €ì¥ ê²½ë¡œ
PDF_DIR = Path(__file__).parent / "data" / "pdfs_llm"
PDF_DIR.mkdir(parents=True, exist_ok=True)

# íƒ€ê²Ÿ ë¬¸ì„œ IDs
TARGET_DOCS = [
    "7e39d5e1-b652-4d2a-aacb-a437a10dba72",  # ë©”íŠ¸ë¼ì´í”„
    "2a92d88d-3279-4039-9d31-6796af9501f4"   # ì‚¼ì„±í™”ì¬
]


class HybridLLMExtractor:
    """íŠ¹ì • ë¬¸ì„œë§Œ LLM ì²˜ë¦¬í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œê¸°"""

    def __init__(self):
        """LLM ì¶”ì¶œê¸° ì´ˆê¸°í™”"""
        self.llm_extractor = LLMEntityExtractor()

    async def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        logger.info("=" * 80)
        logger.info("ğŸ¯ Hybrid LLM Entity Extractor - Targeted Processing")
        logger.info(f"ğŸ“‹ Target Documents: {len(TARGET_DOCS)}")
        logger.info(f"ğŸ’° Budget: ~$11 (2 documents Ã— $5-6 each)")
        logger.info("=" * 80)

        # 1. íƒ€ê²Ÿ ë¬¸ì„œ ì¡°íšŒ
        async with AsyncSessionLocal() as db:
            query_str = f"""
                SELECT id, title, insurer, category, product_type, pdf_url
                FROM crawler_documents
                WHERE id IN ('{TARGET_DOCS[0]}', '{TARGET_DOCS[1]}')
                ORDER BY insurer
            """
            result = await db.execute(sql_text(query_str))
            documents = result.fetchall()

        logger.info(f"ğŸ“„ Found {len(documents)} target documents")

        if len(documents) != 2:
            logger.error(f"âŒ Expected 2 documents, found {len(documents)}")
            return

        # 2. ê° ë¬¸ì„œ ì²˜ë¦¬
        total_entities = 0
        total_relationships = 0
        processed_count = 0
        failed_count = 0

        for doc in documents:
            doc_id, title, insurer, category, product_type, pdf_url = doc
            doc_id = str(doc_id)

            try:
                logger.info(f"\n{'=' * 80}")
                logger.info(f"ğŸ“„ Processing: {title[:70]}...")
                logger.info(f"   ë³´í—˜ì‚¬: {insurer}")
                logger.info(f"   ì¹´í…Œê³ ë¦¬: {category}")
                logger.info(f"   Document ID: {doc_id}")

                # Step 1: PDF ë‹¤ìš´ë¡œë“œ
                pdf_path = await self.download_pdf(doc_id, pdf_url, title)
                if not pdf_path:
                    logger.error(f"Failed to download PDF for {doc_id}")
                    failed_count += 1
                    continue

                # Step 2: í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = await self.extract_text_from_pdf(pdf_path)
                if not text or len(text) < 500:
                    logger.warning(f"Insufficient text extracted: {len(text)} chars")
                    failed_count += 1
                    continue

                logger.info(f"âœ… Extracted {len(text):,} characters")

                # Step 3: LLM ê¸°ë°˜ ì—”í‹°í‹° ë° ê´€ê³„ ì¶”ì¶œ (ì²­í¬ ë‹¨ìœ„)
                chunks = self._chunk_text(text, chunk_size=4000)
                logger.info(f"   ğŸ“„ Split into {len(chunks)} chunks for LLM processing")
                logger.info(f"   â±ï¸  Estimated time: {len(chunks) * 2} minutes")
                logger.info(f"   ğŸ’° Estimated cost: ${len(chunks) * 0.15:.2f}")

                entities, relationships = self.llm_extractor.extract_from_chunks(
                    chunks=chunks,
                    insurer=insurer,
                    product_type=product_type or "ì•½ê´€",
                    document_id=doc_id
                )

                # Step 4: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                saved_entities, saved_relationships = await self.save_entities(
                    doc_id, entities, relationships
                )

                total_entities += saved_entities
                total_relationships += saved_relationships
                processed_count += 1

                logger.info(f"âœ… Saved {saved_entities} entities, {saved_relationships} relationships")
                logger.info(f"   ğŸ“Š Relationship ratio: {saved_relationships / saved_entities if saved_entities else 0:.2f}")

            except Exception as e:
                logger.error(f"âŒ Error processing {doc_id}: {e}", exc_info=True)
                failed_count += 1

        # 3. ìµœì¢… í†µê³„
        logger.info(f"\n{'=' * 80}")
        logger.info(f"ğŸ“Š Final Statistics:")
        logger.info(f"   Total Documents: {len(documents)}")
        logger.info(f"   Processed: {processed_count}")
        logger.info(f"   Failed: {failed_count}")
        logger.info(f"   Total Entities: {total_entities}")
        logger.info(f"   Total Relationships: {total_relationships}")
        logger.info(f"   Overall Ratio: {total_relationships / total_entities if total_entities else 0:.2f}")
        logger.info(f"=" * 80)

        # 4. Neo4j ë¹„êµ í†µê³„ ì¡°íšŒ
        await self.show_comparison_stats()

    async def download_pdf(self, doc_id: str, pdf_url: str, title: str) -> Optional[Path]:
        """PDF ë‹¤ìš´ë¡œë“œ"""
        try:
            # íŒŒì¼ëª… ìƒì„±
            safe_filename = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in title[:50])
            pdf_filename = f"{doc_id}_{safe_filename}.pdf"
            pdf_path = PDF_DIR / pdf_filename

            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
            if pdf_path.exists():
                logger.info(f"   âœ… PDF already exists: {pdf_filename}")
                return pdf_path

            # PDF ë‹¤ìš´ë¡œë“œ
            logger.info(f"   â¬‡ï¸  Downloading PDF...")
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()

            # íŒŒì¼ ì €ì¥
            with open(pdf_path, 'wb') as f:
                f.write(response.content)

            logger.info(f"   âœ… Downloaded: {len(response.content):,} bytes")
            return pdf_path

        except Exception as e:
            logger.error(f"   âŒ Download failed: {e}")
            return None

    async def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text_parts = []

            with pdfplumber.open(pdf_path) as pdf:
                logger.info(f"   ğŸ“„ Extracting text from {len(pdf.pages)} pages...")
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)

                    if page_num % 10 == 0:
                        logger.info(f"      Processed {page_num}/{len(pdf.pages)} pages")

            full_text = "\n\n".join(text_parts)
            return full_text

        except Exception as e:
            logger.error(f"   âŒ Text extraction failed: {e}")
            return ""

    def _chunk_text(self, text: str, chunk_size: int = 4000) -> List[str]:
        """
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (LLM ì»¨í…ìŠ¤íŠ¸ ì œí•œ ëŒ€ì‘)
        """
        chunks = []
        current_pos = 0

        while current_pos < len(text):
            chunk = text[current_pos:current_pos + chunk_size]
            chunks.append(chunk)
            current_pos += chunk_size

        return chunks

    async def save_entities(
        self,
        doc_id: str,
        entities: List[Dict],
        relationships: List[Dict]
    ) -> tuple[int, int]:
        """ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            async with AsyncSessionLocal() as db:
                # ê¸°ì¡´ ì—”í‹°í‹° ì‚­ì œ
                await db.execute(
                    sql_text("DELETE FROM knowledge_entities WHERE document_id = :doc_id"),
                    {"doc_id": doc_id}
                )

                # ì—”í‹°í‹° ì €ì¥
                entity_count = 0
                for entity in entities:
                    await db.execute(sql_text("""
                        INSERT INTO knowledge_entities (
                            entity_id, label, type, description, source_text,
                            document_id, insurer, product_type, created_at
                        ) VALUES (
                            :entity_id, :label, :type, :description, :source_text,
                            :document_id, :insurer, :product_type, :created_at
                        )
                    """), entity)
                    entity_count += 1

                # ê´€ê³„ ì €ì¥
                relationship_count = 0
                for rel in relationships:
                    try:
                        await db.execute(sql_text("""
                            INSERT INTO knowledge_relationships (
                                source_entity_id, target_entity_id, type, description, created_at
                            ) VALUES (
                                :source_entity_id, :target_entity_id, :type, :description, :created_at
                            )
                        """), rel)
                        relationship_count += 1
                    except Exception:
                        pass  # ì¤‘ë³µ ê´€ê³„ëŠ” ë¬´ì‹œ

                await db.commit()

                return entity_count, relationship_count

        except Exception as e:
            logger.error(f"   âŒ Save failed: {e}", exc_info=True)
            return 0, 0

    async def show_comparison_stats(self):
        """ì²˜ë¦¬ ì „í›„ í†µê³„ ë¹„êµ"""
        try:
            async with AsyncSessionLocal() as db:
                # ì „ì²´ í†µê³„
                total_entities_query = "SELECT COUNT(*) FROM knowledge_entities"
                total_rels_query = "SELECT COUNT(*) FROM knowledge_relationships"

                result = await db.execute(sql_text(total_entities_query))
                total_entities = result.scalar()

                result = await db.execute(sql_text(total_rels_query))
                total_rels = result.scalar()

                # LLM ì²˜ë¦¬ ë¬¸ì„œì˜ í†µê³„
                llm_entities_query = f"""
                    SELECT COUNT(*) FROM knowledge_entities
                    WHERE document_id IN ('{TARGET_DOCS[0]}', '{TARGET_DOCS[1]}')
                """
                llm_rels_query = f"""
                    SELECT COUNT(*) FROM knowledge_relationships r
                    JOIN knowledge_entities e ON r.source_entity_id = e.entity_id
                    WHERE e.document_id IN ('{TARGET_DOCS[0]}', '{TARGET_DOCS[1]}')
                """

                result = await db.execute(sql_text(llm_entities_query))
                llm_entities = result.scalar()

                result = await db.execute(sql_text(llm_rels_query))
                llm_rels = result.scalar()

                logger.info(f"\n{'=' * 80}")
                logger.info(f"ğŸ“Š Database Comparison Statistics:")
                logger.info(f"")
                logger.info(f"   ì „ì²´ ê·¸ë˜í”„:")
                logger.info(f"   - Entities: {total_entities:,}")
                logger.info(f"   - Relationships: {total_rels:,}")
                logger.info(f"   - Ratio: {total_rels / total_entities if total_entities else 0:.2f}")
                logger.info(f"")
                logger.info(f"   LLM ì²˜ë¦¬ ë¬¸ì„œ (2ê°œ):")
                logger.info(f"   - Entities: {llm_entities:,}")
                logger.info(f"   - Relationships: {llm_rels:,}")
                logger.info(f"   - Ratio: {llm_rels / llm_entities if llm_entities else 0:.2f}")
                logger.info(f"")
                logger.info(f"   ğŸ¯ LLM ì²˜ë¦¬ íš¨ê³¼:")
                if llm_entities > 0:
                    overall_ratio = total_rels / total_entities if total_entities else 0
                    llm_ratio = llm_rels / llm_entities
                    improvement = ((llm_ratio - overall_ratio) / overall_ratio * 100) if overall_ratio > 0 else 0
                    logger.info(f"   - Relationship density improvement: {improvement:.1f}%")
                logger.info(f"=" * 80)

        except Exception as e:
            logger.error(f"Failed to show comparison stats: {e}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    extractor = HybridLLMExtractor()
    await extractor.run()


if __name__ == "__main__":
    asyncio.run(main())
