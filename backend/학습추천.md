# InsureGraph Pro - GraphRAG ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ êµ¬í˜„ ì œì•ˆ

## í˜„ì¬ ìƒí™© ë¶„ì„

### âœ… ì´ë¯¸ êµ¬í˜„ëœ ê²ƒ
- **RelationExtractor**: ê´€ê³„ ì¶”ì¶œ (COVERS, EXCLUDES, REQUIRES, REDUCES, LIMITS, REFERENCES)
- **EntityLinker**: ì§ˆë³‘ ì—”í‹°í‹° ì˜¨í†¨ë¡œì§€ ì—°ê²°, KCD ì½”ë“œ ë§¤í•‘
- **CriticalDataExtractor**: ê¸ˆì•¡, ê¸°ê°„, ì§ˆë³‘ì½”ë“œ ì¶”ì¶œ
- **PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ

### âŒ êµ¬í˜„ë˜ì§€ ì•Šì€ ê²ƒ
- ì›Œì»¤ê°€ ìœ„ ì»´í¬ë„ŒíŠ¸ë“¤ì„ **ì‹¤ì œë¡œ í˜¸ì¶œ**í•˜ëŠ” ë¶€ë¶„
- ì¶”ì¶œëœ ì—”í‹°í‹°/ê´€ê³„ë¥¼ Neo4jì— **ì €ì¥**í•˜ëŠ” ë¶€ë¶„
- ì‹¤ì œ ì§€ì‹ ê·¸ë˜í”„ **êµ¬ì¶•** ë¡œì§

**í˜„ì¬ ë¬¸ì œ**: `parallel_document_processor.py` Line 229-236ì—ì„œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œì´ placeholderë¡œë§Œ ì¡´ì¬
```python
# í˜„ì¬ ì½”ë“œ (ì‹¤ì œ ì‘ì—… ì—†ìŒ)
await update_progress("extracting_entities", 60)
await asyncio.sleep(1)  # ğŸ‘ˆ ë‹¨ìˆœ 1ì´ˆ ëŒ€ê¸°

await update_progress("extracting_relationships", 80)
await asyncio.sleep(1)  # ğŸ‘ˆ ë‹¨ìˆœ 1ì´ˆ ëŒ€ê¸°
```

---

## í•˜ë“œì›¨ì–´/ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­

### í•„ìš”í•œ ê²ƒ
âœ… **í˜„ì¬ Macì—ì„œ ì¶©ë¶„íˆ ê°€ëŠ¥** - ë³„ë„ ì„œë²„ ë¶ˆí•„ìš”

**ì†Œí”„íŠ¸ì›¨ì–´**:
- Neo4j (âœ… ì´ë¯¸ ì„¤ì¹˜ë¨)
- PostgreSQL (âœ… ì´ë¯¸ ì„¤ì¹˜ë¨)
- LLM API í‚¤:
  - Upstage Solar Pro API (1ì°¨ ì¶”ì¶œ) - ì €ë ´
  - OpenAI GPT-4o (2ì°¨ ê²€ì¦) - ì •í™•ë„ ë†’ìŒ

### ì˜ˆìƒ ë¹„ìš©
**58ê°œ ë¬¸ì„œ ì „ì²´ ì¬ì²˜ë¦¬ ê¸°ì¤€**:
- Upstage Solar Pro: ~$5-10
- OpenAI GPT-4o: ~$20-30
- **ì´ ì˜ˆìƒ ë¹„ìš©**: $30-40

**ì‹ ê·œ ë¬¸ì„œ ì²˜ë¦¬**:
- ë¬¸ì„œë‹¹: ~$0.50-0.70

---

## ê°œë°œ ì‘ì—… ìƒì„¸ ê³„íš

### Phase 1: ì—”í‹°í‹° ì¶”ì¶œ í†µí•© (2-3ì‹œê°„)

**íŒŒì¼**: `backend/app/services/parallel_document_processor.py`

**ì‘ì—… ë‚´ìš©**:
1. `CriticalDataExtractor` í˜¸ì¶œí•˜ì—¬ ê¸ˆì•¡, ê¸°ê°„, ì§ˆë³‘ì½”ë“œ ì¶”ì¶œ
2. `EntityLinker`ë¡œ ì§ˆë³‘ ì—”í‹°í‹°ë¥¼ í‘œì¤€ ì˜¨í†¨ë¡œì§€ì— ì—°ê²°
3. ì¶”ì¶œëœ ë°ì´í„°ë¥¼ `processing_detail` JSON í•„ë“œì— ì €ì¥

**ì½”ë“œ ì˜ˆì‹œ**:
```python
# Line 229 ìˆ˜ì •
await update_progress("extracting_entities", 60)

# Critical Data ì¶”ì¶œ
from app.services.critical_data_extractor import CriticalDataExtractor
critical_extractor = CriticalDataExtractor()
critical_data = critical_extractor.extract(extracted_text)

# Entity Linking
from app.services.entity_linker import EntityLinker
entity_linker = EntityLinker()
linked_entities = []
for disease_mention in critical_data.diseases:
    linked = entity_linker.link(disease_mention)
    linked_entities.append(linked)

# ê²°ê³¼ ì €ì¥
await update_progress("extracting_entities", 70, {
    "entities_found": len(linked_entities),
    "amounts": len(critical_data.amounts),
    "periods": len(critical_data.periods)
})
```

---

### Phase 2: ê´€ê³„ ì¶”ì¶œ í†µí•© (3-4ì‹œê°„)

**íŒŒì¼**: `backend/app/services/parallel_document_processor.py`

**ì‘ì—… ë‚´ìš©**:
1. í…ìŠ¤íŠ¸ë¥¼ clause(ì¡°í•­) ë‹¨ìœ„ë¡œ ë¶„í• 
2. `RelationExtractor`ë¡œ LLM ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ
3. Cascade ì „ëµ: Upstage Solar â†’ GPT-4o (ì‹ ë¢°ë„ < 0.7ì¸ ê²½ìš°)
4. Critical Dataë¡œ ê²€ì¦
5. ê²°ê³¼ë¥¼ `processing_detail`ì— ì €ì¥

**ì½”ë“œ ì˜ˆì‹œ**:
```python
# Line 232 ìˆ˜ì •
await update_progress("extracting_relationships", 80)

# í…ìŠ¤íŠ¸ë¥¼ ì¡°í•­ ë‹¨ìœ„ë¡œ ë¶„í• 
clauses = split_into_clauses(extracted_text)

# ê´€ê³„ ì¶”ì¶œ
from app.services.ingestion.relation_extractor import RelationExtractor
relation_extractor = RelationExtractor()

all_relations = []
for i, clause in enumerate(clauses):
    result = await relation_extractor.extract(
        clause_text=clause,
        critical_data=critical_data
    )
    all_relations.extend(result.relations)

    await update_progress("extracting_relationships", 80 + (i / len(clauses)) * 10)

# ê²°ê³¼ ì €ì¥
await update_progress("extracting_relationships", 90, {
    "relations_found": len(all_relations),
    "confidence_avg": sum(r.confidence for r in all_relations) / len(all_relations)
})
```

---

### Phase 3: Neo4j ì €ì¥ ë¡œì§ (2-3ì‹œê°„)

**íŒŒì¼**: `backend/app/services/graph_builder.py` (ì‹ ê·œ ìƒì„±)

**ì‘ì—… ë‚´ìš©**:
1. ì¶”ì¶œëœ ì—”í‹°í‹°ë¥¼ Neo4j Nodeë¡œ ì €ì¥
   - `Disease`, `Coverage`, `Condition`, `Amount`, `Period` ë…¸ë“œ
2. ì¶”ì¶œëœ ê´€ê³„ë¥¼ Neo4j Relationshipìœ¼ë¡œ ì €ì¥
   - `COVERS`, `EXCLUDES`, `REQUIRES`, `REDUCES`, `LIMITS`
3. ë©”íƒ€ë°ì´í„° ì—°ê²° (ë¬¸ì„œ, ë³´í—˜ì‚¬, ìƒí’ˆíƒ€ì…)

**Neo4j ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ**:
```cypher
// ë…¸ë“œ íƒ€ì…
(:Insurance {name, type})
(:Product {name, code})
(:Disease {name, kcd_code, category})
(:Coverage {name, amount})
(:Condition {type, value})

// ê´€ê³„ íƒ€ì…
(:Product)-[:COVERS]->(:Disease)
(:Product)-[:EXCLUDES]->(:Disease)
(:Coverage)-[:REQUIRES]->(:Condition)
(:Coverage)-[:REDUCES {ratio}]->(:Amount)
```

**ì½”ë“œ ì˜ˆì‹œ**:
```python
from neo4j import GraphDatabase

class GraphBuilder:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    async def build_from_extraction(self, document_id, entities, relations):
        with self.driver.session() as session:
            # 1. Document ë…¸ë“œ ìƒì„±
            session.run("""
                MERGE (d:Document {id: $doc_id})
                SET d.processed_at = datetime()
            """, doc_id=document_id)

            # 2. Entity ë…¸ë“œ ìƒì„±
            for entity in entities:
                if entity.type == "disease":
                    session.run("""
                        MERGE (e:Disease {name: $name})
                        SET e.kcd_code = $kcd_code,
                            e.category = $category
                        MERGE (d:Document {id: $doc_id})
                        MERGE (d)-[:MENTIONS]->(e)
                    """, name=entity.name, kcd_code=entity.kcd_code,
                         category=entity.category, doc_id=document_id)

            # 3. Relation ìƒì„±
            for relation in relations:
                session.run(f"""
                    MATCH (s) WHERE s.name = $subject
                    MATCH (o) WHERE o.name = $object
                    MERGE (s)-[r:{relation.action}]->(o)
                    SET r.confidence = $confidence,
                        r.document_id = $doc_id
                """, subject=relation.subject, object=relation.object,
                     confidence=relation.confidence, doc_id=document_id)
```

---

### Phase 4: Graph Updater ìˆ˜ì • (1-2ì‹œê°„)

**íŒŒì¼**: `backend/worker_graph_updater.py`

**ì‘ì—… ë‚´ìš©**:
1. ê¸°ì¡´ ë‹¨ìˆœ ê³„ì¸µ êµ¬ì¡° (ë³´í—˜ì‚¬â†’ìƒí’ˆíƒ€ì…â†’ë¬¸ì„œ) ìœ ì§€
2. **ì¶”ê°€**: ì—”í‹°í‹°/ê´€ê³„ ê¸°ë°˜ ê·¸ë˜í”„ë„ í•¨ê»˜ í‘œì‹œ
3. ë‘ ê°€ì§€ ë·° ëª¨ë“œ ì§€ì›:
   - **Document View**: í˜„ì¬ ê³„ì¸µ êµ¬ì¡°
   - **Entity View**: ì§ˆë³‘/ë³´ì¥/ì¡°ê±´ ì¤‘ì‹¬ ê·¸ë˜í”„

**ìˆ˜ì • í¬ì¸íŠ¸**:
```python
# generate_graph() ë©”ì„œë“œ í™•ì¥
async def generate_graph(self):
    # ê¸°ì¡´ ë¡œì§ ìœ ì§€
    nodes, edges = await self._generate_document_hierarchy()

    # ì‹ ê·œ ì¶”ê°€: ì—”í‹°í‹° ê·¸ë˜í”„
    entity_nodes, entity_edges = await self._generate_entity_graph()

    # ë‘ ê·¸ë˜í”„ ë³‘í•© ë˜ëŠ” ë³„ë„ ì €ì¥
    graph_data = {
        "document_view": {"nodes": nodes, "edges": edges},
        "entity_view": {"nodes": entity_nodes, "edges": entity_edges}
    }
```

---

### Phase 5: í…ŒìŠ¤íŠ¸ & ë””ë²„ê¹… (2-3ì‹œê°„)

**í…ŒìŠ¤íŠ¸ í•­ëª©**:
1. **ìƒ˜í”Œ ë¬¸ì„œ E2E í…ŒìŠ¤íŠ¸**
   - 1-2ê°œ ë¬¸ì„œë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
   - LLM ì‘ë‹µ í’ˆì§ˆ í™•ì¸
   - Neo4j ì €ì¥ í™•ì¸

2. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**
   - ë¬¸ì„œë‹¹ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
   - ë³‘ë ¬ ì²˜ë¦¬ ë¶€í•˜ í…ŒìŠ¤íŠ¸
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

3. **ì—ëŸ¬ í•¸ë“¤ë§**
   - LLM API íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
   - Neo4j ì—°ê²° ì‹¤íŒ¨ ë³µêµ¬
   - ì˜ëª»ëœ ë°ì´í„° í¬ë§· ì²˜ë¦¬

4. **ë¹„ìš© ìµœì í™”**
   - ë¶ˆí•„ìš”í•œ LLM í˜¸ì¶œ ì œê±°
   - ìºì‹± ì „ëµ ì ìš©
   - Batch ì²˜ë¦¬ ìµœì í™”

---

## ì´ ì˜ˆìƒ ì‹œê°„

| Phase | ì‘ì—… ë‚´ìš© | ì˜ˆìƒ ì‹œê°„ |
|-------|----------|----------|
| Phase 1 | ì—”í‹°í‹° ì¶”ì¶œ í†µí•© | 2-3ì‹œê°„ |
| Phase 2 | ê´€ê³„ ì¶”ì¶œ í†µí•© | 3-4ì‹œê°„ |
| Phase 3 | Neo4j ì €ì¥ ë¡œì§ | 2-3ì‹œê°„ |
| Phase 4 | Graph Updater ìˆ˜ì • | 1-2ì‹œê°„ |
| Phase 5 | í…ŒìŠ¤íŠ¸ & ë””ë²„ê¹… | 2-3ì‹œê°„ |
| **í•©ê³„** | | **10-15ì‹œê°„** |

**ì¼ì • ë¶„ì‚°**:
- **Day 1** (4-5ì‹œê°„): Phase 1-2 (ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ)
- **Day 2** (3-4ì‹œê°„): Phase 3 (Neo4j ì €ì¥)
- **Day 3** (3-4ì‹œê°„): Phase 4-5 (í†µí•© & í…ŒìŠ¤íŠ¸)

---

## ì‹¤í–‰ ì˜µì…˜

### Option A: ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (ì¶”ì²œ ğŸŒŸ)
**ëª©ì **: í”„ë¡œí† íƒ€ì… ê²€ì¦ í›„ ì „ì²´ ì ìš© ê²°ì •

**ì§„í–‰ ë°©ì‹**:
1. 1-2ê°œ ë¬¸ì„œë§Œ ì„ íƒí•˜ì—¬ í…ŒìŠ¤íŠ¸
2. ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ í’ˆì§ˆ í™•ì¸
3. Neo4j ê·¸ë˜í”„ ì‹œê°í™” í™•ì¸
4. ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë©´ ì „ì²´ ì ìš©

**ì‹œê°„**: 2-3ì‹œê°„
**ë¹„ìš©**: < $2
**ë¦¬ìŠ¤í¬**: ë‚®ìŒ âœ…

---

### Option B: ì‹ ê·œ ë¬¸ì„œë¶€í„° ì ìš©
**ëª©ì **: ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•˜ë©´ì„œ ì ì§„ì  í™•ì¥

**ì§„í–‰ ë°©ì‹**:
1. Phase 1-5 ì „ì²´ êµ¬í˜„
2. ê¸°ì¡´ 58ê°œ ë¬¸ì„œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
3. ìƒˆë¡œ í¬ë¡¤ë§ë˜ëŠ” ë¬¸ì„œë¶€í„° ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ì ìš©
4. ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì§€ì‹ ê·¸ë˜í”„ í™•ì¥

**ì‹œê°„**: 10-15ì‹œê°„
**ë¹„ìš©**: ì‹ ê·œ ë¬¸ì„œë‹¹ $0.50-0.70
**ë¦¬ìŠ¤í¬**: ì¤‘ê°„ âš ï¸

---

### Option C: ì „ì²´ ì¬ì²˜ë¦¬
**ëª©ì **: ì™„ì „í•œ ì§€ì‹ ê·¸ë˜í”„ ì¦‰ì‹œ êµ¬ì¶•

**ì§„í–‰ ë°©ì‹**:
1. Phase 1-5 ì „ì²´ êµ¬í˜„
2. ê¸°ì¡´ 58ê°œ ë¬¸ì„œë¥¼ "completed" â†’ "pending"ìœ¼ë¡œ ë³€ê²½
3. ì›Œì»¤ê°€ ì „ì²´ ì¬ì²˜ë¦¬í•˜ì—¬ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ
4. ì™„ì „í•œ ì§€ì‹ ê·¸ë˜í”„ ì™„ì„±

**ì‹œê°„**: 10-15ì‹œê°„ (êµ¬í˜„) + 3-4ì‹œê°„ (ì¬ì²˜ë¦¬)
**ë¹„ìš©**: $30-40
**ë¦¬ìŠ¤í¬**: ë†’ìŒ ğŸ”´
**ì£¼ì˜**: API í‚¤ í•œë„, ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„± í•„ìš”

---

## ì‘ì—… ë‚œì´ë„ ë¶„ì„

### ì‰¬ìš´ ë¶€ë¶„ âœ…
- ì½”ë“œ êµ¬ì¡°ê°€ ì´ë¯¸ ì˜ ì„¤ê³„ë˜ì–´ ìˆìŒ
- `RelationExtractor`, `EntityLinker` ë“± í•µì‹¬ ë¡œì§ì´ ì´ë¯¸ ì™„ì„±ë¨
- ë‹¨ìˆœíˆ "ì—°ê²°"ë§Œ í•˜ë©´ ë¨
- Neo4j, PostgreSQL ì¸í”„ë¼ ì¤€ë¹„ ì™„ë£Œ

### ì–´ë ¤ìš´ ë¶€ë¶„ âš ï¸
- **LLM í”„ë¡¬í”„íŠ¸ íŠœë‹**: ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™”
- **ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬**: ì—ëŸ¬ í•¸ë“¤ë§, ì¬ì‹œë„ ë¡œì§, ì§„í–‰ ìƒí™© ì¶”ì 
- **Neo4j ìŠ¤í‚¤ë§ˆ ì„¤ê³„**: í™•ì¥ ê°€ëŠ¥í•˜ë©´ì„œë„ ì¿¼ë¦¬ íš¨ìœ¨ì ì¸ êµ¬ì¡°
- **ë¹„ìš© ìµœì í™”**: ë¶ˆí•„ìš”í•œ LLM í˜¸ì¶œ ìµœì†Œí™”
- **í’ˆì§ˆ ê²€ì¦**: LLM ì¶”ì¶œ ê²°ê³¼ì˜ ì •í™•ì„± ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜

---

## ì‚¬ì „ ì¤€ë¹„ì‚¬í•­

### 1. LLM API í‚¤ í™•ì¸
```bash
# .env íŒŒì¼ í™•ì¸
UPSTAGE_API_KEY=sk-xxx...
OPENAI_API_KEY=sk-xxx...
```

### 2. API ì‚¬ìš©ëŸ‰ í•œë„ í™•ì¸
- Upstage: [https://console.upstage.ai/api-keys](https://console.upstage.ai/api-keys)
- OpenAI: [https://platform.openai.com/usage](https://platform.openai.com/usage)

### 3. Neo4j ìƒíƒœ í™•ì¸
```bash
# Neo4j ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
cypher-shell -u neo4j -p "change-me-in-production" "MATCH (n) RETURN count(n)"
```

### 4. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ ì •
```sql
-- ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ 1-2ê°œ ì„ íƒ
SELECT id, insurer, product_type, title
FROM crawler_documents
WHERE status = 'completed'
LIMIT 2;
```

---

## ì˜ˆìƒ ê²°ê³¼ë¬¼

### êµ¬í˜„ ì™„ë£Œ í›„ ì–»ê²Œ ë˜ëŠ” ê²ƒ

**1. í’ë¶€í•œ ì§€ì‹ ê·¸ë˜í”„**:
```
ë³´í—˜ìƒí’ˆ -> COVERS -> ì§ˆë³‘
ë³´í—˜ìƒí’ˆ -> EXCLUDES -> íŠ¹ì • ì§ˆë³‘
ë³´ì¥ë‚´ìš© -> REQUIRES -> ì¡°ê±´ (ì˜ˆ: 3ê°œì›” ë©´ì±…ê¸°ê°„)
ë³´ì¥ë‚´ìš© -> REDUCES -> ê¸ˆì•¡ (ì˜ˆ: 50% ê°ì•¡)
```

**2. ìƒì„¸í•œ ì—”í‹°í‹° ì •ë³´**:
- ì§ˆë³‘ëª… + KCD ì½”ë“œ ë§¤í•‘
- ê¸ˆì•¡ ì •ë³´ (ë³´ì¥ê¸ˆì•¡, í•œë„ ë“±)
- ê¸°ê°„ ì •ë³´ (ë©´ì±…ê¸°ê°„, ë³´í—˜ê¸°ê°„ ë“±)

**3. ê³ ë„í™”ëœ ê²€ìƒ‰**:
- "íŠ¹ì • ì§ˆë³‘ì„ ë³´ì¥í•˜ëŠ” ëª¨ë“  ìƒí’ˆ ì°¾ê¸°"
- "ë©´ì±…ê¸°ê°„ ì—†ëŠ” ì•”ë³´í—˜ ì°¾ê¸°"
- "ë³´ì¥ê¸ˆì•¡ 5ì²œë§Œì› ì´ìƒ ìƒí’ˆ ë¹„êµ"

**4. ì‹œê°í™” ê°œì„ **:
- í˜„ì¬: ë³´í—˜ì‚¬ â†’ ìƒí’ˆíƒ€ì… â†’ ë¬¸ì„œ (ë‹¨ìˆœ ê³„ì¸µ)
- ê°œì„  í›„: ì§ˆë³‘ â†” ë³´ì¥ â†” ì¡°ê±´ (ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬)

---

## ë¦¬ìŠ¤í¬ & ëŒ€ì‘ ë°©ì•ˆ

### ë¦¬ìŠ¤í¬ 1: LLM ì¶”ì¶œ ì •í™•ë„ ë¶€ì¡±
**ì¦ìƒ**: ì˜ëª»ëœ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ
**ëŒ€ì‘**:
- Cascade ì „ëµ í™œìš© (Upstage â†’ GPT-4o)
- Critical Dataë¡œ ê²€ì¦ ê°•í™”
- ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì • (< 0.7ì¸ ê²½ìš° ì¬ì‹œë„)

### ë¦¬ìŠ¤í¬ 2: API ë¹„ìš© ì´ˆê³¼
**ì¦ìƒ**: ì˜ˆìƒë³´ë‹¤ ë§ì€ LLM API í˜¸ì¶œ
**ëŒ€ì‘**:
- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¡œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
- ìºì‹± ì „ëµ (ê°™ì€ ì¡°í•­ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€)
- Batch ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ

### ë¦¬ìŠ¤í¬ 3: ì²˜ë¦¬ ì‹œê°„ ê³¼ë‹¤
**ì¦ìƒ**: ë¬¸ì„œë‹¹ ì²˜ë¦¬ ì‹œê°„ > 5ë¶„
**ëŒ€ì‘**:
- ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” (í˜„ì¬ max_concurrent ì¡°ì •)
- LLM íƒ€ì„ì•„ì›ƒ ì„¤ì •
- ê¸´ ë¬¸ì„œëŠ” ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 

### ë¦¬ìŠ¤í¬ 4: Neo4j ë©”ëª¨ë¦¬ ë¶€ì¡±
**ì¦ìƒ**: ëŒ€ëŸ‰ ë…¸ë“œ/ê´€ê³„ ìƒì„± ì‹œ ë©”ëª¨ë¦¬ ì˜¤ë¥˜
**ëŒ€ì‘**:
- Batch ë‹¨ìœ„ ì»¤ë°‹ (1000ê°œì”©)
- ì¸ë±ìŠ¤ ìµœì í™”
- í•„ìš”ì‹œ Neo4j ë©”ëª¨ë¦¬ ì„¤ì • ì¦ê°€

---

## ì„±ê³µ ì§€í‘œ

êµ¬í˜„ ì™„ë£Œ í›„ ë‹¤ìŒ ì§€í‘œë¡œ ì„±ê³µ ì—¬ë¶€ íŒë‹¨:

1. **ì²˜ë¦¬ ì„±ê³µë¥ **: > 95% (58ê°œ ì¤‘ 55ê°œ ì´ìƒ)
2. **ì—”í‹°í‹° ì¶”ì¶œ ì •í™•ë„**: > 80% (ìˆ˜ë™ ê²€ì¦ ìƒ˜í”Œ 10ê°œ)
3. **ê´€ê³„ ì¶”ì¶œ ì •í™•ë„**: > 70% (ìˆ˜ë™ ê²€ì¦ ìƒ˜í”Œ 10ê°œ)
4. **ë¬¸ì„œë‹¹ ì²˜ë¦¬ ì‹œê°„**: < 5ë¶„
5. **API ë¹„ìš©**: < $50 (58ê°œ ì „ì²´ ì¬ì²˜ë¦¬)
6. **Neo4j ë…¸ë“œ ìˆ˜**: > 500ê°œ (ì—”í‹°í‹° ë…¸ë“œ)
7. **Neo4j ê´€ê³„ ìˆ˜**: > 300ê°œ (ì˜ë¯¸ìˆëŠ” ê´€ê³„)

---

## ë³´í—˜ ì•½ê´€ íŠ¹ì„±ì„ í™œìš©í•œ ë¹„ìš© ìµœì í™” ì „ëµ ğŸ¯

### ë³´í—˜ ì•½ê´€ ë¬¸ì„œì˜ íŠ¹ì„±

ë³´í—˜ ì•½ê´€ì€ **ë²„ì „ ì—…ê·¸ë ˆì´ë“œ ì‹œ ë³€ê²½ì´ ë§¤ìš° ì ì€ íŠ¹ì„±**ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

**ë³€ê²½ì´ ì ì€ ë¶€ë¶„ (80-90%)**:
- í‘œì¤€ ì•½ê´€ ì¡°í•­ (ë³´í—˜ê¸ˆ ì§€ê¸‰ ì‚¬ìœ , ë©´ì±…ì‚¬í•­)
- ì§ˆë³‘ ì •ì˜ (ì•”, ë‡Œì¡¸ì¤‘, ì‹¬ê·¼ê²½ìƒ‰ ë“±)
- ì¼ë°˜ ì¡°ê±´ (ê³„ì•½ ì²´ê²°, í•´ì§€, í™˜ê¸‰ê¸ˆ ë“±)
- ìš©ì–´ ì •ì˜
- ë²•ì  ì¡°í•­

**ìì£¼ ë³€ê²½ë˜ëŠ” ë¶€ë¶„ (10-20%)**:
- ë³´ì¥ ê¸ˆì•¡
- ë³´í—˜ë£Œ
- íŠ¹ì•½ ì¶”ê°€/ì‚­ì œ
- ë©´ì±…ê¸°ê°„
- ì¶œì‹œì¼/ê°œì •ì¼
- ì„¸ë¶€ ì¡°ê±´ (ë‚˜ì´ ì œí•œ, ê°±ì‹  ì¡°ê±´ ë“±)

ì´ íŠ¹ì„±ì„ í™œìš©í•˜ë©´ **LLM API ë¹„ìš©ì„ 75-90% ì ˆê°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### ì „ëµ 1: ë¬¸ì„œ Diff ê¸°ë°˜ ì¦ë¶„ í•™ìŠµ â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ì´ì „ ë²„ì „ê³¼ ë¹„êµí•˜ì—¬ **ë³€ê²½ëœ ë¶€ë¶„ë§Œ** LLMìœ¼ë¡œ ë¶„ì„

**êµ¬í˜„ ë°©ë²•**:

```python
from difflib import unified_diff
import hashlib

class IncrementalLearner:
    """ì¦ë¶„ í•™ìŠµ ì—”ì§„"""

    def find_previous_version(self, document_id):
        """ë™ì¼ ìƒí’ˆì˜ ì´ì „ ë²„ì „ ì°¾ê¸°"""
        doc = get_document(document_id)

        # ë¬¸ì„œëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
        # ì˜ˆ: "ê°œì¸ìš©ì• ë‹ˆì¹´ë‹¤ì´ë ‰íŠ¸ìë™ì°¨ë³´í—˜ - ë³´í—˜ì•½ê´€ (2023.12)"
        product_name = self._extract_product_name(doc.title)

        # DBì—ì„œ ë™ì¼ ìƒí’ˆì˜ ì´ì „ ë²„ì „ ê²€ìƒ‰
        query = """
            SELECT id, title, updated_at
            FROM crawler_documents
            WHERE insurer = :insurer
              AND product_type = :product_type
              AND title LIKE :product_pattern
              AND status = 'completed'
              AND id != :current_id
            ORDER BY updated_at DESC
            LIMIT 1
        """

        previous = db.execute(query, {
            'insurer': doc.insurer,
            'product_type': doc.product_type,
            'product_pattern': f'%{product_name}%',
            'current_id': document_id
        }).fetchone()

        return previous['id'] if previous else None

    def calculate_diff_ratio(self, old_doc_id, new_doc_id):
        """ë‘ ë¬¸ì„œ ê°„ ë³€ê²½ ë¹„ìœ¨ ê³„ì‚°"""
        old_text = get_document_text(old_doc_id)
        new_text = get_document_text(new_doc_id)

        # ë¼ì¸ ë‹¨ìœ„ diff
        diff = list(unified_diff(
            old_text.splitlines(),
            new_text.splitlines(),
            lineterm=''
        ))

        # ë³€ê²½ëœ ë¼ì¸ ìˆ˜
        changed_lines = sum(1 for line in diff if line.startswith(('+', '-')))
        total_lines = len(new_text.splitlines())

        diff_ratio = changed_lines / total_lines if total_lines > 0 else 1.0
        return diff_ratio

    async def incremental_learning(self, document_id, previous_version_id):
        """ì¦ë¶„ í•™ìŠµ ì‹¤í–‰"""

        old_doc = get_document(previous_version_id)
        new_doc = get_document(document_id)

        # Diff ê³„ì‚°
        diff_sections = self._extract_changed_sections(old_doc.text, new_doc.text)

        logger.info(f"Changed sections: {len(diff_sections)} ({len(diff_sections)/len(new_doc.text.split('\\n\\n'))*100:.1f}%)")

        if len(diff_sections) < 50:  # ë³€ê²½ëœ ì¡°í•­ì´ 50ê°œ ë¯¸ë§Œ
            # ë³€ê²½ëœ ë¶€ë¶„ë§Œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ (ì €ë ´!)
            new_extractions = await extract_entities_relations(diff_sections)

            # ì´ì „ ë²„ì „ì˜ ì—”í‹°í‹°/ê´€ê³„ë¥¼ Neo4jì—ì„œ ë³µì‚¬
            await copy_unchanged_graph(previous_version_id, document_id)

            # ë³€ê²½ëœ ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸
            await update_graph(document_id, new_extractions)

            return {
                'method': 'incremental',
                'cost_saving': '80-90%',
                'sections_analyzed': len(diff_sections)
            }
        else:
            # ë³€ê²½ì´ ë§ìœ¼ë©´ ì „ì²´ ì¬í•™ìŠµ
            return await full_learning(document_id)

    def _extract_changed_sections(self, old_text, new_text):
        """ë³€ê²½ëœ ì¡°í•­ë§Œ ì¶”ì¶œ"""
        old_sections = old_text.split('\\n\\n')
        new_sections = new_text.split('\\n\\n')

        # í•´ì‹œ ê¸°ë°˜ ë³€ê²½ ê°ì§€
        old_hashes = {
            hashlib.md5(self._normalize(s).encode()).hexdigest(): s
            for s in old_sections
        }

        changed_sections = []
        for new_section in new_sections:
            new_hash = hashlib.md5(self._normalize(new_section).encode()).hexdigest()
            if new_hash not in old_hashes:
                changed_sections.append(new_section)

        return changed_sections

    def _normalize(self, text):
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë¬´ì‹œ)"""
        import re
        text = re.sub(r'\\s+', ' ', text)
        text = re.sub(r'[^\\w\\sê°€-í£]', '', text)
        return text.strip().lower()
```

**íš¨ê³¼**:
- ë¹„ìš© ì ˆê°: **80-90%** (ë³€ê²½ëœ 10-20%ë§Œ LLM í˜¸ì¶œ)
- ì‹œê°„ ì ˆê°: **70-80%** (ëŒ€ë¶€ë¶„ ê·¸ë˜í”„ ë³µì‚¬ë§Œ)
- ì¼ê´€ì„± ìœ ì§€: ê¸°ì¡´ ì§€ì‹ ê·¸ë˜í”„ ì¬í™œìš©

---

### ì „ëµ 2: í…œí”Œë¦¿ ì¶”ì¶œ + ë³€ìˆ˜í™” â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: í‘œì¤€ ì•½ê´€ì„ **í…œí”Œë¦¿**ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ë³€ê²½ë˜ëŠ” ê°’ë§Œ **ë³€ìˆ˜**ë¡œ ê´€ë¦¬

**êµ¬í˜„ ë‹¨ê³„**:

**Step 1: í…œí”Œë¦¿ ì¶”ì¶œ (ìµœì´ˆ 1íšŒ)**
```python
class InsuranceTemplateExtractor:
    """ë³´í—˜ ì•½ê´€ í…œí”Œë¦¿ ì¶”ì¶œê¸°"""

    def extract_common_templates(self, documents: List[str]):
        """ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê³µí†µ íŒ¨í„´ ì¶”ì¶œ"""

        # 1. ëª¨ë“  ë¬¸ì„œë¥¼ ì¡°í•­ ë‹¨ìœ„ë¡œ ë¶„í• 
        all_clauses = []
        for doc in documents:
            clauses = doc.split('\\n\\n')
            all_clauses.extend(clauses)

        # 2. ì •ê·œí™” ë° ë¹ˆë„ ê³„ì‚°
        clause_frequency = {}
        for clause in all_clauses:
            normalized = self._normalize_for_template(clause)
            clause_frequency[normalized] = clause_frequency.get(normalized, 0) + 1

        # 3. 90% ì´ìƒ ë“±ì¥í•˜ëŠ” ì¡°í•­ì„ í…œí”Œë¦¿ìœ¼ë¡œ ì¶”ì¶œ
        threshold = len(documents) * 0.9
        templates = {
            clause: freq
            for clause, freq in clause_frequency.items()
            if freq >= threshold
        }

        logger.info(f"Extracted {len(templates)} common templates from {len(documents)} documents")

        # 4. í…œí”Œë¦¿ì„ íŒŒì¼ë¡œ ì €ì¥
        with open('data/insurance_templates.json', 'w', encoding='utf-8') as f:
            json.dump(templates, f, ensure_ascii=False, indent=2)

        return templates

    def _normalize_for_template(self, clause):
        """ìˆ«ì, ë‚ ì§œ, ê¸ˆì•¡ì„ ë³€ìˆ˜ë¡œ ë³€í™˜"""
        import re

        # ê¸ˆì•¡ -> {amount}
        clause = re.sub(r'\\d+[ì²œë§Œì–µ]?ì›', '{amount}', clause)
        clause = re.sub(r'\\d+,\\d+ì›', '{amount}', clause)

        # ê¸°ê°„ -> {period}
        clause = re.sub(r'\\d+ê°œì›”', '{period_months}', clause)
        clause = re.sub(r'\\d+ë…„', '{period_years}', clause)
        clause = re.sub(r'\\d+ì¼', '{period_days}', clause)

        # ë‚ ì§œ -> {date}
        clause = re.sub(r'\\d{4}[-.]\\d{2}[-.]\\d{2}', '{date}', clause)

        # ë¹„ìœ¨ -> {ratio}
        clause = re.sub(r'\\d+%', '{ratio}', clause)

        # ìˆ«ì -> {number}
        clause = re.sub(r'\\d+', '{number}', clause)

        return clause
```

**Step 2: ì‹ ê·œ ë¬¸ì„œëŠ” í…œí”Œë¦¿ ë§¤ì¹­**
```python
class TemplateMatcher:
    """í…œí”Œë¦¿ ê¸°ë°˜ í•™ìŠµ"""

    def __init__(self):
        # ì‚¬ì „ ì¶”ì¶œëœ í…œí”Œë¦¿ ë¡œë“œ
        with open('data/insurance_templates.json', 'r', encoding='utf-8') as f:
            self.templates = json.load(f)

        # í…œí”Œë¦¿ë³„ ì‚¬ì „ ì¶”ì¶œëœ ì—”í‹°í‹°/ê´€ê³„ ë¡œë“œ
        with open('data/template_graph.json', 'r', encoding='utf-8') as f:
            self.template_graphs = json.load(f)

    async def template_based_learning(self, document_id):
        """í…œí”Œë¦¿ ê¸°ë°˜ ê³ ì† í•™ìŠµ"""

        doc = get_document(document_id)
        clauses = doc.text.split('\\n\\n')

        matched_count = 0
        extracted_data = []

        for clause in clauses:
            # í…œí”Œë¦¿ ë§¤ì¹­
            template_match = self._match_template(clause)

            if template_match:
                # í…œí”Œë¦¿ ë§¤ì¹­ ì„±ê³µ! (LLM í˜¸ì¶œ ë¶ˆí•„ìš”)
                matched_count += 1

                # ë³€ìˆ˜ë§Œ ì¶”ì¶œ (ì •ê·œì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ)
                variables = self._extract_variables(clause)

                # í…œí”Œë¦¿ì˜ ê¸°ì¡´ ê·¸ë˜í”„ ë³µì‚¬
                graph = copy.deepcopy(self.template_graphs[template_match['id']])

                # ë³€ìˆ˜ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                self._update_graph_variables(graph, variables)

                extracted_data.append(graph)
            else:
                # í…œí”Œë¦¿ ë§¤ì¹­ ì‹¤íŒ¨, LLM í˜¸ì¶œ í•„ìš”
                result = await extract_with_llm(clause)
                extracted_data.append(result)

        match_rate = matched_count / len(clauses) * 100
        logger.info(f"Template match rate: {match_rate:.1f}% ({matched_count}/{len(clauses)})")

        return {
            'method': 'template-based',
            'match_rate': match_rate,
            'cost_saving': f'{match_rate:.0f}%',
            'data': extracted_data
        }

    def _match_template(self, clause):
        """ì¡°í•­ì„ í…œí”Œë¦¿ì— ë§¤ì¹­"""
        from difflib import SequenceMatcher

        normalized = InsuranceTemplateExtractor()._normalize_for_template(clause)

        best_match = None
        best_similarity = 0.0

        for template_id, template_text in self.templates.items():
            similarity = SequenceMatcher(None, normalized, template_text).ratio()
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = {'id': template_id, 'similarity': similarity}

        # 90% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë§¤ì¹­ ì„±ê³µ
        return best_match if best_similarity >= 0.9 else None

    def _extract_variables(self, clause):
        """ë³€ìˆ˜ê°’ ì¶”ì¶œ (ì •ê·œì‹)"""
        import re

        variables = {}

        # ê¸ˆì•¡
        amounts = re.findall(r'(\\d+[ì²œë§Œì–µ]?ì›|\\d+,\\d+ì›)', clause)
        if amounts:
            variables['amounts'] = amounts

        # ê¸°ê°„
        periods = re.findall(r'(\\d+ê°œì›”|\\d+ë…„|\\d+ì¼)', clause)
        if periods:
            variables['periods'] = periods

        # ë‚ ì§œ
        dates = re.findall(r'(\\d{4}[-.]\\d{2}[-.]\\d{2})', clause)
        if dates:
            variables['dates'] = dates

        # ë¹„ìœ¨
        ratios = re.findall(r'(\\d+%)', clause)
        if ratios:
            variables['ratios'] = ratios

        return variables

    def _update_graph_variables(self, graph, variables):
        """ê·¸ë˜í”„ì˜ ë³€ìˆ˜ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸"""
        # Neo4j ë…¸ë“œ ì†ì„± ì—…ë°ì´íŠ¸
        for node in graph['nodes']:
            if 'amount' in node and variables.get('amounts'):
                node['amount'] = variables['amounts'][0]
            if 'period' in node and variables.get('periods'):
                node['period'] = variables['periods'][0]

        return graph
```

**íš¨ê³¼**:
- ë¹„ìš© ì ˆê°: **95%** (ë³€ìˆ˜ ì¶”ì¶œë§Œ, LLM í˜¸ì¶œ ì—†ìŒ)
- ì†ë„ í–¥ìƒ: **10-20ë°°** (ì •ê·œì‹ vs LLM)
- í’ˆì§ˆ í–¥ìƒ: í‘œì¤€ íŒ¨í„´ ì¬ì‚¬ìš©ìœ¼ë¡œ ì¼ê´€ì„± ì¦ê°€

---

### ì „ëµ 3: ì²´ì¸ì§€ ë¡œê·¸ ê¸°ë°˜ í•™ìŠµ â­â­â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ë³´í—˜ ì•½ê´€ ë¬¸ì„œì— í¬í•¨ëœ **ê°œì • ì´ë ¥** í™œìš©

ë³´í—˜ ì•½ê´€ ë¬¸ì„œì—ëŠ” ë³´í†µ "ê°œì • ì´ë ¥" ë˜ëŠ” "ì£¼ìš” ë³€ê²½ì‚¬í•­" ì„¹ì…˜ì´ ìˆìŠµë‹ˆë‹¤:

```
ê°œì • ì´ë ¥
----------
2023.12.01 - ì•”ì§„ë‹¨íŠ¹ì•½ ë³´ì¥ê¸ˆì•¡ 3ì²œë§Œì› -> 5ì²œë§Œì›
2023.12.01 - ë‡Œì¶œí˜ˆì§„ë‹¨íŠ¹ì•½ ì‹ ì„¤
2023.06.01 - ë©´ì±…ê¸°ê°„ 90ì¼ -> 60ì¼ë¡œ ë‹¨ì¶•
```

**êµ¬í˜„ ë°©ë²•**:

```python
class ChangeLogParser:
    """ê°œì • ì´ë ¥ íŒŒì„œ"""

    def parse_changelog(self, text):
        """ê°œì • ì´ë ¥ ì„¹ì…˜ ì¶”ì¶œ ë° íŒŒì‹±"""

        # 1. ê°œì • ì´ë ¥ ì„¹ì…˜ ì°¾ê¸°
        changelog_section = self._extract_changelog_section(text)

        if not changelog_section:
            return None

        # 2. ê° ë³€ê²½ í•­ëª© íŒŒì‹±
        changes = []
        for line in changelog_section.split('\\n'):
            change = self._parse_change_line(line)
            if change:
                changes.append(change)

        return changes

    def _extract_changelog_section(self, text):
        """ê°œì • ì´ë ¥ ì„¹ì…˜ ì¶”ì¶œ"""
        import re

        # íŒ¨í„´: "ê°œì • ì´ë ¥", "ì£¼ìš” ë³€ê²½ì‚¬í•­", "ì•½ê´€ ë³€ê²½ ë‚´ì—­" ë“±
        patterns = [
            r'ê°œì •\\s*ì´ë ¥[:\\s]*([\\s\\S]+?)(?=\\n\\n|$)',
            r'ì£¼ìš”\\s*ë³€ê²½\\s*ì‚¬í•­[:\\s]*([\\s\\S]+?)(?=\\n\\n|$)',
            r'ì•½ê´€\\s*ë³€ê²½\\s*ë‚´ì—­[:\\s]*([\\s\\S]+?)(?=\\n\\n|$)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.MULTILINE)
            if match:
                return match.group(1)

        return None

    def _parse_change_line(self, line):
        """
        ê°œì • ë‚´ì—­ íŒŒì‹±
        ì˜ˆ: "2023.12.01 - ì•”ì§„ë‹¨íŠ¹ì•½ ë³´ì¥ê¸ˆì•¡ 3ì²œë§Œì› -> 5ì²œë§Œì›"
        """
        import re

        # íŒ¨í„´ 1: "A -> B" í˜•íƒœ
        pattern1 = r'(\\d{4}[.-]\\d{2}[.-]\\d{2})\\s*-\\s*(.+?)\\s+(.+?)\\s+(.+?)\\s*->\\s*(.+)'
        match = re.match(pattern1, line)

        if match:
            return {
                'date': match.group(1),
                'section': match.group(2),
                'attribute': match.group(3),
                'old_value': match.group(4),
                'new_value': match.group(5),
                'change_type': 'update'
            }

        # íŒ¨í„´ 2: "ì‹ ì„¤" í˜•íƒœ
        pattern2 = r'(\\d{4}[.-]\\d{2}[.-]\\d{2})\\s*-\\s*(.+?)\\s+ì‹ ì„¤'
        match = re.match(pattern2, line)

        if match:
            return {
                'date': match.group(1),
                'section': match.group(2),
                'change_type': 'added'
            }

        # íŒ¨í„´ 3: "ì‚­ì œ" í˜•íƒœ
        pattern3 = r'(\\d{4}[.-]\\d{2}[.-]\\d{2})\\s*-\\s*(.+?)\\s+ì‚­ì œ'
        match = re.match(pattern3, line)

        if match:
            return {
                'date': match.group(1),
                'section': match.group(2),
                'change_type': 'deleted'
            }

        return None

class ChangeLogBasedLearner:
    """ì²´ì¸ì§€ ë¡œê·¸ ê¸°ë°˜ í•™ìŠµ"""

    async def changelog_based_update(self, document_id):
        """ì²´ì¸ì§€ ë¡œê·¸ ê¸°ë°˜ ê³ ì† ì—…ë°ì´íŠ¸"""

        doc = get_document(document_id)
        parser = ChangeLogParser()

        # ê°œì • ì´ë ¥ íŒŒì‹±
        changes = parser.parse_changelog(doc.text)

        if not changes:
            logger.info("No changelog found, falling back to full learning")
            return None

        logger.info(f"Found {len(changes)} changes in changelog")

        # ì´ì „ ë²„ì „ ì°¾ê¸°
        previous_version_id = self.find_previous_version(document_id)

        if not previous_version_id:
            logger.warning("No previous version found")
            return None

        # ì´ì „ ë²„ì „ ê·¸ë˜í”„ ë³µì‚¬
        await copy_graph(previous_version_id, document_id)

        # ê°œì • ì´ë ¥ì— ë”°ë¼ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        for change in changes:
            await self._apply_change(document_id, change)

        return {
            'method': 'changelog-based',
            'changes_applied': len(changes),
            'cost_saving': '99%',  # LLM í˜¸ì¶œ ê±°ì˜ ì—†ìŒ!
            'llm_calls': 0
        }

    async def _apply_change(self, document_id, change):
        """ê°œì • ë‚´ì—­ì„ Neo4j ê·¸ë˜í”„ì— ë°˜ì˜"""

        if change['change_type'] == 'update':
            # ì†ì„± ì—…ë°ì´íŠ¸
            await neo4j_update_node_attribute(
                document_id=document_id,
                section=change['section'],
                attribute=change['attribute'],
                old_value=change['old_value'],
                new_value=change['new_value']
            )

        elif change['change_type'] == 'added':
            # ì‹ ê·œ ë…¸ë“œ ì¶”ê°€ (ì´ ë¶€ë¶„ë§Œ LLM í•„ìš”í•  ìˆ˜ ìˆìŒ)
            # í•˜ì§€ë§Œ ë§¤ìš° ì‘ì€ í…ìŠ¤íŠ¸ë§Œ ë¶„ì„í•˜ë©´ ë˜ë¯€ë¡œ ë¹„ìš© ë¯¸ë¯¸
            await create_new_coverage_node(
                document_id=document_id,
                section=change['section']
            )

        elif change['change_type'] == 'deleted':
            # ë…¸ë“œ ì‚­ì œ (ë˜ëŠ” ë¹„í™œì„±í™”)
            await delete_coverage_node(
                document_id=document_id,
                section=change['section']
            )
```

**íš¨ê³¼**:
- ë¹„ìš©: **ê±°ì˜ 0ì›** (LLM í˜¸ì¶œ ìµœì†Œí™”)
- ì†ë„: **ì¦‰ì‹œ** (ë‹¨ìˆœ íŒŒì‹± + Neo4j ì—…ë°ì´íŠ¸)
- ì •í™•ë„: **ë§¤ìš° ë†’ìŒ** (ë³´í—˜ì‚¬ ê³µì‹ ë¬¸ì„œ í™œìš©)

---

### ì „ëµ 4: ì˜ë¯¸ì  ì²­í‚¹ + ìºì‹± â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ë¬¸ì„œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ , ë™ì¼í•œ ì¡°í•­ì€ **ìºì‹œ**ì—ì„œ ì¬ì‚¬ìš©

**êµ¬í˜„ ë°©ë²•**:

```python
import hashlib
from functools import lru_cache
import redis

class SemanticChunkingLearner:
    """ì˜ë¯¸ì  ì²­í‚¹ + ìºì‹± í•™ìŠµê¸°"""

    def __init__(self):
        # Redis ìºì‹œ ì—°ê²°
        self.cache = redis.Redis(host='localhost', port=6379, db=0)
        self.cache_hits = 0
        self.cache_misses = 0

    def chunk_document(self, text):
        """ë¬¸ì„œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """

        # ì¡°í•­ ë‹¨ìœ„ë¡œ ë¶„í•  (ë¹ˆ ì¤„ ê¸°ì¤€)
        chunks = text.split('\\n\\n')

        # ê° ì²­í¬ ì •ê·œí™” ë° í•´ì‹œ ê³„ì‚°
        processed_chunks = []
        for chunk in chunks:
            if not chunk.strip():
                continue

            normalized = self._normalize(chunk)
            chunk_hash = hashlib.sha256(normalized.encode('utf-8')).hexdigest()

            processed_chunks.append({
                'original': chunk,
                'normalized': normalized,
                'hash': chunk_hash
            })

        return processed_chunks

    async def learn_with_cache(self, document_id):
        """ìºì‹±ì„ í™œìš©í•œ í•™ìŠµ"""

        doc = get_document(document_id)
        chunks = self.chunk_document(doc.text)

        extracted_data = []
        self.cache_hits = 0
        self.cache_misses = 0

        for i, chunk in enumerate(chunks):
            # ìºì‹œ í™•ì¸
            cached_result = self.cache.get(f"chunk:{chunk['hash']}")

            if cached_result:
                # ìºì‹œ íˆíŠ¸! LLM í˜¸ì¶œ ìƒëµ
                self.cache_hits += 1
                result = json.loads(cached_result)
                extracted_data.append(result)

                logger.debug(f"Cache HIT [{i+1}/{len(chunks)}]: {chunk['original'][:50]}...")
            else:
                # ìºì‹œ ë¯¸ìŠ¤, LLM í˜¸ì¶œ í•„ìš”
                self.cache_misses += 1
                result = await extract_entities_and_relations(chunk['original'])

                # ìºì‹œì— ì €ì¥ (1ë…„ ìœ íš¨)
                self.cache.setex(
                    f"chunk:{chunk['hash']}",
                    60 * 60 * 24 * 365,  # 1ë…„
                    json.dumps(result, ensure_ascii=False)
                )

                extracted_data.append(result)

                logger.debug(f"Cache MISS [{i+1}/{len(chunks)}]: {chunk['original'][:50]}...")

        cache_hit_rate = self.cache_hits / len(chunks) * 100 if chunks else 0

        logger.info(f"Cache statistics: {self.cache_hits} hits, {self.cache_misses} misses ({cache_hit_rate:.1f}% hit rate)")

        return {
            'method': 'cached',
            'cache_hit_rate': cache_hit_rate,
            'cost_saving': f'{cache_hit_rate:.0f}%',
            'llm_calls': self.cache_misses,
            'data': extracted_data
        }

    def _normalize(self, text):
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°±, ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)"""
        import re

        # ê³µë°± ì •ê·œí™”
        text = re.sub(r'\\s+', ' ', text)

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, í•œê¸€, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì  ìœ ì§€)
        text = re.sub(r'[^\\w\\sê°€-í£.,!?()%-]', '', text)

        return text.strip()

    def get_cache_stats(self):
        """ìºì‹œ í†µê³„"""
        total_keys = self.cache.dbsize()
        cache_size_mb = self.cache.info()['used_memory'] / 1024 / 1024

        return {
            'total_cached_chunks': total_keys,
            'cache_size_mb': round(cache_size_mb, 2)
        }
```

**íš¨ê³¼**:
- **1ë²ˆì§¸ ë¬¸ì„œ**: 100% LLM í˜¸ì¶œ (ìºì‹œ êµ¬ì¶•)
- **2ë²ˆì§¸ ë¬¸ì„œ**: ~80% ìºì‹œ íˆíŠ¸ (20%ë§Œ LLM)
- **10ë²ˆì§¸ ë¬¸ì„œ**: ~95% ìºì‹œ íˆíŠ¸ (5%ë§Œ LLM)
- **ëˆ„ì  ë¹„ìš© ì ˆê°**: 70-80%

---

### í†µí•© ì „ëµ: ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì—”ì§„ ğŸš€

4ê°€ì§€ ì „ëµì„ ìƒí™©ì— ë§ê²Œ ìë™ ì„ íƒ:

```python
class SmartInsuranceLearner:
    """í†µí•© ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì—”ì§„"""

    def __init__(self):
        self.incremental_learner = IncrementalLearner()
        self.template_matcher = TemplateMatcher()
        self.changelog_learner = ChangeLogBasedLearner()
        self.chunk_learner = SemanticChunkingLearner()

    async def learn_document(self, document_id):
        """ìµœì  ì „ëµ ìë™ ì„ íƒ ë° ì‹¤í–‰"""

        logger.info(f"\\n{'='*80}")
        logger.info(f"Starting smart learning for document: {document_id}")
        logger.info(f"{'='*80}\\n")

        # 1. ë™ì¼ ìƒí’ˆì˜ ì´ì „ ë²„ì „ ì°¾ê¸°
        previous_version = self.incremental_learner.find_previous_version(document_id)

        if previous_version:
            logger.info(f"âœ“ Found previous version: {previous_version}")

            # ì „ëµ 3: ì²´ì¸ì§€ ë¡œê·¸ í™•ì¸ (ìµœìš°ì„ )
            doc = get_document(document_id)
            if 'ê°œì • ì´ë ¥' in doc.text or 'ì£¼ìš” ë³€ê²½ì‚¬í•­' in doc.text:
                logger.info("â†’ Strategy: CHANGELOG-BASED (Cost: ~$0)")
                result = await self.changelog_learner.changelog_based_update(document_id)

                if result:
                    self._log_result(result)
                    return result

            # ì „ëµ 1: Diff ê¸°ë°˜ ì¦ë¶„ í•™ìŠµ
            diff_ratio = self.incremental_learner.calculate_diff_ratio(
                previous_version,
                document_id
            )

            logger.info(f"  Document diff ratio: {diff_ratio*100:.1f}%")

            if diff_ratio < 0.3:  # ë³€ê²½ì´ 30% ë¯¸ë§Œ
                logger.info(f"â†’ Strategy: INCREMENTAL (Cost: ~${0.70 * diff_ratio:.2f})")
                result = await self.incremental_learner.incremental_learning(
                    document_id,
                    previous_version
                )
                self._log_result(result)
                return result
            else:
                logger.info(f"  Diff ratio too high ({diff_ratio*100:.1f}%), trying other strategies...")
        else:
            logger.info("âœ— No previous version found")

        # 2. í…œí”Œë¦¿ ë§¤ì¹­ ì‹œë„
        logger.info("\\nChecking template matching...")
        doc = get_document(document_id)
        sample_clauses = doc.text.split('\\n\\n')[:10]  # ì²« 10ê°œ ì¡°í•­ ìƒ˜í”Œë§

        match_count = 0
        for clause in sample_clauses:
            if self.template_matcher._match_template(clause):
                match_count += 1

        match_rate = match_count / len(sample_clauses) if sample_clauses else 0
        logger.info(f"  Template match rate (sample): {match_rate*100:.1f}%")

        if match_rate >= 0.7:  # 70% ì´ìƒ ë§¤ì¹­
            logger.info(f"â†’ Strategy: TEMPLATE-BASED (Cost: ~${0.70 * (1-match_rate):.2f})")
            result = await self.template_matcher.template_based_learning(document_id)
            self._log_result(result)
            return result

        # 3. ìºì‹±ì„ í™œìš©í•œ ì „ì²´ í•™ìŠµ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        logger.info("\\nâ†’ Strategy: FULL LEARNING WITH CACHE (Cost: ~$0.70)")
        result = await self.chunk_learner.learn_with_cache(document_id)
        self._log_result(result)
        return result

    def _log_result(self, result):
        """ê²°ê³¼ ë¡œê¹…"""
        logger.info(f"\\n{'='*80}")
        logger.info(f"Learning completed:")
        logger.info(f"  Method: {result.get('method', 'unknown').upper()}")
        logger.info(f"  Cost saving: {result.get('cost_saving', 'N/A')}")

        if 'cache_hit_rate' in result:
            logger.info(f"  Cache hit rate: {result['cache_hit_rate']:.1f}%")
        if 'match_rate' in result:
            logger.info(f"  Template match rate: {result['match_rate']:.1f}%")
        if 'changes_applied' in result:
            logger.info(f"  Changes applied: {result['changes_applied']}")

        logger.info(f"{'='*80}\\n")
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# parallel_document_processor.py ì—ì„œ í˜¸ì¶œ
smart_learner = SmartInsuranceLearner()

# Line 229-236 ëŒ€ì²´
result = await smart_learner.learn_document(document_id)

logger.info(f"Smart learning result: {result['method']} (saving: {result['cost_saving']})")
```

---

### ë¹„ìš© ì ˆê° íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜

**ì‹œë‚˜ë¦¬ì˜¤: 58ê°œ ë¬¸ì„œ í•™ìŠµ**

| ì „ëµ | ì ìš© ë¬¸ì„œ ìˆ˜ | ë¬¸ì„œë‹¹ ë¹„ìš© | ì†Œê³„ |
|------|-------------|------------|------|
| **ê¸°ì¡´ ë°©ì‹** (ì „ì²´ LLM) | 58ê°œ | $0.70 | **$40.60** |
| | | | |
| **ìŠ¤ë§ˆíŠ¸ í•™ìŠµ** | | | |
| í…œí”Œë¦¿ ì¶”ì¶œ (ìµœì´ˆ) | 2ê°œ | $0.70 | $1.40 |
| ì²´ì¸ì§€ ë¡œê·¸ ê¸°ë°˜ | 15ê°œ | $0.00 | $0.00 |
| ì¦ë¶„ í•™ìŠµ (diff < 20%) | 25ê°œ | $0.10 | $2.50 |
| í…œí”Œë¦¿ ë§¤ì¹­ (70%+) | 10ê°œ | $0.20 | $2.00 |
| ìºì‹± ì „ì²´ í•™ìŠµ | 6ê°œ | $0.50 | $3.00 |
| **í•©ê³„** | **58ê°œ** | | **$8.90** |

**ì ˆê°ì•¡**: $40.60 - $8.90 = **$31.70 (78% ì ˆê°)** ğŸ‰

---

### ì‹œê°„ ì ˆê° íš¨ê³¼

| ë°©ì‹ | ë¬¸ì„œë‹¹ ì‹œê°„ | 58ê°œ ì´ ì‹œê°„ |
|------|------------|-------------|
| **ê¸°ì¡´ ë°©ì‹** | 3ë¶„ | 174ë¶„ (2.9ì‹œê°„) |
| **ìŠ¤ë§ˆíŠ¸ í•™ìŠµ** | 0.5ë¶„ | 29ë¶„ (0.5ì‹œê°„) |

**ì‹œê°„ ì ˆê°**: 174ë¶„ - 29ë¶„ = **145ë¶„ (83% ì ˆê°)** âš¡

---

### êµ¬í˜„ ìš°ì„ ìˆœìœ„

**Phase 0: ì¤€ë¹„** (1ì‹œê°„)
- Redis ì„¤ì¹˜ ë° ì„¤ì • (ìºì‹±ìš©)
- í…œí”Œë¦¿ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (`data/`)
- ë¬¸ì„œ ë²„ì „ ì¶”ì¶œ ë¡œì§ êµ¬í˜„

**Phase 1: ìºì‹± êµ¬í˜„** (2ì‹œê°„)
- `SemanticChunkingLearner` í´ë˜ìŠ¤ êµ¬í˜„
- Redis ì—°ë™
- ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ë¡œê¹…

**Phase 2: Diff ê¸°ë°˜ ì¦ë¶„ í•™ìŠµ** (3ì‹œê°„)
- `IncrementalLearner` í´ë˜ìŠ¤ êµ¬í˜„
- ë¬¸ì„œ ë²„ì „ ê°ì§€ ë° Diff ê³„ì‚°
- ê·¸ë˜í”„ ë³µì‚¬ ë¡œì§

**Phase 3: í…œí”Œë¦¿ ê¸°ë°˜ í•™ìŠµ** (4ì‹œê°„)
- `InsuranceTemplateExtractor` êµ¬í˜„
- í…œí”Œë¦¿ ì¶”ì¶œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
- `TemplateMatcher` êµ¬í˜„

**Phase 4: ì²´ì¸ì§€ ë¡œê·¸ íŒŒì‹±** (1ì‹œê°„)
- `ChangeLogParser` êµ¬í˜„
- ê°œì • ì´ë ¥ íŒ¨í„´ ì¸ì‹
- Neo4j ì—…ë°ì´íŠ¸ ë¡œì§

**Phase 5: í†µí•© ë° í…ŒìŠ¤íŠ¸** (2ì‹œê°„)
- `SmartInsuranceLearner` í†µí•©
- `parallel_document_processor.py` ìˆ˜ì •
- E2E í…ŒìŠ¤íŠ¸

**ì´ ì˜ˆìƒ ì‹œê°„**: **13ì‹œê°„**

---

### ì„±ê³µ ì§€í‘œ (ì—…ë°ì´íŠ¸)

ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì „ëµ ë„ì… í›„:

1. **í‰ê·  ë¹„ìš©**: < $0.20/ë¬¸ì„œ (ê¸°ì¡´ $0.70 ëŒ€ë¹„ 71% ì ˆê°)
2. **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: < 30ì´ˆ/ë¬¸ì„œ (ê¸°ì¡´ 3ë¶„ ëŒ€ë¹„ 83% ë‹¨ì¶•)
3. **ìºì‹œ íˆíŠ¸ìœ¨**: > 70% (10ê°œ ë¬¸ì„œ ì´í›„)
4. **í…œí”Œë¦¿ ë§¤ì¹­ë¥ **: > 60%
5. **ì²´ì¸ì§€ ë¡œê·¸ í™œìš©ë¥ **: > 25%
6. **ì „ì²´ ë¹„ìš© ì ˆê°**: > 75%

---

## ì°¸ê³  ìë£Œ

**ì½”ë“œ ìœ„ì¹˜**:
- `backend/app/services/ingestion/relation_extractor.py` - ê´€ê³„ ì¶”ì¶œ ë¡œì§
- `backend/app/services/ingestion/entity_linker.py` - ì—”í‹°í‹° ì—°ê²° ë¡œì§
- `backend/app/services/critical_data_extractor.py` - ê¸ˆì•¡/ê¸°ê°„/ì§ˆë³‘ì½”ë“œ ì¶”ì¶œ
- `backend/app/services/parallel_document_processor.py` - ë©”ì¸ ì²˜ë¦¬ ë¡œì§ (ìˆ˜ì • í•„ìš”)
- `backend/worker_graph_updater.py` - ê·¸ë˜í”„ ì—…ë°ì´í„° (ìˆ˜ì • í•„ìš”)

**ë°ì´í„°ë² ì´ìŠ¤**:
- PostgreSQL: `crawler_documents` í…Œì´ë¸”
- Neo4j: `bolt://localhost:7687`

**ëª¨ë‹ˆí„°ë§**:
- ëŒ€ì‹œë³´ë“œ: http://localhost:3000/admin/dashboard
- Neo4j Browser: http://localhost:7474

---

## ê²°ë¡ 

**ì¶”ì²œ ë°©í–¥**: Option A (ìƒ˜í”Œ í…ŒìŠ¤íŠ¸)

**ì´ìœ **:
1. ë¦¬ìŠ¤í¬ ìµœì†Œí™”
2. ë¹ ë¥¸ ê²€ì¦ (2-3ì‹œê°„)
3. ë¹„ìš© íš¨ìœ¨ì  (<$2)
4. ê²°ê³¼ í™•ì¸ í›„ í™•ì¥ ê²°ì • ê°€ëŠ¥

**ë‹¤ìŒ ë‹¨ê³„**:
1. LLM API í‚¤ í™•ì¸
2. í…ŒìŠ¤íŠ¸ ë¬¸ì„œ 1-2ê°œ ì„ ì •
3. Phase 1-2 êµ¬í˜„ (ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ)
4. ê²°ê³¼ ê²€ì¦ í›„ ì „ì²´ ì ìš© ì—¬ë¶€ ê²°ì •

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2025-12-02
**ì‘ì„±ì**: Claude (AI Assistant)
**í”„ë¡œì íŠ¸**: InsureGraph Pro
**ë²„ì „**: 1.0
