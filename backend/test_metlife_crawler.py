"""
MetLife Crawler í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë©”íŠ¸ë¼ì´í”„ ë³´í—˜ ì•½ê´€ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
"""
import asyncio
import sys
from loguru import logger

# ë¡œê¹… ì„¤ì •
logger.remove()
logger.add(sys.stdout, level="INFO")

from app.services.crawlers.metlife_crawler import MetLifeCrawler


async def test_metlife_crawler():
    """ë©”íŠ¸ë¼ì´í”„ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""

    print("=" * 80)
    print("MetLife ë³´í—˜ ì•½ê´€ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = MetLifeCrawler()

    # 1. íŒë§¤ìƒí’ˆ í¬ë¡¤ë§ (ìµœëŒ€ 5ê°œë§Œ)
    print("\n[1] íŒë§¤ìƒí’ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
    active_products = await crawler.get_product_list(category='active')

    print(f"\nâœ… íŒë§¤ìƒí’ˆ ë°œê²¬: {len(active_products)}ê°œ")

    if active_products:
        print("\nğŸ“‹ íŒë§¤ìƒí’ˆ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
        for i, product in enumerate(active_products[:5], 1):
            print(f"\n  {i}. {product['product_name']}")
            print(f"     ìƒí’ˆ ìœ í˜•: {product['product_type']}")
            print(f"     íŒë§¤ê¸°ê°„: {product['sales_period']}")
            print(f"     ì¹´í…Œê³ ë¦¬: {product['category']}")
            print(f"     ë‹¤ìš´ë¡œë“œ URL: {product['download_url'][:80]}...")

    # 2. íŒë§¤ì¤‘ì§€ìƒí’ˆ í¬ë¡¤ë§ (ìµœëŒ€ 5ê°œë§Œ)
    print("\n\n[2] íŒë§¤ì¤‘ì§€ìƒí’ˆ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
    discontinued_products = await crawler.get_product_list(category='discontinued')

    print(f"\nâœ… íŒë§¤ì¤‘ì§€ìƒí’ˆ ë°œê²¬: {len(discontinued_products)}ê°œ")

    if discontinued_products:
        print("\nğŸ“‹ íŒë§¤ì¤‘ì§€ìƒí’ˆ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
        for i, product in enumerate(discontinued_products[:5], 1):
            print(f"\n  {i}. {product['product_name']}")
            print(f"     ìƒí’ˆ ìœ í˜•: {product['product_type']}")
            print(f"     íŒë§¤ê¸°ê°„: {product['sales_period']}")
            print(f"     ì¹´í…Œê³ ë¦¬: {product['category']}")
            print(f"     ë‹¤ìš´ë¡œë“œ URL: {product['download_url'][:80]}...")

    # 3. ì „ì²´ í¬ë¡¤ë§ (í†µê³„ë§Œ)
    print("\n\n[3] ì „ì²´ í¬ë¡¤ë§ í†µê³„...")
    all_products = await crawler.get_product_list()

    print(f"\nâœ… ì´ ìƒí’ˆ ìˆ˜: {len(all_products)}ê°œ")

    # ìƒí’ˆ ìœ í˜•ë³„ í†µê³„
    product_types = {}
    for product in all_products:
        ptype = product['product_type']
        product_types[ptype] = product_types.get(ptype, 0) + 1

    print("\nğŸ“Š ìƒí’ˆ ìœ í˜•ë³„ í†µê³„:")
    for ptype, count in sorted(product_types.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {ptype}: {count}ê°œ")

    # 4. URL ê²€ì¦ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ìƒí’ˆ)
    if all_products:
        print("\n\n[4] URL ê²€ì¦ í…ŒìŠ¤íŠ¸...")
        first_product = all_products[0]
        is_valid = await crawler.validate_url(first_product['download_url'])

        print(f"\n  ìƒí’ˆ: {first_product['product_name']}")
        print(f"  URL: {first_product['download_url'][:80]}...")
        print(f"  ê²€ì¦ ê²°ê³¼: {'âœ… ìœ íš¨' if is_valid else 'âŒ ë¬´íš¨'}")

    # 5. JSON ì €ì¥ í…ŒìŠ¤íŠ¸
    print("\n\n[5] JSON ì €ì¥ í…ŒìŠ¤íŠ¸...")
    json_path = await crawler.save_results_to_json(all_products)

    print(f"\nâœ… í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
    print(f"  ì €ì¥ ê²½ë¡œ: {json_path}")

    # 6. JSON ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("\n\n[6] JSON ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    loaded_data = await crawler.load_results_from_json(json_path)

    print(f"\nâœ… JSON ë¡œë“œ ì™„ë£Œ!")
    print(f"  ì½”ë©˜íŠ¸: {loaded_data['metadata']['comment']}")
    print(f"  í¬ë¡¤ë§ ì‹œê°: {loaded_data['metadata']['crawl_timestamp']}")
    print(f"  ì´ ìƒí’ˆ: {loaded_data['metadata']['total_products']}ê°œ")
    print(f"  íŒë§¤ìƒí’ˆ: {loaded_data['metadata']['active_products']}ê°œ")
    print(f"  íŒë§¤ì¤‘ì§€ìƒí’ˆ: {loaded_data['metadata']['discontinued_products']}ê°œ")

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(test_metlife_crawler())
