"""
Rule-based Entity Extractor for Insurance Documents

ë³´í—˜ ë¬¸ì„œì—ì„œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
ì™¸ë¶€ API ì—†ì´ íŒ¨í„´ ë§¤ì¹­ê³¼ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
"""
import re
import uuid
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from loguru import logger


class RuleBasedEntityExtractor:
    """ê·œì¹™ ê¸°ë°˜ ë³´í—˜ ì—”í‹°í‹° ì¶”ì¶œê¸°"""

    # ì—”í‹°í‹° íƒ€ì… ì •ì˜
    ENTITY_TYPES = {
        "coverage_item": "ë³´ì¥í•­ëª©",
        "benefit_amount": "ë³´í—˜ê¸ˆì•¡",
        "payment_condition": "ì§€ê¸‰ì¡°ê±´",
        "exclusion": "ë©´ì±…ì‚¬í•­",
        "deductible": "ìê¸°ë¶€ë‹´ê¸ˆ",
        "rider": "íŠ¹ì•½",
        "eligibility": "ê°€ì…ì¡°ê±´",
        "article": "ì•½ê´€ì¡°í•­",
        "term": "ë³´í—˜ìš©ì–´",
        "period": "ê¸°ê°„"
    }

    def __init__(self):
        """ì¶”ì¶œ íŒ¨í„´ ì´ˆê¸°í™”"""
        # ë³´ì¥í•­ëª© íŒ¨í„´
        self.coverage_patterns = [
            r"([ê°€-í£]{2,10})\s*ë³´ì¥",
            r"([ê°€-í£]{2,10})\s*ë‹´ë³´",
            r"([ê°€-í£]{2,10})\s*ê¸‰ë¶€",
            r"([ê°€-í£]{2,10})\s*ë³´í—˜ê¸ˆ",
            r"(ì•”|ì§ˆë³‘|ìƒí•´|ì‚¬ë§|í›„ìœ ì¥í•´|ì…ì›|ìˆ˜ìˆ |í†µì›)\s*[ë³´ë‹´ê¸‰]",
        ]

        # ë³´í—˜ê¸ˆì•¡ íŒ¨í„´
        self.benefit_patterns = [
            r"([\d,]+ì›|[\d,]+ë§Œì›|[\d,]+ì–µì›)",
            r"ë³´í—˜ê¸ˆ\s*([\d,]+ì›)",
            r"ê¸‰ë¶€ê¸ˆ\s*([\d,]+ì›)",
            r"([\d,]+)%\s*ì§€ê¸‰",
        ]

        # ì§€ê¸‰ì¡°ê±´ íŒ¨í„´
        self.payment_condition_patterns = [
            r"(ì§„ë‹¨\s*í™•ì •\s*ì‹œ|ì‚¬ë§\s*ì‹œ|ë°œìƒ\s*ì‹œ)\s*ì§€ê¸‰",
            r"([ê°€-í£\s]+)\s*ê²½ìš°\s*ì§€ê¸‰",
            r"ì§€ê¸‰ì‚¬ìœ \s*:\s*([^\.]+)",
            r"ë‹¤ìŒ\s*ê°\s*í˜¸ì˜\s*([^\.]+)\s*ì§€ê¸‰",
        ]

        # ë©´ì±…ì‚¬í•­ íŒ¨í„´
        self.exclusion_patterns = [
            r"ë³´ì¥í•˜ì§€\s*[ì•Šì•„ì•„ë‹ˆ]",
            r"ë©´ì±…\s*ê¸°ê°„",
            r"ì§€ê¸‰\s*[ë¶ˆë¶€]\s*ê°€",
            r"ì œì™¸\s*[ë©ë˜]",
            r"([ê°€-í£\s]+)\s*ê²½ìš°ì—ëŠ”\s*[ë³´ì§€ì•Š]",
            r"ë‹¤ìŒ\s*ê°\s*í˜¸ì˜\s*ê²½ìš°\s*[ë³´ì§€ì•Š]",
        ]

        # ìê¸°ë¶€ë‹´ê¸ˆ íŒ¨í„´
        self.deductible_patterns = [
            r"ìê¸°ë¶€ë‹´ê¸ˆ\s*([\d,]+ì›)",
            r"ë³¸ì¸ë¶€ë‹´ê¸ˆ\s*([\d,]+ì›)",
            r"ê³µì œê¸ˆì•¡\s*([\d,]+ì›)",
            r"([\d,]+ì›)\s*ê³µì œ",
        ]

        # íŠ¹ì•½ íŒ¨í„´
        self.rider_patterns = [
            r"([ê°€-í£]{2,15})\s*íŠ¹ì•½",
            r"íŠ¹ì•½\s*:\s*([ê°€-í£]{2,15})",
        ]

        # ê°€ì…ì¡°ê±´ íŒ¨í„´
        self.eligibility_patterns = [
            r"ê°€ì…\s*ì—°ë ¹\s*:\s*([^\.]+)",
            r"(ë§Œ\s*[\d]+ì„¸\s*~\s*ë§Œ\s*[\d]+ì„¸)",
            r"ê°€ì…\s*ì¡°ê±´\s*:\s*([^\.]+)",
            r"í”¼ë³´í—˜ì\s*ìê²©\s*:\s*([^\.]+)",
        ]

        # ì•½ê´€ì¡°í•­ íŒ¨í„´
        self.article_patterns = [
            r"ì œ\s*([\d]+)\s*ì¡°",
            r"ì œ\s*([\d]+)\s*í•­",
            r"ì œ\s*([\d]+)\s*í˜¸",
            r"ì¡°í•­\s*([\d]+)",
        ]

        # ë³´í—˜ìš©ì–´ íŒ¨í„´
        self.term_patterns = [
            r"(í”¼ë³´í—˜ì|ë³´í—˜ìˆ˜ìµì|ë³´í—˜ê³„ì•½ì|ë³´í—˜ë£Œ|ë³´í—˜ê¸°ê°„)",
            r"(ë³´í—˜ê°€ì…ê¸ˆì•¡|ì±…ì„ì¤€ë¹„ê¸ˆ|í•´ì•½í™˜ê¸‰ê¸ˆ)",
            r"(ì§„ë‹¨í™•ì •|í›„ìœ ì¥í•´|ìƒí•´|ì§ˆë³‘|ì•”)",
        ]

        # ê¸°ê°„ íŒ¨í„´
        self.period_patterns = [
            r"([\d]+ì¼|[\d]+ê°œì›”|[\d]+ë…„)",
            r"ë³´í—˜ê¸°ê°„\s*:\s*([^\.]+)",
            r"ë‚©ì…ê¸°ê°„\s*:\s*([^\.]+)",
            r"ë©´ì±…ê¸°ê°„\s*:\s*([^\.]+)",
        ]

    def extract_entities(self, text: str, document_id: int, insurer: str, product_type: str) -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ì—”í‹°í‹° ì¶”ì¶œ

        Args:
            text: ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            document_id: ë¬¸ì„œ ID
            insurer: ë³´í—˜ì‚¬
            product_type: ìƒí’ˆ íƒ€ì…

        Returns:
            ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        """
        entities = []

        # ê° íƒ€ì…ë³„ ì¶”ì¶œ
        entities.extend(self._extract_by_pattern("coverage_item", self.coverage_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("benefit_amount", self.benefit_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("payment_condition", self.payment_condition_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("exclusion", self.exclusion_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("deductible", self.deductible_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("rider", self.rider_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("eligibility", self.eligibility_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("article", self.article_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("term", self.term_patterns, text, document_id, insurer, product_type))
        entities.extend(self._extract_by_pattern("period", self.period_patterns, text, document_id, insurer, product_type))

        logger.info(f"ğŸ“Š Extracted {len(entities)} entities from document {document_id}")
        return entities

    def _extract_by_pattern(
        self,
        entity_type: str,
        patterns: List[str],
        text: str,
        document_id: int,
        insurer: str,
        product_type: str
    ) -> List[Dict]:
        """
        íŒ¨í„´ìœ¼ë¡œ ì—”í‹°í‹° ì¶”ì¶œ

        Args:
            entity_type: ì—”í‹°í‹° íƒ€ì…
            patterns: ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
            text: ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            document_id: ë¬¸ì„œ ID
            insurer: ë³´í—˜ì‚¬
            product_type: ìƒí’ˆ íƒ€ì…

        Returns:
            ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        """
        entities = []
        seen_labels = set()  # ì¤‘ë³µ ë°©ì§€

        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                # ë§¤ì¹­ëœ ê·¸ë£¹ ì¶”ì¶œ
                label = match.group(1) if match.groups() else match.group(0)
                label = label.strip()

                # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¼ë²¨ ì œì™¸
                if len(label) < 2 or len(label) > 50:
                    continue

                # ì¤‘ë³µ ì œì™¸
                if label in seen_labels:
                    continue
                seen_labels.add(label)

                # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë§¤ì¹­ëœ ìœ„ì¹˜ ì•ë’¤ 50ì)
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end].strip()

                # ì„¤ëª… ìƒì„± (ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì²« ë¬¸ì¥ ì¶”ì¶œ)
                description = self._extract_first_sentence(context)

                entity = {
                    "entity_id": str(uuid.uuid4()),
                    "label": label,
                    "type": entity_type,
                    "description": description,
                    "source_text": context[:200],  # ìµœëŒ€ 200ì
                    "document_id": document_id,
                    "insurer": insurer,
                    "product_type": product_type,
                    "created_at": datetime.utcnow()
                }
                entities.append(entity)

        return entities

    def _extract_first_sentence(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ ì¶”ì¶œ

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            ì²« ë²ˆì§¸ ë¬¸ì¥ (ìµœëŒ€ 100ì)
        """
        # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[\.!?]\s+', text)
        if sentences:
            return sentences[0][:100]
        return text[:100]

    def extract_relationships(self, entities: List[Dict], text: str) -> List[Dict]:
        """
        ì—”í‹°í‹° ê°„ ê´€ê³„ ì¶”ì¶œ

        Args:
            entities: ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            ê´€ê³„ ë¦¬ìŠ¤íŠ¸
        """
        relationships = []

        # ì—”í‹°í‹°ë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        entities_by_type = {}
        for entity in entities:
            entity_type = entity["type"]
            if entity_type not in entities_by_type:
                entities_by_type[entity_type] = []
            entities_by_type[entity_type].append(entity)

        # ê·œì¹™ ê¸°ë°˜ ê´€ê³„ ìƒì„±
        # 1. ë³´ì¥í•­ëª© -> ë³´í—˜ê¸ˆì•¡
        if "coverage_item" in entities_by_type and "benefit_amount" in entities_by_type:
            for coverage in entities_by_type["coverage_item"]:
                for amount in entities_by_type["benefit_amount"]:
                    # í…ìŠ¤íŠ¸ ë‚´ ê±°ë¦¬ê°€ ê°€ê¹Œìš´ ê²½ìš° ê´€ê³„ ìƒì„±
                    if self._is_close_in_text(coverage["source_text"], amount["source_text"], text, max_distance=100):
                        relationships.append({
                            "source_entity_id": coverage["entity_id"],
                            "target_entity_id": amount["entity_id"],
                            "type": "has_amount",
                            "description": f"{coverage['label']}ì˜ ë³´í—˜ê¸ˆì•¡ì€ {amount['label']}ì…ë‹ˆë‹¤",
                            "created_at": datetime.utcnow()
                        })

        # 2. ë³´ì¥í•­ëª© -> ì§€ê¸‰ì¡°ê±´
        if "coverage_item" in entities_by_type and "payment_condition" in entities_by_type:
            for coverage in entities_by_type["coverage_item"]:
                for condition in entities_by_type["payment_condition"]:
                    if self._is_close_in_text(coverage["source_text"], condition["source_text"], text, max_distance=150):
                        relationships.append({
                            "source_entity_id": coverage["entity_id"],
                            "target_entity_id": condition["entity_id"],
                            "type": "requires",
                            "description": f"{coverage['label']} ì§€ê¸‰ ì¡°ê±´: {condition['label']}",
                            "created_at": datetime.utcnow()
                        })

        # 3. ë³´ì¥í•­ëª© -> ë©´ì±…ì‚¬í•­
        if "coverage_item" in entities_by_type and "exclusion" in entities_by_type:
            for coverage in entities_by_type["coverage_item"]:
                for exclusion in entities_by_type["exclusion"]:
                    if self._is_close_in_text(coverage["source_text"], exclusion["source_text"], text, max_distance=150):
                        relationships.append({
                            "source_entity_id": coverage["entity_id"],
                            "target_entity_id": exclusion["entity_id"],
                            "type": "excludes",
                            "description": f"{coverage['label']} ë©´ì±…ì‚¬í•­: {exclusion['label']}",
                            "created_at": datetime.utcnow()
                        })

        # 4. íŠ¹ì•½ -> ë³´ì¥í•­ëª©
        if "rider" in entities_by_type and "coverage_item" in entities_by_type:
            for rider in entities_by_type["rider"]:
                for coverage in entities_by_type["coverage_item"]:
                    if self._is_close_in_text(rider["source_text"], coverage["source_text"], text, max_distance=200):
                        relationships.append({
                            "source_entity_id": rider["entity_id"],
                            "target_entity_id": coverage["entity_id"],
                            "type": "provides",
                            "description": f"{rider['label']}ì€ {coverage['label']}ì„ ì œê³µí•©ë‹ˆë‹¤",
                            "created_at": datetime.utcnow()
                        })

        # 5. ì•½ê´€ì¡°í•­ -> ë³´í—˜ìš©ì–´
        if "article" in entities_by_type and "term" in entities_by_type:
            for article in entities_by_type["article"]:
                for term in entities_by_type["term"]:
                    if self._is_close_in_text(article["source_text"], term["source_text"], text, max_distance=100):
                        relationships.append({
                            "source_entity_id": article["entity_id"],
                            "target_entity_id": term["entity_id"],
                            "type": "defines",
                            "description": f"{article['label']}ì—ì„œ {term['label']}ì„ ì •ì˜í•©ë‹ˆë‹¤",
                            "created_at": datetime.utcnow()
                        })

        logger.info(f"ğŸ”— Extracted {len(relationships)} relationships")
        return relationships

    def _is_close_in_text(self, text1: str, text2: str, full_text: str, max_distance: int = 100) -> bool:
        """
        ë‘ í…ìŠ¤íŠ¸ê°€ ì›ë³¸ì—ì„œ ê°€ê¹Œìš´ì§€ í™•ì¸

        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            full_text: ì „ì²´ í…ìŠ¤íŠ¸
            max_distance: ìµœëŒ€ ê±°ë¦¬ (ë¬¸ì ìˆ˜)

        Returns:
            ê°€ê¹Œìš°ë©´ True
        """
        try:
            pos1 = full_text.find(text1[:20])  # ì²« 20ìë¡œ ê²€ìƒ‰
            pos2 = full_text.find(text2[:20])

            if pos1 == -1 or pos2 == -1:
                return False

            return abs(pos1 - pos2) <= max_distance
        except:
            return False


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rule_extractor = RuleBasedEntityExtractor()
