"""
Unstructured.io ê¸°ë°˜ ë³´í—˜ ì•½ê´€ ê³ ê¸‰ ì²­í‚¹ ì‹œìŠ¤í…œ

Features:
- Document layout analysis (ì œëª©, ë³¸ë¬¸, í‘œ, ë¦¬ìŠ¤íŠ¸ êµ¬ë¶„)
- Semantic chunking (ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹)
- Table structure preservation (í‘œ êµ¬ì¡° ì™„ë²½ ë³´ì¡´)
- Hierarchy preservation (ì¥-ì ˆ-ì¡° ê³„ì¸µ êµ¬ì¡° ìœ ì§€)
- Metadata extraction (í˜ì´ì§€, ì¢Œí‘œ, í°íŠ¸ ì •ë³´)
"""
import re
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from loguru import logger

try:
    from unstructured.partition.pdf import partition_pdf
    from unstructured.chunking.title import chunk_by_title
    from unstructured.staging.base import elements_to_json
    UNSTRUCTURED_AVAILABLE = True
except ImportError:
    UNSTRUCTURED_AVAILABLE = False
    logger.warning("Unstructured.io not available - install with: pip install unstructured[pdf]")


class UnstructuredInsuranceChunker:
    """
    Unstructured.io ê¸°ë°˜ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ì²­í‚¹ ì‹œìŠ¤í…œ

    ë³´í—˜ ì•½ê´€ì˜ íŠ¹ìˆ˜í•œ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ìµœì ì˜ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:
    - ì œNì¥ (Chapter): ëŒ€ë¶„ë¥˜
    - ì œNì¡° (Article): ì¤‘ë¶„ë¥˜
    - â‘ â‘¡â‘¢ (Paragraph): ì†Œë¶„ë¥˜
    - í‘œ (Table): ë…ë¦½ ì²­í¬ë¡œ ë³´ì¡´
    """

    # ë³´í—˜ ì•½ê´€ íŒ¨í„´
    CHAPTER_PATTERN = re.compile(r'^ì œ\s*[0-9]+\s*ì¥')  # ì œ1ì¥, ì œ 1 ì¥
    ARTICLE_PATTERN = re.compile(r'^ì œ\s*[0-9]+\s*ì¡°')  # ì œ1ì¡°, ì œ 1 ì¡°
    PARAGRAPH_PATTERN = re.compile(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®]')  # í•­ ë²ˆí˜¸
    LIST_PATTERN = re.compile(r'^\d+\.|^[ê°€-í£]\.|^[-â€¢]')  # 1. ê°€. - â€¢

    def __init__(
        self,
        strategy: str = "hi_res",  # hi_res, fast, ocr_only
        max_characters: int = 1500,
        new_after_n_chars: int = 1200,
        combine_text_under_n_chars: int = 200,
        overlap: int = 100,
    ):
        """
        Initialize Unstructured Insurance Chunker

        Args:
            strategy: PDF íŒŒì‹± ì „ëµ
                - hi_res: ê³ í•´ìƒë„ ë¶„ì„ (ëŠë¦¼, ì •í™•)
                - fast: ë¹ ë¥¸ ë¶„ì„ (ë¹ ë¦„, ì ë‹¹)
                - ocr_only: OCRë§Œ ì‚¬ìš© (ì´ë¯¸ì§€ PDF)
            max_characters: ì²­í¬ ìµœëŒ€ í¬ê¸°
            new_after_n_chars: ì´ í¬ê¸° ì´í›„ ìƒˆ ì²­í¬ ìƒì„±
            combine_text_under_n_chars: ì´ë³´ë‹¤ ì‘ì€ ì²­í¬ëŠ” ë³‘í•©
            overlap: ì²­í¬ ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜ (ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´)
        """
        if not UNSTRUCTURED_AVAILABLE:
            raise ImportError(
                "Unstructured.ioê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "ì„¤ì¹˜: pip install unstructured[pdf] unstructured-inference pdf2image"
            )

        self.strategy = strategy
        self.max_characters = max_characters
        self.new_after_n_chars = new_after_n_chars
        self.combine_text_under_n_chars = combine_text_under_n_chars
        self.overlap = overlap

        logger.info(
            f"UnstructuredInsuranceChunker initialized: "
            f"strategy={strategy}, max_chars={max_characters}"
        )

    def parse_and_chunk(
        self,
        pdf_path: str,
        extract_images: bool = False,
        infer_table_structure: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        PDF íŒŒì‹± + ì²­í‚¹ (Unstructured.io)

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            extract_images: ì´ë¯¸ì§€ ì¶”ì¶œ ì—¬ë¶€
            infer_table_structure: í‘œ êµ¬ì¡° ì¶”ë¡  ì—¬ë¶€

        Returns:
            List[Dict]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
                - chunk_id: ì²­í¬ ID
                - type: ìš”ì†Œ íƒ€ì… (Title, NarrativeText, Table, etc.)
                - content: ì²­í¬ ë‚´ìš©
                - metadata: ë©”íƒ€ë°ì´í„° (í˜ì´ì§€, ì¢Œí‘œ, ê³„ì¸µ ë“±)
        """
        logger.info(f"ğŸ“„ Parsing PDF with Unstructured.io: {pdf_path}")
        logger.info(f"   Strategy: {self.strategy}, Extract images: {extract_images}")

        # 1. PDF íŒŒì‹± (Unstructured.io)
        elements = partition_pdf(
            filename=pdf_path,
            strategy=self.strategy,
            infer_table_structure=infer_table_structure,
            extract_images_in_pdf=extract_images,
            # ê³ ê¸‰ ì˜µì…˜
            include_page_breaks=True,
            languages=["kor", "eng"],  # í•œêµ­ì–´ + ì˜ì–´
        )

        logger.info(f"âœ… Extracted {len(elements)} elements")

        # ìš”ì†Œ íƒ€ì…ë³„ í†µê³„
        element_types = {}
        for elem in elements:
            elem_type = type(elem).__name__
            element_types[elem_type] = element_types.get(elem_type, 0) + 1
        logger.info(f"ğŸ“Š Element types: {element_types}")

        # 2. ë³´í—˜ ì•½ê´€ êµ¬ì¡° ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
        enriched_elements = self._enrich_insurance_structure(elements)

        # 3. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ì œëª© ê¸°ì¤€)
        chunks = chunk_by_title(
            elements=enriched_elements,
            max_characters=self.max_characters,
            new_after_n_chars=self.new_after_n_chars,
            combine_text_under_n_chars=self.combine_text_under_n_chars,
            overlap=self.overlap,
        )

        logger.info(f"âœ… Created {len(chunks)} semantic chunks")

        # 4. ì²­í¬ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        result_chunks = []
        for idx, chunk in enumerate(chunks):
            chunk_dict = self._element_to_dict(chunk, chunk_id=f"chunk_{idx}")
            result_chunks.append(chunk_dict)

        logger.info(f"ğŸ“¦ Final chunk count: {len(result_chunks)}")

        return result_chunks

    def _enrich_insurance_structure(self, elements: List[Any]) -> List[Any]:
        """
        ë³´í—˜ ì•½ê´€ì˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€

        ê³„ì¸µ:
        - level_0: ì œNì¥ (Chapter)
        - level_1: ì œNì¡° (Article)
        - level_2: â‘ â‘¡â‘¢ (Paragraph)
        - level_3: 1.ê°€.- (List item)
        """
        current_chapter = None
        current_article = None
        current_paragraph = None

        for elem in elements:
            text = str(elem).strip()

            # ì¥ ê°ì§€
            if self.CHAPTER_PATTERN.match(text):
                current_chapter = text
                current_article = None
                current_paragraph = None
                elem.metadata.category = "chapter"
                elem.metadata.hierarchy_level = 0
                elem.metadata.chapter = current_chapter

            # ì¡° ê°ì§€
            elif self.ARTICLE_PATTERN.match(text):
                current_article = text
                current_paragraph = None
                elem.metadata.category = "article"
                elem.metadata.hierarchy_level = 1
                elem.metadata.chapter = current_chapter
                elem.metadata.article = current_article

            # í•­ ê°ì§€
            elif self.PARAGRAPH_PATTERN.match(text):
                current_paragraph = text[:2]  # â‘  ë¶€ë¶„ë§Œ
                elem.metadata.category = "paragraph"
                elem.metadata.hierarchy_level = 2
                elem.metadata.chapter = current_chapter
                elem.metadata.article = current_article
                elem.metadata.paragraph = current_paragraph

            # ë¦¬ìŠ¤íŠ¸ ê°ì§€
            elif self.LIST_PATTERN.match(text):
                elem.metadata.category = "list_item"
                elem.metadata.hierarchy_level = 3
                elem.metadata.chapter = current_chapter
                elem.metadata.article = current_article
                elem.metadata.paragraph = current_paragraph

            # ì¼ë°˜ í…ìŠ¤íŠ¸
            else:
                # ì»¨í…ìŠ¤íŠ¸ ìƒì†
                elem.metadata.chapter = current_chapter
                elem.metadata.article = current_article
                elem.metadata.paragraph = current_paragraph

        return elements

    def _element_to_dict(self, element: Any, chunk_id: str) -> Dict[str, Any]:
        """
        Unstructured elementë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜

        Returns:
            Dict with:
                - chunk_id: ì²­í¬ ID
                - type: ìš”ì†Œ íƒ€ì…
                - content: ë‚´ìš©
                - metadata: í˜ì´ì§€, ì¢Œí‘œ, ê³„ì¸µ ì •ë³´
        """
        # ê¸°ë³¸ ì •ë³´
        chunk = {
            "chunk_id": chunk_id,
            "type": type(element).__name__,
            "content": str(element),
        }

        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = {}
        if hasattr(element, "metadata"):
            meta = element.metadata

            # í˜ì´ì§€ ì •ë³´
            if hasattr(meta, "page_number"):
                metadata["page"] = meta.page_number

            # ì¢Œí‘œ ì •ë³´ (ë°”ìš´ë”© ë°•ìŠ¤)
            if hasattr(meta, "coordinates"):
                coords = meta.coordinates
                if coords:
                    metadata["coordinates"] = {
                        "points": coords.points if hasattr(coords, "points") else None,
                        "system": coords.system if hasattr(coords, "system") else None,
                    }

            # ë³´í—˜ ì•½ê´€ ê³„ì¸µ ì •ë³´
            if hasattr(meta, "category"):
                metadata["category"] = meta.category
            if hasattr(meta, "hierarchy_level"):
                metadata["hierarchy_level"] = meta.hierarchy_level
            if hasattr(meta, "chapter"):
                metadata["chapter"] = meta.chapter
            if hasattr(meta, "article"):
                metadata["article"] = meta.article
            if hasattr(meta, "paragraph"):
                metadata["paragraph"] = meta.paragraph

            # ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
            if hasattr(meta, "filename"):
                metadata["filename"] = meta.filename
            if hasattr(meta, "file_directory"):
                metadata["file_directory"] = meta.file_directory
            if hasattr(meta, "languages"):
                metadata["languages"] = meta.languages

        chunk["metadata"] = metadata

        return chunk

    def analyze_document_structure(self, pdf_path: str) -> Dict[str, Any]:
        """
        ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ì²­í‚¹ ì „ ë¯¸ë¦¬ë³´ê¸°)

        Returns:
            Dict with:
                - total_pages: ì´ í˜ì´ì§€ ìˆ˜
                - element_count: ìš”ì†Œ ìˆ˜
                - element_types: ìš”ì†Œ íƒ€ì…ë³„ ê°œìˆ˜
                - chapters: ì¥ ëª©ë¡
                - articles: ì¡° ëª©ë¡
                - tables: í‘œ ê°œìˆ˜
        """
        logger.info(f"ğŸ“Š Analyzing document structure: {pdf_path}")

        # PDF íŒŒì‹±
        elements = partition_pdf(
            filename=pdf_path,
            strategy="fast",  # ë¹ ë¥¸ ë¶„ì„
            infer_table_structure=True,
            languages=["kor", "eng"],
        )

        # í†µê³„ ìˆ˜ì§‘
        element_types = {}
        chapters = []
        articles = []
        tables = 0
        pages = set()

        for elem in elements:
            # ìš”ì†Œ íƒ€ì…
            elem_type = type(elem).__name__
            element_types[elem_type] = element_types.get(elem_type, 0) + 1

            # í˜ì´ì§€
            if hasattr(elem.metadata, "page_number"):
                pages.add(elem.metadata.page_number)

            # ë‚´ìš© ë¶„ì„
            text = str(elem).strip()

            if self.CHAPTER_PATTERN.match(text):
                chapters.append(text)
            elif self.ARTICLE_PATTERN.match(text):
                articles.append(text)
            elif elem_type == "Table":
                tables += 1

        analysis = {
            "total_pages": len(pages),
            "element_count": len(elements),
            "element_types": element_types,
            "chapters": chapters,
            "articles": articles,
            "tables": tables,
        }

        logger.info(f"âœ… Analysis complete:")
        logger.info(f"   Pages: {analysis['total_pages']}")
        logger.info(f"   Elements: {analysis['element_count']}")
        logger.info(f"   Chapters: {len(chapters)}")
        logger.info(f"   Articles: {len(articles)}")
        logger.info(f"   Tables: {tables}")

        return analysis


# Singleton instance
_chunker: Optional[UnstructuredInsuranceChunker] = None


def get_unstructured_chunker(
    strategy: str = "hi_res",
    **kwargs
) -> UnstructuredInsuranceChunker:
    """Get or create singleton chunker instance"""
    global _chunker
    if _chunker is None:
        _chunker = UnstructuredInsuranceChunker(strategy=strategy, **kwargs)
    return _chunker


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    import sys

    if len(sys.argv) < 2:
        print("Usage: python unstructured_chunker.py <pdf_path>")
        sys.exit(1)

    pdf_path = sys.argv[1]

    print("=" * 70)
    print("ğŸ§ª Unstructured Insurance Chunker Test")
    print("=" * 70)

    # ì²­í‚¹
    chunker = get_unstructured_chunker(strategy="fast")

    # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
    print("\nğŸ“Š Document Structure Analysis:")
    analysis = chunker.analyze_document_structure(pdf_path)

    # ì²­í‚¹ ì‹¤í–‰
    print("\nğŸ“¦ Chunking:")
    chunks = chunker.parse_and_chunk(pdf_path)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… Created {len(chunks)} chunks")
    print("\nì²« 3ê°œ ì²­í¬:")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"Type: {chunk['type']}")
        print(f"Content: {chunk['content'][:200]}...")
        print(f"Metadata: {chunk['metadata']}")

    print("\n" + "=" * 70)
