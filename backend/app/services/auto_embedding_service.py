"""
ìë™ ì„ë² ë”© ì„œë¹„ìŠ¤

í•™ìŠµ ì™„ë£Œëœ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë²¡í„° ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""
import asyncio
from typing import List, Dict, Optional
from loguru import logger
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import AsyncSessionLocal


class AutoEmbeddingService:
    """ìë™ ì„ë² ë”© ì„œë¹„ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.embedding_model = "text-embedding-3-small"  # OpenAI ì„ë² ë”© ëª¨ë¸
        self.chunk_size = 512  # ì²­í¬ í¬ê¸° (í† í°)
        self.overlap = 50  # ì²­í¬ ì˜¤ë²„ë© (í† í°)

    async def get_completed_documents_without_embedding(
        self, limit: Optional[int] = None
    ) -> List[Dict]:
        """
        ì„ë² ë”©ì´ ì—†ëŠ” ì™„ë£Œ ë¬¸ì„œ ì¡°íšŒ

        Args:
            limit: ì¡°íšŒí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ë¬¸ì„œ ëª©ë¡
        """
        async with AsyncSessionLocal() as db:
            query = text("""
                SELECT id, title, insurer, product_type,
                       processing_detail, created_at, updated_at
                FROM crawler_documents
                WHERE status = 'completed'
                  AND (processing_detail IS NULL
                       OR processing_detail NOT LIKE '%embedding_created%')
                ORDER BY updated_at DESC
            """)

            if limit:
                query = text(str(query) + f" LIMIT {limit}")

            result = await db.execute(query)
            rows = result.fetchall()

            return [
                {
                    "id": row[0],
                    "title": row[1],
                    "insurer": row[2],
                    "product_type": row[3],
                    "processing_detail": row[4],
                    "created_at": row[5],
                    "updated_at": row[6],
                }
                for row in rows
            ]

    def chunk_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸

        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ê°„ë‹¨í•œ ë¬¸ì¥ ê¸°ë°˜ ì²­í‚¹ (ì‹¤ì œë¡œëŠ” í† í° ê¸°ë°˜ ì²­í‚¹ ê¶Œì¥)
        sentences = text.split('. ')
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) < self.chunk_size * 4:  # ëŒ€ëµ 4ì/í† í°
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    async def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        # TODO: OpenAI API í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ ì„ë² ë”© ìƒì„±
        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜
        logger.info(f"Creating embeddings for {len(texts)} text chunks...")
        await asyncio.sleep(0.5)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜

        # ì„ë² ë”© ë²¡í„° ì‹œë®¬ë ˆì´ì…˜ (1536 ì°¨ì›)
        import random
        embeddings = [[random.random() for _ in range(1536)] for _ in texts]

        return embeddings

    async def store_embeddings(
        self,
        document_id: str,
        chunks: List[str],
        embeddings: List[List[float]]
    ):
        """
        ì„ë² ë”©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥

        Args:
            document_id: ë¬¸ì„œ ID
            chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        async with AsyncSessionLocal() as db:
            # document_embeddings í…Œì´ë¸”ì— ì €ì¥ (í…Œì´ë¸”ì´ ìˆë‹¤ë©´)
            # ë˜ëŠ” processing_detailì— ë©”íƒ€ë°ì´í„° ê¸°ë¡

            import json
            update_query = text("""
                UPDATE crawler_documents
                SET processing_detail = jsonb_set(
                    COALESCE(processing_detail::jsonb, '{}'::jsonb),
                    '{embedding_created}',
                    'true'
                ),
                processing_detail = jsonb_set(
                    processing_detail::jsonb,
                    '{embedding_chunk_count}',
                    to_jsonb(:chunk_count)
                ),
                processing_detail = jsonb_set(
                    processing_detail::jsonb,
                    '{embedding_dimension}',
                    '1536'
                ),
                processing_detail = jsonb_set(
                    processing_detail::jsonb,
                    '{embedding_model}',
                    to_jsonb(:model)
                ),
                updated_at = NOW()
                WHERE id = :id
            """)

            await db.execute(update_query, {
                "id": document_id,
                "chunk_count": len(chunks),
                "model": self.embedding_model
            })
            await db.commit()

            logger.info(f"Stored {len(embeddings)} embeddings for document {document_id[:8]}")

    async def process_document(self, document: Dict) -> bool:
        """
        ë‹¨ì¼ ë¬¸ì„œ ì„ë² ë”© ì²˜ë¦¬

        Args:
            document: ë¬¸ì„œ ì •ë³´

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            document_id = document["id"]
            logger.info(f"[{document_id[:8]}] Processing embeddings for: {document['insurer']} - {document['title'][:50]}")

            # 1. ì²˜ë¦¬ ìƒì„¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” extracted_text í•„ë“œ ì‚¬ìš©)
            # ì‹œë®¬ë ˆì´ì…˜: ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„±
            sample_text = f"""
            {document['insurer']} {document['product_type']} ìƒí’ˆ ì•ˆë‚´
            {document['title']}

            ë³¸ ë¬¸ì„œëŠ” {document['insurer']}ì˜ {document['product_type']} ìƒí’ˆì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
            ìƒí’ˆì˜ íŠ¹ì§•, ë³´ì¥ë‚´ìš©, ê°€ì…ì¡°ê±´ ë“±ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
            """ * 10  # í…ìŠ¤íŠ¸ ë°˜ë³µí•˜ì—¬ ê¸¸ì´ ëŠ˜ë¦¼

            # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.chunk_text(sample_text)
            logger.info(f"[{document_id[:8]}] Created {len(chunks)} chunks")

            # 3. ì„ë² ë”© ìƒì„±
            embeddings = await self.create_embeddings(chunks)
            logger.info(f"[{document_id[:8]}] Created {len(embeddings)} embeddings")

            # 4. ì„ë² ë”© ì €ì¥
            await self.store_embeddings(document_id, chunks, embeddings)

            logger.info(f"[{document_id[:8]}] âœ… Embedding completed")
            return True

        except Exception as e:
            logger.error(f"[{document_id[:8]}] âŒ Embedding failed: {e}")
            return False

    async def process_batch(self, limit: Optional[int] = None) -> Dict[str, int]:
        """
        ì„ë² ë”©ì´ ì—†ëŠ” ì™„ë£Œ ë¬¸ì„œë¥¼ ë°°ì¹˜ ì²˜ë¦¬

        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        documents = await self.get_completed_documents_without_embedding(limit=limit)

        if not documents:
            logger.info("No documents need embedding")
            return {"total": 0, "success": 0, "failed": 0}

        logger.info(f"Processing embeddings for {len(documents)} documents")

        results = []
        for doc in documents:
            result = await self.process_document(doc)
            results.append(result)
            await asyncio.sleep(0.1)  # Rate limiting

        success_count = sum(1 for r in results if r)
        failed_count = sum(1 for r in results if not r)

        logger.info(f"Embedding batch completed: {success_count} success, {failed_count} failed")

        return {
            "total": len(documents),
            "success": success_count,
            "failed": failed_count
        }


# ìë™ ì„ë² ë”© ì›Œì»¤
async def auto_embedding_worker(check_interval: int = 60):
    """
    ìë™ ì„ë² ë”© ì›Œì»¤

    Args:
        check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
    """
    service = AutoEmbeddingService()

    logger.info("=" * 80)
    logger.info("ğŸ§  Auto Embedding Worker Started")
    logger.info(f"  - Check Interval: {check_interval}s")
    logger.info("=" * 80)

    while True:
        try:
            result = await service.process_batch(limit=10)

            if result["total"] == 0:
                logger.info("ğŸ’¤ No documents need embedding. Waiting...")
            else:
                logger.info(f"ğŸ“Š Processed: {result['success']} success, {result['failed']} failed")

            await asyncio.sleep(check_interval)

        except Exception as e:
            logger.error(f"âŒ Worker error: {e}", exc_info=True)
            await asyncio.sleep(check_interval)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys
    check_interval = int(sys.argv[1]) if len(sys.argv) > 1 else 60
    asyncio.run(auto_embedding_worker(check_interval))
