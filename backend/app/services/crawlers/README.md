# Insurance Company Crawlers

ë³´í—˜ì‚¬ë³„ ì•½ê´€ ìˆ˜ì§‘ì„ ìœ„í•œ ìœ ì—°í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ

## ğŸ“‹ ê°œìš”

ê° ë³´í—˜ì‚¬ë§ˆë‹¤ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°, ì•½ê´€ ì œê³µ ë°©ì‹, ì¸ì¦ ë°©ë²•ì´ ë‹¤ë¦…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ **Strategy Pattern**ê³¼ **Factory Pattern**ì„ ì‚¬ìš©í•˜ì—¬ ë³´í—˜ì‚¬ë³„ë¡œ ë‹¤ë¥¸ í¬ë¡¤ë§ ì „ëµì„ ì‰½ê²Œ ì¶”ê°€í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
crawlers/
â”œâ”€â”€ base_crawler.py           # ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”œâ”€â”€ samsung_life_crawler.py   # ì‚¼ì„±ìƒëª… í¬ë¡¤ëŸ¬
â”œâ”€â”€ kb_insurance_crawler.py   # KBì†í•´ë³´í—˜ í¬ë¡¤ëŸ¬
â”œâ”€â”€ crawler_factory.py        # í¬ë¡¤ëŸ¬ ìƒì„± íŒ©í† ë¦¬
â”œâ”€â”€ crawler_registry.py       # í¬ë¡¤ëŸ¬ ë“±ë¡/ê´€ë¦¬
â”œâ”€â”€ config_loader.py          # YAML ì„¤ì • ë¡œë”
â””â”€â”€ configs/                  # ë³´í—˜ì‚¬ë³„ ì„¤ì • íŒŒì¼
    â”œâ”€â”€ samsung_life.yaml
    â””â”€â”€ kb_insurance.yaml
```

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### 1. ë³´í—˜ì‚¬ë³„ í¬ë¡¤ë§ ì „ëµ

ê° ë³´í—˜ì‚¬ë§ˆë‹¤ ë‹¤ë¥¸ í¬ë¡¤ë§ ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤:

- **Static HTML**: requests + BeautifulSoup
- **Dynamic JS**: Playwright/Selenium (JavaScript ë Œë”ë§ í•„ìš”)
- **API**: REST API ì§ì ‘ í˜¸ì¶œ
- **Hybrid**: ë³µí•© ë°©ì‹ (API + ìŠ¤í¬ë˜í•‘)

### 2. ìœ ì—°í•œ ì„¤ì • ê´€ë¦¬

YAML íŒŒì¼ë¡œ ê° ë³´í—˜ì‚¬ì˜ ì„¤ì •ì„ ê´€ë¦¬:

```yaml
insurer_code: samsung_life
insurer_name: ì‚¼ì„±ìƒëª…
base_url: https://www.samsunglife.com
crawl_method: dynamic_js
product_list_selector: .product-list .product-item
request_delay: 1.5
```

### 3. ì‰¬ìš´ í™•ì¥

ìƒˆë¡œìš´ ë³´í—˜ì‚¬ ì¶”ê°€ê°€ ê°„ë‹¨í•©ë‹ˆë‹¤:

1. `BaseInsurerCrawler`ë¥¼ ìƒì†í•œ ìƒˆ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì‘ì„±
2. ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
3. ì„¤ì • YAML íŒŒì¼ ì‘ì„± (ì„ íƒì‚¬í•­)

## ğŸ“– ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©

```python
from app.services.crawlers import get_crawler

# 1. í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
crawler = get_crawler("samsung_life")

# 2. ì—°ê²° í…ŒìŠ¤íŠ¸
is_connected = await crawler.test_connection()

# 3. ìƒí’ˆ ëª©ë¡ ì¡°íšŒ
products = await crawler.get_product_list(category="ì•”ë³´í—˜")

# 4. íŠ¹ì • ìƒí’ˆì˜ ì•½ê´€ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
metadata = await crawler.get_policy_metadata(product_id="12345")

# 5. ì•½ê´€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
filepath = await crawler.download_policy(metadata, save_path="/tmp/policies")

# 6. ì „ì²´ ì•½ê´€ í¬ë¡¤ë§
all_metadata = await crawler.crawl_all_policies(
    categories=["ì•”ë³´í—˜", "ì‹¤ì†ë³´í—˜"],
    save_dir="/tmp/policies"
)
```

### ëª¨ë“  ë³´í—˜ì‚¬ í¬ë¡¤ë§

```python
from app.services.crawlers import get_all_crawlers

# ëª¨ë“  ë“±ë¡ëœ í¬ë¡¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
crawlers = get_all_crawlers()

for insurer_code, crawler in crawlers.items():
    print(f"Crawling {crawler.config.insurer_name}...")

    try:
        metadata_list = await crawler.crawl_all_policies()
        print(f"  âœ… Found {len(metadata_list)} policies")
    except Exception as e:
        print(f"  âŒ Error: {e}")
```

### ì‚¬ìš© ê°€ëŠ¥í•œ ë³´í—˜ì‚¬ í™•ì¸

```python
from app.services.crawlers import get_available_insurers, is_insurer_supported

# ì§€ì›ë˜ëŠ” ë³´í—˜ì‚¬ ëª©ë¡
insurers = get_available_insurers()
print(f"Available insurers: {', '.join(insurers)}")

# íŠ¹ì • ë³´í—˜ì‚¬ ì§€ì› ì—¬ë¶€
if is_insurer_supported("samsung_life"):
    print("ì‚¼ì„±ìƒëª… í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
```

### ì„¤ì • íŒŒì¼ ë¡œë“œ

```python
from app.services.crawlers.config_loader import load_insurer_config

# YAML íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
config = load_insurer_config("samsung_life")

print(f"Insurer: {config.insurer_name}")
print(f"Base URL: {config.base_url}")
print(f"Crawl Method: {config.crawl_method}")
```

## ğŸ†• ìƒˆë¡œìš´ ë³´í—˜ì‚¬ í¬ë¡¤ëŸ¬ ì¶”ê°€í•˜ê¸°

### Step 1: í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì‘ì„±

```python
# app/services/crawlers/hyundai_marine_crawler.py

from app.services.crawlers.base_crawler import (
    BaseInsurerCrawler,
    InsurerConfig,
    PolicyMetadata,
    CrawlMethod,
)

class HyundaiMarineCrawler(BaseInsurerCrawler):
    """í˜„ëŒ€í•´ìƒ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        config = InsurerConfig(
            insurer_code="hyundai_marine",
            insurer_name="í˜„ëŒ€í•´ìƒ",
            base_url="https://www.hi.co.kr",
            crawl_method=CrawlMethod.STATIC_HTML,
            # ... ê¸°íƒ€ ì„¤ì •
        )
        super().__init__(config)

    async def get_product_list(self, category=None):
        # êµ¬í˜„
        pass

    async def get_policy_metadata(self, product_id):
        # êµ¬í˜„
        pass

    async def download_policy(self, metadata, save_path):
        # êµ¬í˜„
        pass
```

### Step 2: ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡

```python
# app/services/crawlers/crawler_registry.py (ìˆ˜ì •)

def _auto_register_crawlers():
    try:
        from app.services.crawlers.samsung_life_crawler import SamsungLifeCrawler
        from app.services.crawlers.kb_insurance_crawler import KBInsuranceCrawler
        from app.services.crawlers.hyundai_marine_crawler import HyundaiMarineCrawler  # ì¶”ê°€

        register_crawler("samsung_life", SamsungLifeCrawler)
        register_crawler("kb_insurance", KBInsuranceCrawler)
        register_crawler("hyundai_marine", HyundaiMarineCrawler)  # ì¶”ê°€

    except Exception as e:
        logger.error(f"Failed to auto-register crawlers: {e}")
```

### Step 3: ì„¤ì • íŒŒì¼ ì‘ì„± (ì„ íƒ)

```yaml
# app/services/crawlers/configs/hyundai_marine.yaml

insurer_code: hyundai_marine
insurer_name: í˜„ëŒ€í•´ìƒ
base_url: https://www.hi.co.kr
crawl_method: static_html
product_list_selector: .product-item
request_delay: 1.0
```

## ğŸ”§ ì„¤ì • ì˜µì…˜

### CrawlMethod

- `static_html`: ì •ì  HTML í˜ì´ì§€ (requests + BeautifulSoup)
- `dynamic_js`: JavaScript ë Œë”ë§ í•„ìš” (Playwright)
- `api`: REST API ì§ì ‘ í˜¸ì¶œ
- `hybrid`: ë³µí•© ë°©ì‹

### AuthMethod

- `none`: ì¸ì¦ ë¶ˆí•„ìš”
- `basic`: Basic Authentication
- `oauth`: OAuth 2.0
- `session`: ì„¸ì…˜ ê¸°ë°˜ ì¸ì¦
- `api_key`: API í‚¤ ì¸ì¦

### ì£¼ìš” ì„¤ì • í•­ëª©

| í•­ëª© | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `insurer_code` | ë³´í—˜ì‚¬ ì½”ë“œ | `samsung_life` |
| `insurer_name` | ë³´í—˜ì‚¬ ì´ë¦„ | `ì‚¼ì„±ìƒëª…` |
| `base_url` | ê¸°ë³¸ URL | `https://...` |
| `crawl_method` | í¬ë¡¤ë§ ë°©ë²• | `dynamic_js` |
| `product_list_selector` | ìƒí’ˆ ëª©ë¡ CSS ì…€ë ‰í„° | `.product-list` |
| `request_delay` | ìš”ì²­ ê°„ ì§€ì—° (ì´ˆ) | `1.5` |
| `has_pagination` | í˜ì´ì§€ë„¤ì´ì…˜ ì—¬ë¶€ | `true` |
| `max_pages` | ìµœëŒ€ í˜ì´ì§€ ìˆ˜ | `50` |

## ğŸ“Š PolicyMetadata êµ¬ì¡°

í¬ë¡¤ë§ëœ ì•½ê´€ ë©”íƒ€ë°ì´í„°:

```python
@dataclass
class PolicyMetadata:
    insurer_code: str             # ë³´í—˜ì‚¬ ì½”ë“œ
    insurer_name: str             # ë³´í—˜ì‚¬ ì´ë¦„
    product_name: str             # ìƒí’ˆëª…
    product_code: Optional[str]   # ìƒí’ˆ ì½”ë“œ
    category: Optional[str]       # ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "ì•”ë³´í—˜")
    sub_category: Optional[str]   # í•˜ìœ„ ì¹´í…Œê³ ë¦¬
    download_url: Optional[str]   # ë‹¤ìš´ë¡œë“œ URL
    effective_date: Optional[str] # ì‹œí–‰ì¼
    version: Optional[str]        # ë²„ì „
    file_type: str = "pdf"        # íŒŒì¼ ìœ í˜•
    file_size: Optional[int]      # íŒŒì¼ í¬ê¸°
    additional_info: Optional[Dict]  # ì¶”ê°€ ì •ë³´
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```python
# íŠ¹ì • ë³´í—˜ì‚¬ ì—°ê²° í…ŒìŠ¤íŠ¸
crawler = get_crawler("samsung_life")
success = await crawler.test_connection()

if success:
    print("âœ… Connection successful")
else:
    print("âŒ Connection failed")

# ë©”íƒ€ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
metadata = await crawler.get_policy_metadata("12345")
is_valid = crawler.validate_metadata(metadata)
```

## ğŸ¨ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### API ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš©

```python
from fastapi import APIRouter, HTTPException
from app.services.crawlers import get_crawler, get_available_insurers

router = APIRouter()

@router.get("/insurers")
async def list_insurers():
    """ì§€ì›ë˜ëŠ” ë³´í—˜ì‚¬ ëª©ë¡"""
    return {"insurers": get_available_insurers()}

@router.post("/crawl/{insurer_code}")
async def crawl_insurer(insurer_code: str, category: Optional[str] = None):
    """íŠ¹ì • ë³´í—˜ì‚¬ í¬ë¡¤ë§"""
    try:
        crawler = get_crawler(insurer_code)
        metadata_list = await crawler.crawl_all_policies(
            categories=[category] if category else None
        )

        return {
            "status": "success",
            "insurer": insurer_code,
            "policies_found": len(metadata_list),
            "metadata": [vars(m) for m in metadata_list]
        }

    except KeyError:
        raise HTTPException(status_code=404, detail=f"Insurer not found: {insurer_code}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Celery Taskì—ì„œ ì‚¬ìš©

```python
from celery import shared_task
from app.services.crawlers import get_crawler

@shared_task
def crawl_insurer_task(insurer_code: str, categories: List[str] = None):
    """ë¹„ë™ê¸° í¬ë¡¤ë§ ì‘ì—…"""
    import asyncio

    async def _crawl():
        crawler = get_crawler(insurer_code)
        return await crawler.crawl_all_policies(categories=categories)

    metadata_list = asyncio.run(_crawl())

    return {
        "insurer": insurer_code,
        "policies": len(metadata_list),
    }
```

## ğŸ’¡ Best Practices

1. **Rate Limiting ì¤€ìˆ˜**: ê° ë³´í—˜ì‚¬ì˜ `request_delay` ì„¤ì •ì„ ì§€ì¼œì£¼ì„¸ìš”
2. **ì—ëŸ¬ í•¸ë“¤ë§**: try-exceptë¡œ ê°œë³„ ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”
3. **ë¡œê¹…**: ê° ë‹¨ê³„ë§ˆë‹¤ ë¡œê·¸ë¥¼ ë‚¨ê²¨ ë””ë²„ê¹…ì„ ìš©ì´í•˜ê²Œ í•˜ì„¸ìš”
4. **ì„¤ì • ë¶„ë¦¬**: í•˜ë“œì½”ë”© ëŒ€ì‹  YAML ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”
5. **í…ŒìŠ¤íŠ¸**: ìƒˆ í¬ë¡¤ëŸ¬ ì¶”ê°€ ì‹œ `test_connection()`ìœ¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”

## ğŸ”’ ì£¼ì˜ì‚¬í•­

- **ì›¹ì‚¬ì´íŠ¸ ë³€ê²½**: ë³´í—˜ì‚¬ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ ì…€ë ‰í„°ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤
- **ì´ìš© ì•½ê´€**: ê° ë³´í—˜ì‚¬ì˜ í¬ë¡¤ë§ ì •ì±…ì„ í™•ì¸í•˜ì„¸ìš”
- **ê°œì¸ì •ë³´**: ì•½ê´€ ë¬¸ì„œ ì™¸ì— ê°œì¸ì •ë³´ëŠ” ìˆ˜ì§‘í•˜ì§€ ë§ˆì„¸ìš”
- **ì„œë²„ ë¶€í•˜**: rate limitingì„ ì¤€ìˆ˜í•˜ì—¬ ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ë§ˆì„¸ìš”

## ğŸ“ TODO

- [ ] ì¶”ê°€ ë³´í—˜ì‚¬ í¬ë¡¤ëŸ¬ êµ¬í˜„ (í•œí™”ìƒëª…, AIA, ë©”ë¦¬ì¸ í™”ì¬ ë“±)
- [ ] Playwright í†µí•© (JavaScript ë Œë”ë§)
- [ ] í¬ë¡¤ë§ ê²°ê³¼ ìºì‹±
- [ ] í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§ (ì£¼ê¸°ì  ì—…ë°ì´íŠ¸)
- [ ] ì•½ê´€ ë³€ê²½ ê°ì§€
- [ ] ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  ì¶”ì 

## ğŸ“š ì°¸ê³ 

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Playwright Python](https://playwright.dev/python/)
- [Strategy Pattern](https://refactoring.guru/design-patterns/strategy)
- [Factory Pattern](https://refactoring.guru/design-patterns/factory-method)
