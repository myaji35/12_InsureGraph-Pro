"""
Smart Insurance Document Chunker
ë³´í—˜ ì•½ê´€ íŠ¹í™” ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì‹œìŠ¤í…œ (í…Œì´ë¸” ë³´ì¡´)

Unstructured.io ê°œë… ê¸°ë°˜, pdfplumber êµ¬í˜„
"""
import re
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
import pdfplumber


class SmartInsuranceChunker:
    """ë³´í—˜ ì•½ê´€ íŠ¹í™” ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (í…Œì´ë¸” ë³´ì¡´)"""

    def __init__(
        self,
        max_chars: int = 1500,
        target_chars: int = 1200,
        min_chars: int = 200
    ):
        """
        Initialize Smart Chunker

        Args:
            max_chars: ì²­í¬ ìµœëŒ€ í¬ê¸°
            target_chars: ì²­í¬ ëª©í‘œ í¬ê¸°
            min_chars: ì²­í¬ ìµœì†Œ í¬ê¸° (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ë³‘í•©)
        """
        self.max_chars = max_chars
        self.target_chars = target_chars
        self.min_chars = min_chars

    def parse_and_chunk(
        self,
        pdf_path: str,
        output_dir: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        PDF íŒŒì‹± + ì²­í‚¹ (í‘œ ë³´ì¡´)

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒ)

        Returns:
            List[Dict]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ“„ PDF íŒŒì‹± ì¤‘: {pdf_path}")

        # PDF íŒŒì‹± (í˜ì´ì§€ë³„)
        elements = self._extract_elements(pdf_path)

        logger.info(f"âœ… {len(elements)}ê°œ ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ")

        # ìš”ì†Œ íƒ€ì…ë³„ í†µê³„
        element_types = {}
        for elem in elements:
            elem_type = elem['type']
            element_types[elem_type] = element_types.get(elem_type, 0) + 1
        logger.info(f"ğŸ“Š ìš”ì†Œ êµ¬ì„±: {element_types}")

        # ì œëª© ê¸°ë°˜ ì²­í‚¹ (í‘œ ë³´ì¡´)
        chunks = self._chunk_by_structure(elements)

        logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # ì €ì¥ (ì„ íƒ)
        if output_dir:
            self._save_chunks(chunks, output_dir)

        return chunks

    def _extract_elements(self, pdf_path: str) -> List[Dict[str, Any]]:
        """
        PDFì—ì„œ ìš”ì†Œ ì¶”ì¶œ (í…ìŠ¤íŠ¸ + í‘œ)

        Returns:
            List[Dict]: ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
                - type: 'title', 'text', 'table', 'list'
                - content: ë‚´ìš©
                - page: í˜ì´ì§€ ë²ˆí˜¸
                - metadata: ì¶”ê°€ ì •ë³´
        """
        elements = []

        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, start=1):
                # 1. í‘œ ì¶”ì¶œ
                tables = page.extract_tables()
                if tables:
                    for table_idx, table in enumerate(tables):
                        if not table:
                            continue

                        # í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        table_text = self._table_to_text(table)

                        elements.append({
                            'type': 'table',
                            'content': table_text,
                            'page': page_num,
                            'metadata': {
                                'table_index': table_idx,
                                'rows': len(table),
                                'cols': len(table[0]) if table else 0
                            }
                        })

                # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = page.extract_text()
                if not text:
                    continue

                # í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ìš”ì†Œë¡œ ë¶„í• 
                text_elements = self._parse_text_structure(text, page_num)
                elements.extend(text_elements)

        # í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        elements.sort(key=lambda x: (x['page'], x.get('order', 0)))

        return elements

    def _table_to_text(self, table: List[List[str]]) -> str:
        """í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼)"""
        if not table:
            return ""

        lines = []

        # í—¤ë” (ì²« ë²ˆì§¸ í–‰)
        header = table[0]
        lines.append("| " + " | ".join(str(cell or "") for cell in header) + " |")
        lines.append("|" + "|".join(["---"] * len(header)) + "|")

        # ë°ì´í„° í–‰
        for row in table[1:]:
            lines.append("| " + " | ".join(str(cell or "") for cell in row) + " |")

        return "\n".join(lines)

    def _parse_text_structure(
        self,
        text: str,
        page_num: int
    ) -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ìš”ì†Œë¡œ ë¶„í• 

        ë³´í—˜ ì•½ê´€ êµ¬ì¡°:
        - ì œNì¥ (Chapter)
        - ì œNì¡° (Article)
        - ì¼ë°˜ í…ìŠ¤íŠ¸
        """
        elements = []
        lines = text.split('\n')

        current_section = []
        current_type = 'text'
        order = 0

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì¥ íŒ¨í„´ ê°ì§€
            if self._is_chapter(line):
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section:
                    elements.append({
                        'type': current_type,
                        'content': '\n'.join(current_section),
                        'page': page_num,
                        'order': order,
                        'metadata': {}
                    })
                    order += 1

                # ìƒˆ ì¥ ì‹œì‘
                current_section = [line]
                current_type = 'chapter'

            # ì¡° íŒ¨í„´ ê°ì§€
            elif self._is_article(line):
                # ì´ì „ ì„¹ì…˜ ì €ì¥
                if current_section:
                    elements.append({
                        'type': current_type,
                        'content': '\n'.join(current_section),
                        'page': page_num,
                        'order': order,
                        'metadata': {}
                    })
                    order += 1

                # ìƒˆ ì¡° ì‹œì‘
                current_section = [line]
                current_type = 'article'

            # ë¦¬ìŠ¤íŠ¸ í•­ëª© ê°ì§€
            elif self._is_list_item(line):
                if current_type != 'list':
                    # ì´ì „ ì„¹ì…˜ ì €ì¥
                    if current_section:
                        elements.append({
                            'type': current_type,
                            'content': '\n'.join(current_section),
                            'page': page_num,
                            'order': order,
                            'metadata': {}
                        })
                        order += 1

                    current_section = [line]
                    current_type = 'list'
                else:
                    current_section.append(line)

            # ì¼ë°˜ í…ìŠ¤íŠ¸
            else:
                current_section.append(line)

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section:
            elements.append({
                'type': current_type,
                'content': '\n'.join(current_section),
                'page': page_num,
                'order': order,
                'metadata': {}
            })

        return elements

    def _is_chapter(self, line: str) -> bool:
        """ì¥ íŒ¨í„´ ê°ì§€: ì œ1ì¥, ì œ2ì¥ ë“±"""
        return bool(re.match(r'^ì œ\s*\d+\s*ì¥', line))

    def _is_article(self, line: str) -> bool:
        """ì¡° íŒ¨í„´ ê°ì§€: ì œ1ì¡°, ì œ2ì¡° ë“±"""
        return bool(re.match(r'^ì œ\s*\d+\s*ì¡°', line))

    def _is_list_item(self, line: str) -> bool:
        """ë¦¬ìŠ¤íŠ¸ í•­ëª© ê°ì§€: 1., ê°€., â‘  ë“±"""
        patterns = [
            r'^\d+\.',  # 1., 2., 3.
            r'^[ê°€-í£]\.',  # ê°€., ë‚˜., ë‹¤.
            r'^[â‘ -â‘³]',  # â‘ , â‘¡, â‘¢
            r'^-\s',  # - í•­ëª©
            r'^\*\s',  # * í•­ëª©
        ]
        return any(re.match(pattern, line) for pattern in patterns)

    def _chunk_by_structure(
        self,
        elements: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ (ì œëª© ê¸°ë°˜, í‘œ ë³´ì¡´)

        ì „ëµ:
        1. í‘œëŠ” í•­ìƒ ë…ë¦½ ì²­í¬
        2. ì¥/ì¡° ë‹¨ìœ„ë¡œ ì²­í‚¹
        3. í¬ê¸° ì´ˆê³¼ ì‹œ ë¶„í• 
        4. ì‘ì€ ì²­í¬ëŠ” ë³‘í•©
        """
        chunks = []
        current_chunk = []
        current_length = 0

        for elem in elements:
            elem_length = len(elem['content'])

            # í‘œëŠ” í•­ìƒ ë…ë¦½ ì²­í¬
            if elem['type'] == 'table':
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(self._create_chunk(current_chunk))
                    current_chunk = []
                    current_length = 0

                # í‘œ ì²­í¬ ì¶”ê°€
                chunks.append(self._create_chunk([elem]))
                continue

            # ì¥/ì¡° ì œëª©ì€ ìƒˆ ì²­í¬ ì‹œì‘
            if elem['type'] in ['chapter', 'article']:
                # í˜„ì¬ ì²­í¬ê°€ ëª©í‘œ í¬ê¸° ì´ìƒì´ë©´ ì €ì¥
                if current_length >= self.target_chars:
                    chunks.append(self._create_chunk(current_chunk))
                    current_chunk = []
                    current_length = 0

            # ì²­í¬ì— ì¶”ê°€
            current_chunk.append(elem)
            current_length += elem_length

            # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ ë¶„í• 
            if current_length >= self.max_chars:
                chunks.append(self._create_chunk(current_chunk))
                current_chunk = []
                current_length = 0

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk))

        # ì‘ì€ ì²­í¬ ë³‘í•©
        chunks = self._merge_small_chunks(chunks)

        return chunks

    def _create_chunk(self, elements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìš”ì†Œë“¤ë¡œë¶€í„° ì²­í¬ ìƒì„±"""
        if not elements:
            return None

        # í…ìŠ¤íŠ¸ ê²°í•©
        text = '\n\n'.join(elem['content'] for elem in elements)

        # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        types = [elem['type'] for elem in elements]
        pages = [elem['page'] for elem in elements]

        return {
            'text': text,
            'metadata': {
                'types': types,
                'page_start': min(pages),
                'page_end': max(pages),
                'is_table': 'table' in types,
                'has_chapter': 'chapter' in types,
                'has_article': 'article' in types,
                'length': len(text),
                'element_count': len(elements)
            }
        }

    def _merge_small_chunks(
        self,
        chunks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ì‘ì€ ì²­í¬ë¥¼ ì´ì „ ì²­í¬ì— ë³‘í•©"""
        if not chunks:
            return chunks

        merged = [chunks[0]]

        for chunk in chunks[1:]:
            chunk_length = chunk['metadata']['length']

            # ì‘ì€ ì²­í¬ëŠ” ì´ì „ ì²­í¬ì— ë³‘í•©
            if chunk_length < self.min_chars and not chunk['metadata']['is_table']:
                prev = merged[-1]
                prev['text'] += '\n\n' + chunk['text']
                prev['metadata']['length'] += chunk_length
                prev['metadata']['page_end'] = chunk['metadata']['page_end']
                prev['metadata']['element_count'] += chunk['metadata']['element_count']
            else:
                merged.append(chunk)

        return merged

    def _save_chunks(self, chunks: List[Dict[str, Any]], output_dir: str):
        """ì²­í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        chunk_metadata = []

        for i, chunk in enumerate(chunks):
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
            text_file = output_path / f"chunk_{i:04d}.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                # í—¤ë” ì •ë³´
                f.write(f"[Chunk {i}]\n")
                f.write(f"Pages: {chunk['metadata']['page_start']}-{chunk['metadata']['page_end']}\n")
                f.write(f"Type: {', '.join(set(chunk['metadata']['types']))}\n")
                f.write(f"Length: {chunk['metadata']['length']} chars\n")
                f.write("\n" + "="*80 + "\n\n")

                # ë‚´ìš©
                f.write(chunk['text'])

            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            chunk_metadata.append({
                'chunk_id': i,
                'file': f"chunk_{i:04d}.txt",
                **chunk['metadata']
            })

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_file = output_path / "chunks_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_metadata, f, ensure_ascii=False, indent=2)

        # í†µê³„ ì €ì¥
        stats = self._calculate_stats(chunks)
        stats_file = output_path / "chunks_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_dir}")
        logger.info(f"   - í…ìŠ¤íŠ¸ ì²­í¬: {len(chunks)}ê°œ")
        logger.info(f"   - í‘œ ì²­í¬: {stats['table_chunks']}ê°œ")
        logger.info(f"   - í‰ê·  í¬ê¸°: {stats['avg_length']:.0f} chars")

    def _calculate_stats(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì²­í¬ í†µê³„ ê³„ì‚°"""
        if not chunks:
            return {}

        lengths = [chunk['metadata']['length'] for chunk in chunks]
        table_chunks = sum(1 for chunk in chunks if chunk['metadata']['is_table'])

        return {
            'total_chunks': len(chunks),
            'table_chunks': table_chunks,
            'text_chunks': len(chunks) - table_chunks,
            'avg_length': sum(lengths) / len(lengths),
            'min_length': min(lengths),
            'max_length': max(lengths),
            'total_length': sum(lengths)
        }


# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """Smart Chunker ì‚¬ìš© ì˜ˆì‹œ"""

    chunker = SmartInsuranceChunker(
        max_chars=1500,
        target_chars=1200,
        min_chars=200
    )

    # PDF íŒŒì‹± + ì²­í‚¹
    chunks = chunker.parse_and_chunk(
        pdf_path="sample_insurance.pdf",
        output_dir="graphrag_chunks/"
    )

    # ì²­í¬ í™•ì¸
    for i, chunk in enumerate(chunks[:3]):
        print(f"\n--- Chunk {i} ---")
        print(f"Pages: {chunk['metadata']['page_start']}-{chunk['metadata']['page_end']}")
        print(f"Type: {chunk['metadata']['types']}")
        print(f"Length: {chunk['metadata']['length']} chars")
        print(f"Is Table: {chunk['metadata']['is_table']}")
        print(f"Preview: {chunk['text'][:100]}...")


if __name__ == "__main__":
    import asyncio
    asyncio.run(example_usage())
