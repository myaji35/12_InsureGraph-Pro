"""
LLM Reasoning Service

ê·¸ë˜í”„ íƒìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

Features:
1. Context Assembly - ê·¸ë˜í”„ ê²½ë¡œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¡°ë¦½
2. LLM Integration - OpenAI/Anthropic API í†µí•©
3. Prompt Engineering - ë³´í—˜ ì•½ê´€ ì „ë¬¸ í”„ë¡¬í”„íŠ¸
4. Answer Generation - êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
"""
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from enum import Enum

from app.services.query_parser import ParsedQuery, QueryIntent
from app.services.local_search import SearchResult
from app.services.graph_traversal import GraphPath, TraversalResult
from app.core.config import settings
from loguru import logger

# Optional LLM imports
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI not available - will use mock responses")

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic not available - will use mock responses")


class LLMProvider(str, Enum):
    """LLM provider options"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    MOCK = "mock"


@dataclass
class ReasoningContext:
    """Context assembled for LLM reasoning"""
    query: str
    intent: QueryIntent
    search_results: List[SearchResult]
    graph_paths: List[GraphPath]
    total_sources: int

    def to_text(self) -> str:
        """Convert context to text format for LLM"""
        lines = [
            f"# ì‚¬ìš©ì ì§ˆë¬¸: {self.query}",
            f"# ì§ˆë¬¸ ìœ í˜•: {self.intent.value}",
            f"# ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {self.total_sources}",
            "",
            "## ê´€ë ¨ ì¡°ë¬¸:",
        ]

        # Add search results
        for i, result in enumerate(self.search_results[:10], 1):
            lines.append(f"\n### {i}. {result.node_type} ({result.node_id})")
            lines.append(f"í…ìŠ¤íŠ¸: {result.text[:500]}...")
            if result.metadata:
                lines.append(f"ë©”íƒ€ë°ì´í„°: {result.metadata}")

        # Add graph paths
        if self.graph_paths:
            lines.append("\n## ê·¸ë˜í”„ ê²½ë¡œ:")
            for i, path in enumerate(self.graph_paths[:5], 1):
                lines.append(f"\n### ê²½ë¡œ {i}: {path}")
                for node in path.nodes:
                    lines.append(f"  - {node.node_type}: {node.text[:200]}...")

        return "\n".join(lines)


@dataclass
class ReasoningResult:
    """Result of LLM reasoning"""
    query: str
    answer: str
    confidence: float  # 0.0 - 1.0
    sources: List[Dict[str, Any]]  # Referenced sources
    reasoning_steps: List[str]  # Chain of thought
    provider: LLMProvider

    def to_dict(self) -> Dict[str, Any]:
        return {
            "query": self.query,
            "answer": self.answer,
            "confidence": self.confidence,
            "sources": self.sources,
            "reasoning_steps": self.reasoning_steps,
            "provider": self.provider.value,
        }


class LLMReasoning:
    """
    LLM-based reasoning service for insurance Q&A.

    Features:
    - Multi-provider support (OpenAI, Anthropic)
    - Context assembly from graph results
    - Chain-of-thought reasoning
    - Source citation
    """

    # System prompts for different query intents
    SYSTEM_PROMPTS = {
        QueryIntent.SEARCH: """ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë³´í—˜ ì•½ê´€ ì¡°ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì œê³µëœ ì¡°ë¬¸ì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¡°ë¬¸ì„ ì§ì ‘ ì¸ìš©í•  ë•ŒëŠ” ì¡°í•­ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "ì œ10ì¡°ì— ë”°ë¥´ë©´...")
3. ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
4. ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ì œê³µëœ ì•½ê´€ì—ì„œ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”""",

        QueryIntent.COMPARISON: """ë‹¹ì‹ ì€ ë³´í—˜ ìƒí’ˆ ë¹„êµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì•½ê´€ë“¤ì„ ë¹„êµí•˜ì—¬ ì°¨ì´ì ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ë¹„êµ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ë³´ì¥ ê¸ˆì•¡, ë³´ì¥ ë²”ìœ„, ë©´ì±… ì‚¬í•­ ë“± í•µì‹¬ ì°¨ì´ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
2. ê° í•­ëª©ë³„ë¡œ ë¹„êµí‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
3. ì¥ë‹¨ì ì„ ê°ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
4. ì¡°ë¬¸ ì¶œì²˜ë¥¼ ëª…í™•íˆ ë°íˆì„¸ìš”""",

        QueryIntent.AMOUNT_FILTER: """ë‹¹ì‹ ì€ ë³´í—˜ê¸ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³´í—˜ê¸ˆ ê¸ˆì•¡ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ë³´í—˜ê¸ˆ ê¸ˆì•¡ì„ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”
2. ë³´í—˜ê¸ˆ ì§€ê¸‰ ì¡°ê±´ì„ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”
3. ë©´ì±… ì‚¬í•­ì´ë‚˜ ì œí•œ ì‚¬í•­ë„ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”
4. ê¸ˆì•¡ì´ ë‹¤ë¥¸ ê²½ìš° (ì˜ˆ: ì§ˆë³‘ë³„ ì°¨ë“±) êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”""",

        QueryIntent.COVERAGE_CHECK: """ë‹¹ì‹ ì€ ë³´í—˜ ë³´ì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹ì • ì§ˆë³‘ì´ë‚˜ ìƒí™©ì´ ë³´ì¥ë˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ë³´ì¥ ì—¬ë¶€ë¥¼ ëª…í™•íˆ ë‹µë³€í•˜ì„¸ìš” (ë³´ì¥í•¨/ë³´ì¥ì•ˆí•¨/ì¡°ê±´ë¶€ë³´ì¥)
2. ë³´ì¥ ì¡°ê±´ì´ ìˆë‹¤ë©´ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”
3. ë³´í—˜ê¸ˆ ê¸ˆì•¡ë„ í•¨ê»˜ ì•ˆë‚´í•˜ì„¸ìš”
4. ë©´ì±… ì‚¬í•­ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”""",

        QueryIntent.EXCLUSION_CHECK: """ë‹¹ì‹ ì€ ë³´í—˜ ë©´ì±… ì‚¬í•­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë©´ì±… ì‚¬í•­ì„ ì •í™•í•˜ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ë©´ì±… ì‚¬í•­ì„ ëª…í™•í•˜ê²Œ ë‚˜ì—´í•˜ì„¸ìš”
2. ê° ë©´ì±… ì‚¬í•­ì´ ì ìš©ë˜ëŠ” ì¡°ê±´ì„ ì„¤ëª…í•˜ì„¸ìš”
3. ë©´ì±… ê¸°ê°„ì´ ìˆë‹¤ë©´ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: ê³„ì•½ì¼ë¡œë¶€í„° 90ì¼)
4. ì˜ˆì™¸ ì‚¬í•­ì´ ìˆë‹¤ë©´ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”""",

        QueryIntent.PERIOD_CHECK: """ë‹¹ì‹ ì€ ë³´í—˜ ê¸°ê°„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ëŒ€ê¸° ê¸°ê°„, ë³´í—˜ ê¸°ê°„ ë“± ê¸°ê°„ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ê¸°ê°„ì„ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”
2. ê¸°ê°„ì˜ ì‹œì‘ì ê³¼ ì¢…ë£Œì ì„ ëª…í™•íˆ í•˜ì„¸ìš”
3. ê¸°ê°„ ì¤‘ ì ìš©ë˜ëŠ” ì¡°ê±´ì„ ì„¤ëª…í•˜ì„¸ìš”
4. ê¸°ê°„ë³„ë¡œ ë³´ì¥ ë‚´ìš©ì´ ë‹¤ë¥´ë‹¤ë©´ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”""",
    }

    USER_PROMPT_TEMPLATE = """ë‹¤ìŒ ë³´í—˜ ì•½ê´€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€ í˜•ì‹:
1. ë‹µë³€: (ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€)
2. ê·¼ê±°: (ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì¡°ë¬¸ ì¸ìš©)
3. ì¶”ê°€ ì°¸ê³ ì‚¬í•­: (ì•Œì•„ë‘ë©´ ì¢‹ì€ ì •ë³´)
"""

    def __init__(
        self,
        provider: LLMProvider = LLMProvider.OPENAI,
        model: Optional[str] = None,
        temperature: float = 0.1,
    ):
        """
        Initialize LLM reasoning service.

        Args:
            provider: LLM provider (openai, anthropic, mock)
            model: Model name (optional, uses defaults)
            temperature: LLM temperature (0.0-1.0)
        """
        self.provider = provider
        self.temperature = temperature

        # Set default models
        if model:
            self.model = model
        else:
            if provider == LLMProvider.OPENAI:
                self.model = "gpt-4o-mini"
            elif provider == LLMProvider.ANTHROPIC:
                self.model = "claude-3-5-sonnet-20241022"
            else:
                self.model = "mock"

        # Initialize clients
        self.openai_client = None
        self.anthropic_client = None

        if provider == LLMProvider.OPENAI and OPENAI_AVAILABLE:
            try:
                self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
                logger.info(f"OpenAI client initialized with model: {self.model}")
            except Exception as e:
                logger.warning(f"Failed to initialize OpenAI: {e}")
                self.provider = LLMProvider.MOCK

        elif provider == LLMProvider.ANTHROPIC and ANTHROPIC_AVAILABLE:
            try:
                self.anthropic_client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)
                logger.info(f"Anthropic client initialized with model: {self.model}")
            except Exception as e:
                logger.warning(f"Failed to initialize Anthropic: {e}")
                self.provider = LLMProvider.MOCK

        else:
            self.provider = LLMProvider.MOCK
            logger.info("Using mock LLM provider")

    def assemble_context(
        self,
        parsed_query: ParsedQuery,
        search_results: Optional[List[SearchResult]] = None,
        graph_paths: Optional[List[GraphPath]] = None,
    ) -> ReasoningContext:
        """
        Assemble context from search and graph results.

        Args:
            parsed_query: Parsed query
            search_results: Local search results
            graph_paths: Graph traversal paths

        Returns:
            ReasoningContext for LLM
        """
        search_results = search_results or []
        graph_paths = graph_paths or []

        total_sources = len(search_results) + len(graph_paths)

        return ReasoningContext(
            query=parsed_query.original_query,
            intent=parsed_query.intent,
            search_results=search_results,
            graph_paths=graph_paths,
            total_sources=total_sources,
        )

    def reason(
        self,
        context: ReasoningContext,
    ) -> ReasoningResult:
        """
        Generate answer using LLM reasoning.

        Args:
            context: Assembled reasoning context

        Returns:
            ReasoningResult with answer
        """
        # Get system prompt for intent
        system_prompt = self.SYSTEM_PROMPTS.get(
            context.intent,
            self.SYSTEM_PROMPTS[QueryIntent.SEARCH]
        )

        # Format user prompt
        context_text = context.to_text()
        user_prompt = self.USER_PROMPT_TEMPLATE.format(
            context=context_text,
            query=context.query,
        )

        # Generate answer based on provider
        if self.provider == LLMProvider.OPENAI:
            answer, reasoning_steps = self._reason_openai(system_prompt, user_prompt)
        elif self.provider == LLMProvider.ANTHROPIC:
            answer, reasoning_steps = self._reason_anthropic(system_prompt, user_prompt)
        else:
            answer, reasoning_steps = self._reason_mock(context)

        # Extract sources
        sources = self._extract_sources(context)

        # Calculate confidence (simplified)
        confidence = self._calculate_confidence(context, answer)

        result = ReasoningResult(
            query=context.query,
            answer=answer,
            confidence=confidence,
            sources=sources,
            reasoning_steps=reasoning_steps,
            provider=self.provider,
        )

        logger.info(f"Generated answer for query: {context.query[:50]}... (confidence: {confidence:.2f})")

        return result

    def _reason_openai(self, system_prompt: str, user_prompt: str) -> tuple[str, List[str]]:
        """Generate answer using OpenAI"""
        if not self.openai_client:
            logger.warning("OpenAI client not available, using mock")
            return self._reason_mock_fallback()

        try:
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=self.temperature,
                max_tokens=2000,
            )

            answer = response.choices[0].message.content
            reasoning_steps = ["OpenAI reasoning completed"]

            return answer, reasoning_steps

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            return self._reason_mock_fallback()

    def _reason_anthropic(self, system_prompt: str, user_prompt: str) -> tuple[str, List[str]]:
        """Generate answer using Anthropic"""
        if not self.anthropic_client:
            logger.warning("Anthropic client not available, using mock")
            return self._reason_mock_fallback()

        try:
            response = self.anthropic_client.messages.create(
                model=self.model,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt},
                ],
                temperature=self.temperature,
                max_tokens=2000,
            )

            answer = response.content[0].text
            reasoning_steps = ["Anthropic reasoning completed"]

            return answer, reasoning_steps

        except Exception as e:
            logger.error(f"Anthropic API error: {e}")
            return self._reason_mock_fallback()

    def _reason_mock(self, context: ReasoningContext) -> tuple[str, List[str]]:
        """Generate mock answer for testing"""
        answer_parts = [
            "# ë‹µë³€:",
            f"'{context.query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.",
            "",
            f"ê²€ìƒ‰ëœ {len(context.search_results)}ê°œì˜ ì¡°ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.",
        ]

        # Add sample citations
        if context.search_results:
            answer_parts.append("\n## ê·¼ê±° ì¡°ë¬¸:")
            for i, result in enumerate(context.search_results[:3], 1):
                answer_parts.append(f"{i}. {result.node_id}: {result.text[:100]}...")

        # Add graph info
        if context.graph_paths:
            answer_parts.append(f"\nê´€ë ¨ ê²½ë¡œ {len(context.graph_paths)}ê°œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

        answer_parts.append("\n## ì¶”ê°€ ì°¸ê³ ì‚¬í•­:")
        answer_parts.append("ìì„¸í•œ ë‚´ìš©ì€ ì•½ê´€ ì „ë¬¸ì„ ì°¸ì¡°í•´ì£¼ì„¸ìš”.")

        answer = "\n".join(answer_parts)
        reasoning_steps = [
            "Query parsed",
            f"Found {len(context.search_results)} search results",
            f"Found {len(context.graph_paths)} graph paths",
            "Generated mock answer",
        ]

        return answer, reasoning_steps

    def _reason_mock_fallback(self) -> tuple[str, List[str]]:
        """Fallback mock response"""
        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. LLM ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        reasoning_steps = ["LLM unavailable", "Returned fallback response"]
        return answer, reasoning_steps

    def _extract_sources(self, context: ReasoningContext) -> List[Dict[str, Any]]:
        """Extract source references from context"""
        sources = []

        for result in context.search_results[:10]:
            sources.append({
                "node_id": result.node_id,
                "node_type": result.node_type,
                "text": result.text[:200],
                "relevance_score": result.relevance_score,
            })

        return sources

    def _calculate_confidence(self, context: ReasoningContext, answer: str) -> float:
        """
        Calculate confidence score based on context quality.

        Simplified heuristic:
        - More sources = higher confidence
        - Longer answer = higher confidence (up to a point)
        - Graph paths = higher confidence
        """
        confidence = 0.5  # Base confidence

        # Boost for number of sources
        if context.total_sources >= 5:
            confidence += 0.3
        elif context.total_sources >= 3:
            confidence += 0.2
        elif context.total_sources >= 1:
            confidence += 0.1

        # Boost for graph paths
        if context.graph_paths:
            confidence += 0.1

        # Boost for answer length (reasonable answer)
        if len(answer) > 200:
            confidence += 0.1

        return min(confidence, 1.0)


# Singleton instance
_llm_reasoning: Optional[LLMReasoning] = None


def get_llm_reasoning(
    provider: LLMProvider = LLMProvider.OPENAI,
    **kwargs
) -> LLMReasoning:
    """Get or create singleton LLM reasoning instance"""
    global _llm_reasoning
    if _llm_reasoning is None:
        _llm_reasoning = LLMReasoning(provider=provider, **kwargs)
    return _llm_reasoning


if __name__ == "__main__":
    # Test LLM reasoning
    from app.services.query_parser import get_query_parser
    from app.services.local_search import get_local_search, SearchResult

    print("=" * 70)
    print("ğŸ§ª LLM Reasoning Test")
    print("=" * 70)

    # Parse query
    print("\nğŸ“ Parsing query...")
    parser = get_query_parser()
    query = "ì•”ë³´í—˜ 1ì–µì› ì´ìƒ ë³´ì¥ ìƒí’ˆì€?"
    parsed_query = parser.parse(query)

    print(f"  Query: {parsed_query.original_query}")
    print(f"  Intent: {parsed_query.intent.value}")
    print(f"  Entities: {len(parsed_query.entities)}")

    # Mock search results
    print("\nğŸ” Creating mock search results...")
    mock_results = [
        SearchResult(
            node_type="Article",
            node_id="policy_123_article_ì œ10ì¡°",
            text="ì œ10ì¡° [ì•”ë³´í—˜ê¸ˆ ì§€ê¸‰] íšŒì‚¬ëŠ” í”¼ë³´í—˜ìê°€ ë³´í—˜ê¸°ê°„ ì¤‘ ì•”ìœ¼ë¡œ ì§„ë‹¨ í™•ì •ë˜ì—ˆì„ ë•Œ ì¼ë°˜ì•”ì˜ ê²½ìš° 1ì–µì›ì„ ì§€ê¸‰í•©ë‹ˆë‹¤.",
            relevance_score=0.95,
            metadata={"article_num": "ì œ10ì¡°"},
            article_num="ì œ10ì¡°",
            article_title="ì•”ë³´í—˜ê¸ˆ ì§€ê¸‰",
        ),
        SearchResult(
            node_type="Article",
            node_id="policy_123_article_ì œ11ì¡°",
            text="ì œ11ì¡° [ë©´ì±…ì‚¬í•­] ê³„ì•½ì¼ë¡œë¶€í„° 90ì¼ ì´ë‚´ ì§„ë‹¨ í™•ì •ëœ ì•”ì€ ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            relevance_score=0.85,
            metadata={"article_num": "ì œ11ì¡°"},
            article_num="ì œ11ì¡°",
            article_title="ë©´ì±…ì‚¬í•­",
        ),
    ]

    # Test with mock provider
    print("\nğŸ¤– Testing with MOCK provider...")
    reasoning = get_llm_reasoning(provider=LLMProvider.MOCK)

    context = reasoning.assemble_context(
        parsed_query=parsed_query,
        search_results=mock_results,
    )

    print(f"  Context assembled: {context.total_sources} sources")

    result = reasoning.reason(context)

    print(f"\n{'='*70}")
    print(f"ğŸ“Š Reasoning Result")
    print(f"{'='*70}")
    print(f"Query: {result.query}")
    print(f"Provider: {result.provider.value}")
    print(f"Confidence: {result.confidence:.2f}")
    print(f"\nAnswer:\n{result.answer}")
    print(f"\nReasoning Steps:")
    for i, step in enumerate(result.reasoning_steps, 1):
        print(f"  {i}. {step}")
    print(f"\nSources ({len(result.sources)}):")
    for source in result.sources:
        print(f"  - {source['node_id']}: {source['text'][:80]}...")

    print("\n" + "=" * 70)
    print("âœ… LLM reasoning test complete!")
    print("=" * 70)
