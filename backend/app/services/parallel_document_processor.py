"""
Parallel Document Processor

ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤.
"""
import asyncio
import httpx
import tempfile
import os
from typing import List, Dict, Optional
from loguru import logger
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import AsyncSessionLocal
from app.core.config import settings
from app.services.pdf_text_quality_evaluator import PDFTextQualityEvaluator
from app.services.streaming_pdf_processor import StreamingPDFProcessor
from app.services.hybrid_document_processor import HybridDocumentProcessor
from app.services.learning import SmartInsuranceLearner
from app.services.learning.deep_knowledge_service import DeepKnowledgeService


class ParallelDocumentProcessor:
    """ë³‘ë ¬ ë¬¸ì„œ ì²˜ë¦¬ê¸°"""

    def __init__(
        self,
        max_concurrent: int = 5,
        use_streaming: bool = True,
        use_smart_learning: bool = True,
        use_hybrid: bool = None
    ):
        """
        Args:
            max_concurrent: ë™ì‹œì— ì²˜ë¦¬í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: 5)
                - CPU ì½”ì–´ ìˆ˜ì™€ ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì„¤ì •
                - ë„ˆë¬´ ë†’ìœ¼ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ë‚˜ API ì œí•œì— ê±¸ë¦´ ìˆ˜ ìžˆìŒ
            use_streaming: ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                - True: ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ì—†ì´ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                - False: ê¸°ì¡´ ë°©ì‹ (ìž„ì‹œ íŒŒì¼ ì €ìž¥)
            use_smart_learning: ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                - True: SmartInsuranceLearner ì‚¬ìš© (ë¹„ìš© ì ˆê°)
                - False: ê¸°ì¡´ ë°©ì‹ (ì „ì²´ í•™ìŠµ)
            use_hybrid: í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: settingsì—ì„œ ë¡œë“œ)
                - True: pdfplumber/Upstage ìžë™ ì„ íƒ (ë¹„ìš© ìµœì í™”)
                - False: StreamingPDFProcessor ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
        """
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.use_streaming = use_streaming
        self.use_smart_learning = use_smart_learning

        # í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • (ê¸°ë³¸ê°’ì€ settingsì—ì„œ ë¡œë“œ)
        self.use_hybrid = use_hybrid if use_hybrid is not None else settings.HYBRID_EXTRACTION_ENABLED

        if self.use_hybrid:
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ í™œì„±í™”: strategy={settings.HYBRID_STRATEGY}")
            self.hybrid_processor = HybridDocumentProcessor(
                strategy=settings.HYBRID_STRATEGY,
                complexity_threshold=settings.HYBRID_COMPLEXITY_THRESHOLD,
                quality_threshold=settings.HYBRID_QUALITY_THRESHOLD,
                file_size_threshold_mb=settings.HYBRID_FILE_SIZE_THRESHOLD_MB
            )
            self.streaming_processor = None
        else:
            logger.info("ê¸°ì¡´ StreamingPDFProcessor ì‚¬ìš©")
            self.streaming_processor = StreamingPDFProcessor() if use_streaming else None
            self.hybrid_processor = None

        self.smart_learner = SmartInsuranceLearner() if use_smart_learning else None
        self.deep_knowledge_service = DeepKnowledgeService() if use_smart_learning else None

    async def process_pending_documents(
        self,
        limit: Optional[int] = None,
        insurer: Optional[str] = None
    ) -> Dict[str, int]:
        """
        ëŒ€ê¸° ì¤‘ì¸ ë¬¸ì„œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            limit: ì²˜ë¦¬í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ëª¨ë“  ëŒ€ê¸° ë¬¸ì„œ ì²˜ë¦¬)
            insurer: íŠ¹ì • ë³´í—˜ì‚¬ì˜ ë¬¸ì„œë§Œ ì²˜ë¦¬ (Noneì´ë©´ ëª¨ë“  ë³´í—˜ì‚¬)

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„ (ì„±ê³µ, ì‹¤íŒ¨, ì´ ê°œìˆ˜)
        """
        async with AsyncSessionLocal() as db:
            # ëŒ€ê¸° ì¤‘ì¸ ë¬¸ì„œ ì¡°íšŒ
            query = text("""
                SELECT id, pdf_url, insurer, product_type, title
                FROM crawler_documents
                WHERE status = 'pending'
            """)

            if insurer:
                query = text("""
                    SELECT id, pdf_url, insurer, product_type, title
                    FROM crawler_documents
                    WHERE status = 'pending' AND insurer = :insurer
                """)

            if limit:
                query = text(str(query) + f" LIMIT {limit}")

            result = await db.execute(
                query,
                {"insurer": insurer} if insurer else {}
            )
            pending_docs = result.fetchall()

        if not pending_docs:
            logger.info("No pending documents to process")
            return {"total": 0, "success": 0, "failed": 0}

        logger.info(f"Starting parallel processing of {len(pending_docs)} documents with max_concurrent={self.max_concurrent}")

        # ë³‘ë ¬ ì²˜ë¦¬ ì‹œìž‘
        tasks = []
        for doc in pending_docs:
            task = self._process_document_with_semaphore(
                document_id=str(doc[0]),
                pdf_url=doc[1],
                insurer=doc[2],
                product_type=doc[3],
                product_name=doc[4]
            )
            tasks.append(task)

        # ëª¨ë“  ìž‘ì—… ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì§‘ê³„
        success_count = sum(1 for r in results if r is True)
        failed_count = sum(1 for r in results if r is False or isinstance(r, Exception))

        logger.info(f"Parallel processing completed: {success_count} success, {failed_count} failed out of {len(pending_docs)} total")

        return {
            "total": len(pending_docs),
            "success": success_count,
            "failed": failed_count
        }

    async def _process_document_with_semaphore(
        self,
        document_id: str,
        pdf_url: str,
        insurer: str,
        product_type: str,
        product_name: str
    ) -> bool:
        """
        ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ ì œí•œí•˜ë©´ì„œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Returns:
            ì„±ê³µ ì—¬ë¶€ (True/False)
        """
        async with self.semaphore:
            return await self._process_single_document(
                document_id=document_id,
                pdf_url=pdf_url,
                insurer=insurer,
                product_type=product_type,
                product_name=product_name
            )

    async def _process_single_document(
        self,
        document_id: str,
        pdf_url: str,
        insurer: str,
        product_type: str,
        product_name: str
    ) -> bool:
        """
        ë‹¨ì¼ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Returns:
            ì„±ê³µ ì—¬ë¶€ (True/False)
        """
        logger.info(f"[{document_id[:8]}] Starting processing: {insurer} - {product_type} - {product_name}")

        async with AsyncSessionLocal() as db:
            try:
                async def update_progress(step: str, progress: int, detail: dict = None):
                    """ì§„í–‰ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
                    import json
                    update_query = text("""
                        UPDATE crawler_documents
                        SET processing_step = :step,
                            processing_progress = :progress,
                            processing_detail = :detail,
                            status = 'processing',
                            updated_at = NOW()
                        WHERE id = :id
                    """)
                    await db.execute(update_query, {
                        "id": document_id,
                        "step": step,
                        "progress": progress,
                        "detail": json.dumps(detail) if detail else None
                    })
                    await db.commit()
                    detail_msg = f" - {detail.get('message', '')}" if detail and 'message' in detail else ""
                    logger.info(f"[{document_id[:8]}] {step} ({progress}%){detail_msg}")

                # Step 1: PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ (1% ~ 40%)
                await update_progress("downloading_pdf", 1, {
                    "sub_step": "initializing",
                    "message": "PDF ì²˜ë¦¬ ì´ˆê¸°í™” ì¤‘..."
                })

                import time
                start_time = time.time()

                # PDF ì²˜ë¦¬ ë°©ì‹ ê²°ì •: í•˜ì´ë¸Œë¦¬ë“œ > ìŠ¤íŠ¸ë¦¬ë° > ê¸°ì¡´ ë°©ì‹
                if self.use_hybrid and self.hybrid_processor:
                    # ðŸŒŸ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (pdfplumber/Upstage ìžë™ ì„ íƒ)
                    await update_progress("extracting_text", 10, {
                        "sub_step": "hybrid_mode",
                        "message": f"í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ PDF ì²˜ë¦¬ ì¤‘ (ì „ëžµ: {settings.HYBRID_STRATEGY})"
                    })

                    hybrid_result = await self.hybrid_processor.process_document(pdf_url)

                    extracted_text = hybrid_result["text"]
                    total_pages = hybrid_result["total_pages"]
                    algorithm = hybrid_result.get("algorithm", hybrid_result.get("method", "hybrid"))
                    memory_saved = hybrid_result.get("memory_saved_mb", "100%")
                    hybrid_decision = hybrid_result.get("hybrid_decision", "unknown")
                    decision_reason = hybrid_result.get("decision_reason", "")

                    total_time = int(time.time() - start_time)
                    await update_progress("extracting_text", 40, {
                        "sub_step": "extraction_complete",
                        "message": f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (í•˜ì´ë¸Œë¦¬ë“œ-{hybrid_decision}, {total_time}ì´ˆ)",
                        "algorithm": algorithm,
                        "method": hybrid_result.get("method", "hybrid"),
                        "hybrid_decision": hybrid_decision,
                        "decision_reason": decision_reason,
                        "text_length": len(extracted_text),
                        "total_pages": total_pages,
                        "processing_time_seconds": total_time,
                        "complexity_score": hybrid_result.get("complexity_score"),
                        "quality_score": hybrid_result.get("quality_score")
                    })

                    logger.info(
                        f"[{document_id[:8]}] Hybrid extraction completed: "
                        f"{hybrid_decision} ({decision_reason}), "
                        f"pages={total_pages}, time={total_time}s"
                    )

                    # ìž„ì‹œ íŒŒì¼ ê²½ë¡œëŠ” None (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì´ë¯€ë¡œ íŒŒì¼ ìƒì„± ì•ˆ ë¨)
                    tmp_path = None

                elif self.use_streaming and self.streaming_processor:
                    # ðŸš€ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ (ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
                    await update_progress("extracting_text", 10, {
                        "sub_step": "streaming_mode",
                        "message": "ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ PDF ì²˜ë¦¬ ì¤‘ (ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ì—†ìŒ)"
                    })

                    streaming_result = await self.streaming_processor.process_pdf_streaming(pdf_url)

                    extracted_text = streaming_result["text"]
                    total_pages = streaming_result["total_pages"]
                    algorithm = streaming_result.get("algorithm", "streaming")
                    memory_saved = streaming_result.get("memory_saved_mb", 0)

                    total_time = int(time.time() - start_time)
                    await update_progress("extracting_text", 40, {
                        "sub_step": "extraction_complete",
                        "message": f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ìŠ¤íŠ¸ë¦¬ë°, {total_time}ì´ˆ, ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved}MB)",
                        "algorithm": algorithm,
                        "method": streaming_result["method"],
                        "text_length": len(extracted_text),
                        "total_pages": total_pages,
                        "processing_time_seconds": total_time,
                        "memory_saved_mb": memory_saved
                    })

                    logger.info(f"[{document_id[:8]}] Streaming extraction completed: {algorithm}, pages={total_pages}, time={total_time}s, memory_saved={memory_saved}MB")

                    # ìž„ì‹œ íŒŒì¼ ê²½ë¡œëŠ” None (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì´ë¯€ë¡œ íŒŒì¼ ìƒì„± ì•ˆ ë¨)
                    tmp_path = None

                else:
                    # ðŸ“ ê¸°ì¡´ ë°©ì‹ (ìž„ì‹œ íŒŒì¼ ì €ìž¥)
                    await update_progress("downloading_pdf", 20, {
                        "sub_step": "downloading",
                        "message": "PDF ë‹¤ìš´ë¡œë“œ ì¤‘..."
                    })

                    async with httpx.AsyncClient(timeout=60.0) as client:
                        response = await client.get(pdf_url)
                        response.raise_for_status()
                        pdf_content = response.content

                    # ìž„ì‹œ íŒŒì¼ë¡œ ì €ìž¥
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                        tmp_file.write(pdf_content)
                        tmp_path = tmp_file.name

                    await update_progress("extracting_text", 21, {
                        "sub_step": "pdf_analysis",
                        "message": "PDF ë©”íƒ€ë°ì´í„° ë¶„ì„ ì¤‘"
                    })

                    await update_progress("extracting_text", 23, {
                        "sub_step": "analyzing_algorithms",
                        "message": "ìµœì ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì¤‘..."
                    })

                    # ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ì‹œë„ ë° ìµœê³  í’ˆì§ˆ ê²°ê³¼ ì„ íƒ
                    extraction_result = PDFTextQualityEvaluator.extract_best_quality(tmp_path)

                    if "error" in extraction_result:
                        await update_progress("extracting_text", 35, {
                            "sub_step": "extraction_failed",
                            "message": "ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨",
                            "attempts": extraction_result.get("all_attempts", [])
                        })
                        raise Exception(f"Text extraction failed: {extraction_result['error']}")

                    extracted_text = extraction_result["text"]
                    total_pages = extraction_result["total_pages"]
                    algorithm = extraction_result["algorithm"]
                    quality = extraction_result["quality"]

                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ with í’ˆì§ˆ ì •ë³´
                    total_time = int(time.time() - start_time)
                    await update_progress("extracting_text", 40, {
                        "sub_step": "extraction_complete",
                        "message": f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({algorithm}, {total_time}ì´ˆ)",
                        "algorithm": algorithm,
                        "quality_score": quality["score"],
                        "quality_level": quality["quality_level"],
                        "text_length": len(extracted_text),
                        "total_pages": total_pages,
                        "processing_time_seconds": total_time,
                        "avg_chars_per_page": quality["avg_chars_per_page"],
                        "korean_ratio": quality["korean_ratio"],
                        "english_ratio": quality["english_ratio"]
                    })

                    logger.info(f"[{document_id[:8]}] Text extraction completed: {algorithm}, quality={quality['score']}, time={total_time}s")

                # Step 3-6: ìŠ¤ë§ˆíŠ¸ í•™ìŠµ (Smart Learning)
                if self.use_smart_learning and self.smart_learner:
                    # SmartInsuranceLearner ì‚¬ìš© (ìžë™ìœ¼ë¡œ ìµœì  ì „ëžµ ì„ íƒ)
                    await update_progress("smart_learning", 50, {
                        "sub_step": "initializing",
                        "message": "ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì´ˆê¸°í™” ì¤‘..."
                    })

                    logger.info(f"[{document_id[:8]}] Starting smart learning for {insurer} - {product_type}")

                    # ì‹¤ì œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ë° PostgreSQL ì €ìž¥ (DeepKnowledgeService)
                    async def actual_learning_callback(text_chunk: str) -> Dict:
                        """
                        DeepKnowledgeServiceë¥¼ ì‚¬ìš©í•˜ì—¬ GraphRAG ìŠ¤íƒ€ì¼ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ê³  PostgreSQLì— ì €ìž¥
                        """
                        if not self.deep_knowledge_service:
                            logger.warning(f"[{document_id[:8]}] DeepKnowledgeService not initialized, skipping entity extraction")
                            return {
                                "entities": 0,
                                "relationships": 0,
                                "chunk_length": len(text_chunk),
                                "error": "DeepKnowledgeService not initialized"
                            }

                        try:
                            # chunk_id ìƒì„±
                            import hashlib
                            chunk_hash = hashlib.md5(text_chunk.encode()).hexdigest()[:8]
                            chunk_id = f"{document_id[:8]}_{chunk_hash}"

                            # ë¬¸ì„œ ì •ë³´ ì¤€ë¹„
                            document_info = {
                                "insurer": insurer,
                                "product_type": product_type,
                                "title": document.title or f"{insurer} {product_type}"
                            }

                            # DeepKnowledgeServiceë¡œ ì—”í‹°í‹° ì¶”ì¶œ ë° PostgreSQL ì €ìž¥
                            result = await self.deep_knowledge_service.process_and_extract(
                                chunk_text=text_chunk,
                                document_id=document_id,
                                chunk_id=chunk_id,
                                document_info=document_info
                            )

                            logger.info(
                                f"[{document_id[:8]}] Deep knowledge extracted: "
                                f"{result.get('entities', 0)} entities, {result.get('relationships', 0)} relationships"
                            )

                            return {
                                "entities": result.get("entities", 0),
                                "relationships": result.get("relationships", 0),
                                "chunk_length": len(text_chunk),
                                "nodes_by_type": result.get("nodes_by_type", {}),
                                "relationships_by_type": result.get("relationships_by_type", {}),
                            }

                        except Exception as e:
                            logger.error(f"[{document_id[:8]}] Deep knowledge extraction failed: {e}", exc_info=True)
                            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (í•™ìŠµì€ ê³„ì† ì§„í–‰)
                            return {
                                "entities": 0,
                                "relationships": 0,
                                "chunk_length": len(text_chunk),
                                "error": str(e)
                            }

                    try:
                        # ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ìˆ˜í–‰
                        learning_result = await self.smart_learner.learn_document(
                            document_id=document_id,
                            text=extracted_text,
                            insurer=insurer,
                            product_type=product_type,
                            full_learning_callback=actual_learning_callback
                        )

                        # í•™ìŠµ ì „ëžµê³¼ ë¹„ìš© ì ˆê° ì •ë³´ ë¡œê¹…
                        strategy = learning_result.get("strategy", "unknown")
                        cost_saving = learning_result.get("cost_saving_percent", "0%")

                        # ì¶”ì¶œëœ ì—”í‹°í‹°/ê´€ê³„ ì •ë³´
                        total_entities = learning_result.get("total_entities", 0)
                        total_relationships = learning_result.get("total_relationships", 0)

                        await update_progress("smart_learning_complete", 90, {
                            "sub_step": "completed",
                            "message": f"ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì™„ë£Œ ({strategy} ì „ëžµ, {cost_saving} ì ˆê°, {total_entities}ê°œ ë…¸ë“œ, {total_relationships}ê°œ ê´€ê³„)",
                            "strategy": strategy,
                            "cost_saving": cost_saving,
                            "priority": learning_result.get("priority", 3),
                            "entities": total_entities,
                            "relationships": total_relationships,
                            "nodes_by_type": learning_result.get("nodes_by_type", {}),
                            "relationships_by_type": learning_result.get("relationships_by_type", {})
                        })

                        logger.info(
                            f"[{document_id[:8]}] Smart learning completed: "
                            f"strategy={strategy}, cost_saving={cost_saving}, "
                            f"entities={total_entities}, relationships={total_relationships}"
                        )

                    except Exception as e:
                        logger.error(f"[{document_id[:8]}] Smart learning failed: {e}, falling back to simulation")
                        # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ í´ë°±
                        await update_progress("learning_fallback", 90, {
                            "sub_step": "fallback",
                            "message": "ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜"
                        })

                else:
                    # ê¸°ì¡´ ë°©ì‹ (ì‹œë®¬ë ˆì´ì…˜)
                    await update_progress("extracting_entities", 60)
                    await asyncio.sleep(1)

                    await update_progress("extracting_relationships", 80)
                    await asyncio.sleep(1)

                    await update_progress("building_graph", 90)
                    await asyncio.sleep(1)

                await update_progress("generating_embeddings", 95, {
                    "sub_step": "preparing_embeddings",
                    "message": "ìž„ë² ë”© ìƒì„± ì¤€ë¹„ ì¤‘..."
                })
                await asyncio.sleep(0.5)

                # Step 7: ì™„ë£Œ (100%)
                # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì¸ ê²½ìš° quality ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìžˆìŒ
                completion_detail = {
                    "sub_step": "finalized",
                    "message": "ë¬¸ì„œ í•™ìŠµ ì™„ë£Œ",
                    "total_pages": total_pages,
                    "text_length": len(extracted_text),
                    "algorithm": algorithm
                }

                # ê¸°ì¡´ ë°©ì‹ì¸ ê²½ìš°ì—ë§Œ quality_score ì¶”ê°€
                if not self.use_streaming:
                    completion_detail["quality_score"] = quality["score"]

                await update_progress("completed", 100, completion_detail)

                # statusë¥¼ 'completed'ë¡œ ë³€ê²½
                final_update_query = text("""
                    UPDATE crawler_documents
                    SET status = 'completed'
                    WHERE id = :id
                """)
                await db.execute(final_update_query, {"id": document_id})
                await db.commit()

                logger.info(f"[{document_id[:8]}] Processing completed successfully")

                # ìž„ì‹œ íŒŒì¼ ì •ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì¸ ê²½ìš° tmp_pathê°€ Noneì¼ ìˆ˜ ìžˆìŒ)
                if tmp_path and os.path.exists(tmp_path):
                    os.unlink(tmp_path)
                    logger.debug(f"[{document_id[:8]}] Temporary file deleted: {tmp_path}")

                return True

            except Exception as e:
                logger.error(f"[{document_id[:8]}] Processing failed: {e}")
                # ìƒíƒœë¥¼ 'failed'ë¡œ ì—…ë°ì´íŠ¸
                try:
                    update_query = text("""
                        UPDATE crawler_documents
                        SET status = 'failed',
                            updated_at = NOW(),
                            processing_detail = :error
                        WHERE id = :id
                    """)
                    await db.execute(update_query, {
                        "id": document_id,
                        "error": str(e)
                    })
                    await db.commit()
                except:
                    pass
                return False


# ì‚¬ìš© ì˜ˆì œ
async def process_documents_in_parallel(max_concurrent: int = 5, limit: Optional[int] = None):
    """
    ëŒ€ê¸° ì¤‘ì¸ ë¬¸ì„œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        max_concurrent: ë™ì‹œì— ì²˜ë¦¬í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        limit: ì²˜ë¦¬í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
    """
    processor = ParallelDocumentProcessor(max_concurrent=max_concurrent)
    result = await processor.process_pending_documents(limit=limit)
    return result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys

    max_concurrent = int(sys.argv[1]) if len(sys.argv) > 1 else 5
    limit = int(sys.argv[2]) if len(sys.argv) > 2 else None

    logger.info(f"Starting parallel processing with max_concurrent={max_concurrent}, limit={limit}")
    result = asyncio.run(process_documents_in_parallel(max_concurrent, limit))
    logger.info(f"Final result: {result}")
