"""
Test Crawler API Endpoints

ìƒ˜í”Œ í¬ë¡¤ë§ ë° íŒŒì¼ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
"""
import time
from typing import Optional
from datetime import datetime

from fastapi import APIRouter, HTTPException, status
from loguru import logger

from app.api.v1.models.crawler import (
    TestCrawlRequest,
    TestCrawlResponse,
    DiscoveredFile,
    CrawlStep,
    SaveHeaderRequest,
    HeaderConfigResponse
)
from app.services.playwright_crawler import PlaywrightCrawler, PLAYWRIGHT_AVAILABLE
from app.services.file_extractor import FileExtractor
from app.services.header_storage import get_header_storage


router = APIRouter(prefix="/test-crawler", tags=["Test Crawler"])


@router.post(
    "/crawl",
    response_model=TestCrawlResponse,
    status_code=status.HTTP_200_OK,
    summary="í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰",
    description="URLì„ í¬ë¡¤ë§í•˜ê³  HTMLì„ ì €ì¥í•œ í›„ Claudeë¡œ íŒŒì¼ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."
)
async def test_crawl(request: TestCrawlRequest) -> TestCrawlResponse:
    """
    í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰

    1. Playwrightë¡œ URL í¬ë¡¤ë§
    2. HTML íŒŒì¼ ì €ì¥
    3. Claude APIë¡œ íŒŒì¼ëª… ì¶”ì¶œ
    4. í—¤ë” ì„¤ì • ì €ì¥ (ì˜µì…˜)
    """
    start_time = time.time()
    steps = []
    logs = []

    def add_log(message: str):
        timestamp = datetime.now().strftime("%H:%M:%S")
        logs.append(f"[{timestamp}] {message}")
        logger.info(message)

    def add_step(step_num: int, name: str, status: str, message: Optional[str] = None):
        steps.append(CrawlStep(
            step=step_num,
            name=name,
            status=status,
            message=message
        ))

    try:
        # Check if Playwright is available
        if not PLAYWRIGHT_AVAILABLE:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Playwright is not installed. Run: pip install playwright && playwright install chromium"
            )

        # Step 1: Playwright í˜ì´ì§€ ë¶„ì„
        add_step(1, "Playwright í˜ì´ì§€ ë¶„ì„", "running", "ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        add_log("ğŸŒ Playwright ë¸Œë¼ìš°ì € ì‹¤í–‰")
        add_log(f"URL: {request.url}")

        # Get stored headers if available
        header_storage = get_header_storage()
        stored_headers = header_storage.get_headers_dict_only(request.company_name)

        # Merge with request headers
        final_headers = {**stored_headers, **(request.headers or {})}

        if final_headers:
            add_log(f"ì‚¬ìš©ì ì •ì˜ í—¤ë”: {list(final_headers.keys())}")

        # Initialize crawler
        async with PlaywrightCrawler(headless=True) as crawler:
            # Crawl page
            html_content, metadata = await crawler.crawl_page(
                url=request.url,
                custom_headers=final_headers if final_headers else None,
                wait_time=2000
            )

            add_log(f"í˜ì´ì§€ ë¡œë”© ì™„ë£Œ: {metadata['title']}")
            add_log(f"ì½˜í…ì¸  í¬ê¸°: {metadata['content_length']:,} bytes")
            add_step(1, "Playwright í˜ì´ì§€ ë¶„ì„", "completed", f"ì™„ë£Œ ({metadata['duration']:.2f}ì´ˆ)")

            # Step 2: HTML íŒŒì‹± ë° ë¶„ì„
            add_step(2, "HTML íŒŒì‹± ë° ë¶„ì„", "running", "HTML êµ¬ì¡° ë¶„ì„ ì¤‘...")
            add_log("ğŸ“„ HTML íŒŒì‹± ì¤‘...")

            # Parse HTML structure
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            # Count links
            all_links = soup.find_all('a', href=True)
            pdf_links = [a for a in all_links if '.pdf' in a.get('href', '').lower()]

            add_log(f"ì´ ë§í¬ ìˆ˜: {len(all_links)}ê°œ")
            add_log(f"PDF ë§í¬ ìˆ˜: {len(pdf_links)}ê°œ")
            add_step(2, "HTML íŒŒì‹± ë° ë¶„ì„", "completed", f"{len(all_links)}ê°œ ë§í¬ ë°œê²¬")

            # Step 3: HTML ì €ì¥
            add_step(3, "HTML ì €ì¥", "running", "íŒŒì¼ ì €ì¥ ì¤‘...")
            add_log("ğŸ’¾ HTML íŒŒì¼ ì €ì¥ ì¤‘...")

            html_path = await crawler.save_html(
                html_content=html_content,
                company_name=request.company_name
            )

            import os
            html_size = os.path.getsize(html_path)

            add_log(f"ì €ì¥ ì™„ë£Œ: {html_path}")
            add_log(f"íŒŒì¼ í¬ê¸°: {html_size:,} bytes")
            add_step(3, "HTML ì €ì¥", "completed", f"{html_size:,} bytes ì €ì¥")

        # Step 4: LLM ë¶„ì„ (íŒŒì¼ëª… ì¶”ì¶œ)
        add_step(4, "LLM ë¶„ì„ (íŒŒì¼ëª… ì¶”ì¶œ)", "running", "Claude API í˜¸ì¶œ ì¤‘...")
        add_log("ğŸ¤– Claude Sonnet 4.5 ë¶„ì„ ì‹œì‘")
        add_log("HTMLì—ì„œ ì•½ê´€ íŒŒì¼ëª… ì¶”ì¶œ ì¤‘...")

        # Extract filenames using Claude
        extractor = FileExtractor()
        files = await extractor.extract_filenames(
            html_content=html_content,
            company_name=request.company_name,
            max_files=10
        )

        add_log(f"âœ… ë¶„ì„ ì™„ë£Œ! ë°œê²¬ëœ ë¬¸ì„œ: {len(files)}ê°œ")

        # Convert to DiscoveredFile model
        discovered_files = [
            DiscoveredFile(
                filename=f.get('filename', ''),
                url=f.get('url'),
                confidence=f.get('confidence', 0.0),
                context=f.get('context')
            )
            for f in files
        ]

        for i, file in enumerate(discovered_files, 1):
            add_log(f"  {i}. {file.filename} (ì‹ ë¢°ë„: {file.confidence:.2f})")

        add_step(4, "LLM ë¶„ì„ (íŒŒì¼ëª… ì¶”ì¶œ)", "completed", f"{len(files)}ê°œ íŒŒì¼ ë°œê²¬")

        # Save headers if requested
        if request.save_headers and final_headers:
            header_storage.save_headers(
                company_name=request.company_name,
                headers=final_headers
            )
            add_log(f"ğŸ“ í—¤ë” ì„¤ì • ì €ì¥ ì™„ë£Œ")

        # Calculate duration
        duration = time.time() - start_time

        return TestCrawlResponse(
            success=True,
            company_name=request.company_name,
            url=request.url,
            html_saved=True,
            html_path=html_path,
            html_size=html_size,
            discovered_files=discovered_files,
            total_files=len(discovered_files),
            steps=steps,
            logs=logs,
            duration=duration
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Test crawl failed: {e}", exc_info=True)

        # Add error step
        add_step(0, "ì˜¤ë¥˜ ë°œìƒ", "failed", str(e))
        add_log(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        duration = time.time() - start_time

        return TestCrawlResponse(
            success=False,
            company_name=request.company_name,
            url=request.url,
            html_saved=False,
            discovered_files=[],
            total_files=0,
            steps=steps,
            logs=logs,
            error=str(e),
            duration=duration
        )


@router.post(
    "/headers",
    response_model=HeaderConfigResponse,
    status_code=status.HTTP_200_OK,
    summary="í—¤ë” ì„¤ì • ì €ì¥",
    description="ë³´í—˜ì‚¬ë³„ HTTP í—¤ë” ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤."
)
async def save_headers(request: SaveHeaderRequest) -> HeaderConfigResponse:
    """í—¤ë” ì„¤ì • ì €ì¥"""
    header_storage = get_header_storage()

    config = header_storage.save_headers(
        company_name=request.company_name,
        headers=request.headers
    )

    return HeaderConfigResponse(**config)


@router.get(
    "/headers/{company_name}",
    response_model=HeaderConfigResponse,
    status_code=status.HTTP_200_OK,
    summary="í—¤ë” ì„¤ì • ì¡°íšŒ",
    description="ë³´í—˜ì‚¬ì˜ ì €ì¥ëœ HTTP í—¤ë” ì„¤ì •ì„ ì¡°íšŒí•©ë‹ˆë‹¤."
)
async def get_headers(company_name: str) -> HeaderConfigResponse:
    """í—¤ë” ì„¤ì • ì¡°íšŒ"""
    header_storage = get_header_storage()

    config = header_storage.get_headers(company_name)

    if not config:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Headers not found for {company_name}"
        )

    return HeaderConfigResponse(**config)


@router.delete(
    "/headers/{company_name}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="í—¤ë” ì„¤ì • ì‚­ì œ",
    description="ë³´í—˜ì‚¬ì˜ HTTP í—¤ë” ì„¤ì •ì„ ì‚­ì œí•©ë‹ˆë‹¤."
)
async def delete_headers(company_name: str):
    """í—¤ë” ì„¤ì • ì‚­ì œ"""
    header_storage = get_header_storage()

    success = header_storage.delete_headers(company_name)

    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Headers not found for {company_name}"
        )
