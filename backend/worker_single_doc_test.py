"""
Single Document LLM Test - Process smallest document with Opus

ë©”íŠ¸ë¼ì´í”„ ë³€ì•¡ì—°ê¸ˆë³´í—˜ ì•½ê´€ 1ê°œ ë¬¸ì„œë§Œ LLM ì²˜ë¦¬í•˜ì—¬:
1. ë¹„ìš© ì¸¡ì •
2. ê²°ê³¼ í’ˆì§ˆ í™•ì¸
3. ê·œì¹™ ê¸°ë°˜ê³¼ ë¹„êµ
"""
import sys
import os
import asyncio
from typing import Optional
from pathlib import Path

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import requests
import pdfplumber
from loguru import logger
from sqlalchemy import text as sql_text

from app.core.database import AsyncSessionLocal
from app.services.llm_entity_extractor import LLMEntityExtractor

# PDF ì €ì¥ ê²½ë¡œ
PDF_DIR = Path(__file__).parent / "data" / "pdfs_llm"
PDF_DIR.mkdir(parents=True, exist_ok=True)

# íƒ€ê²Ÿ ë¬¸ì„œ ID (ê°€ì¥ ì‘ì€ 1.1MB ë¬¸ì„œ)
TARGET_DOC_ID = "4ee5193e-f404-4c3c-a191-dbcb91bbdf83"


class SingleDocumentTester:
    """ë‹¨ì¼ ë¬¸ì„œ LLM í…ŒìŠ¤íŠ¸"""

    def __init__(self):
        """LLM ì¶”ì¶œê¸° ì´ˆê¸°í™”"""
        self.llm_extractor = LLMEntityExtractor()

    async def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        logger.info("=" * 80)
        logger.info("ğŸ“„ Single Document LLM Test with Opus")
        logger.info(f"ğŸ¯ Target Document: {TARGET_DOC_ID}")
        logger.info("=" * 80)

        # 1. ë¬¸ì„œ ì¡°íšŒ
        async with AsyncSessionLocal() as db:
            query_str = f"""
                SELECT id, title, insurer, category, product_type, pdf_url
                FROM crawler_documents
                WHERE id = '{TARGET_DOC_ID}' AND status = 'completed'
            """
            result = await db.execute(sql_text(query_str))
            doc = result.fetchone()

        if not doc:
            logger.error(f"âŒ Document not found: {TARGET_DOC_ID}")
            return

        doc_id, title, insurer, category, product_type, pdf_url = doc
        doc_id = str(doc_id)

        logger.info(f"âœ… Found document:")
        logger.info(f"   ì œëª©: {title}")
        logger.info(f"   ë³´í—˜ì‚¬: {insurer}")
        logger.info(f"   ì¹´í…Œê³ ë¦¬: {category}")
        logger.info("")

        try:
            # Step 1: PDF ë‹¤ìš´ë¡œë“œ
            pdf_path = await self.download_pdf(doc_id, pdf_url, title)
            if not pdf_path:
                logger.error(f"Failed to download PDF")
                return

            # Step 2: í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = await self.extract_text_from_pdf(pdf_path)
            if not text or len(text) < 500:
                logger.warning(f"Insufficient text extracted: {len(text)} chars")
                return

            logger.info(f"âœ… Extracted {len(text):,} characters")

            # Step 3: ì²­í¬ ë¶„í• 
            chunks = self._chunk_text(text, chunk_size=4000)
            logger.info(f"ğŸ“„ Split into {len(chunks)} chunks")

            # ì˜ˆìƒ ë¹„ìš© ë° ì‹œê°„ ê³„ì‚°
            estimated_time_minutes = len(chunks) * 2
            estimated_cost = len(chunks) * 0.15

            logger.info(f"â±ï¸  Estimated time: {estimated_time_minutes} minutes ({estimated_time_minutes/60:.1f} hours)")
            logger.info(f"ğŸ’° Estimated cost: ${estimated_cost:.2f}")
            logger.info("")

            # Step 4: LLM ì¶”ì¶œ
            logger.info("ğŸš€ Starting LLM extraction...")
            entities, relationships = self.llm_extractor.extract_from_chunks(
                chunks=chunks,
                insurer=insurer,
                product_type=product_type or "ì—°ê¸ˆë³´í—˜",
                document_id=doc_id
            )

            # Step 5: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            saved_entities, saved_relationships = await self.save_entities(
                doc_id, entities, relationships
            )

            # Step 6: ê²°ê³¼ ì¶œë ¥
            logger.info(f"\n{'=' * 80}")
            logger.info(f"âœ… LLM Extraction Complete!")
            logger.info(f"{'=' * 80}")
            logger.info(f"ğŸ“Š Results:")
            logger.info(f"   Entities: {saved_entities}")
            logger.info(f"   Relationships: {saved_relationships}")
            logger.info(f"   Ratio: {saved_relationships / saved_entities if saved_entities else 0:.2f}")
            logger.info(f"{'=' * 80}")

            # Step 7: ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ë¹„êµ
            await self.show_comparison_stats(doc_id)

        except Exception as e:
            logger.error(f"âŒ Error: {e}", exc_info=True)

    async def download_pdf(self, doc_id: str, pdf_url: str, title: str) -> Optional[Path]:
        """PDF ë‹¤ìš´ë¡œë“œ"""
        try:
            safe_filename = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in title[:50])
            pdf_filename = f"{doc_id}_{safe_filename}.pdf"
            pdf_path = PDF_DIR / pdf_filename

            if pdf_path.exists():
                logger.info(f"âœ… PDF already exists")
                return pdf_path

            logger.info(f"â¬‡ï¸  Downloading PDF...")
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()

            with open(pdf_path, 'wb') as f:
                f.write(response.content)

            logger.info(f"âœ… Downloaded: {len(response.content):,} bytes")
            return pdf_path

        except Exception as e:
            logger.error(f"âŒ Download failed: {e}")
            return None

    async def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text_parts = []

            with pdfplumber.open(pdf_path) as pdf:
                logger.info(f"ğŸ“„ Extracting text from {len(pdf.pages)} pages...")
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)

                    if page_num % 10 == 0:
                        logger.info(f"   Processed {page_num}/{len(pdf.pages)} pages")

            full_text = "\n\n".join(text_parts)
            return full_text

        except Exception as e:
            logger.error(f"âŒ Text extraction failed: {e}")
            return ""

    def _chunk_text(self, text: str, chunk_size: int = 4000) -> list:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        current_pos = 0

        while current_pos < len(text):
            chunk = text[current_pos:current_pos + chunk_size]
            chunks.append(chunk)
            current_pos += chunk_size

        return chunks

    async def save_entities(self, doc_id: str, entities: list, relationships: list) -> tuple[int, int]:
        """ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            async with AsyncSessionLocal() as db:
                # ê¸°ì¡´ ì—”í‹°í‹° ì‚­ì œ
                await db.execute(
                    sql_text("DELETE FROM knowledge_entities WHERE document_id = :doc_id"),
                    {"doc_id": doc_id}
                )

                # ì—”í‹°í‹° ì €ì¥
                entity_count = 0
                for entity in entities:
                    await db.execute(sql_text("""
                        INSERT INTO knowledge_entities (
                            entity_id, label, type, description, source_text,
                            document_id, insurer, product_type, created_at
                        ) VALUES (
                            :entity_id, :label, :type, :description, :source_text,
                            :document_id, :insurer, :product_type, :created_at
                        )
                    """), entity)
                    entity_count += 1

                # ê´€ê³„ ì €ì¥
                relationship_count = 0
                for rel in relationships:
                    try:
                        await db.execute(sql_text("""
                            INSERT INTO knowledge_relationships (
                                source_entity_id, target_entity_id, type, description, created_at
                            ) VALUES (
                                :source_entity_id, :target_entity_id, :type, :description, :created_at
                            )
                        """), rel)
                        relationship_count += 1
                    except Exception:
                        pass  # ì¤‘ë³µ ê´€ê³„ëŠ” ë¬´ì‹œ

                await db.commit()

                return entity_count, relationship_count

        except Exception as e:
            logger.error(f"âŒ Save failed: {e}", exc_info=True)
            return 0, 0

    async def show_comparison_stats(self, doc_id: str):
        """ì²˜ë¦¬ëœ ë¬¸ì„œì˜ í†µê³„ ì¶œë ¥"""
        try:
            async with AsyncSessionLocal() as db:
                # ì „ì²´ í†µê³„
                total_entities_query = "SELECT COUNT(*) FROM knowledge_entities"
                total_rels_query = "SELECT COUNT(*) FROM knowledge_relationships"

                result = await db.execute(sql_text(total_entities_query))
                total_entities = result.scalar()

                result = await db.execute(sql_text(total_rels_query))
                total_rels = result.scalar()

                # ì´ ë¬¸ì„œì˜ í†µê³„
                doc_entities_query = f"""
                    SELECT COUNT(*) FROM knowledge_entities
                    WHERE document_id = '{doc_id}'
                """
                doc_rels_query = f"""
                    SELECT COUNT(*) FROM knowledge_relationships r
                    JOIN knowledge_entities e ON r.source_entity_id = e.entity_id
                    WHERE e.document_id = '{doc_id}'
                """

                result = await db.execute(sql_text(doc_entities_query))
                doc_entities = result.scalar()

                result = await db.execute(sql_text(doc_rels_query))
                doc_rels = result.scalar()

                logger.info(f"\n{'=' * 80}")
                logger.info(f"ğŸ“Š Database Statistics:")
                logger.info(f"")
                logger.info(f"   ì „ì²´ ê·¸ë˜í”„:")
                logger.info(f"   - Entities: {total_entities:,}")
                logger.info(f"   - Relationships: {total_rels:,}")
                logger.info(f"   - Ratio: {total_rels / total_entities if total_entities else 0:.2f}")
                logger.info(f"")
                logger.info(f"   ì´ ë¬¸ì„œ (LLM ì²˜ë¦¬):")
                logger.info(f"   - Entities: {doc_entities:,}")
                logger.info(f"   - Relationships: {doc_rels:,}")
                logger.info(f"   - Ratio: {doc_rels / doc_entities if doc_entities else 0:.2f}")
                logger.info(f"{'=' * 80}")

        except Exception as e:
            logger.error(f"Failed to show comparison stats: {e}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = SingleDocumentTester()
    await tester.run()


if __name__ == "__main__":
    asyncio.run(main())
