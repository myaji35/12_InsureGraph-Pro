"""
ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ Upstage ì¶”ì¶œ ê²°ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸

í˜„ì¬ ì‹œìŠ¤í…œì— í•™ìŠµëœ ë¬¸ì„œ 5ê±´ì„ ì„ íƒí•˜ì—¬:
1. ê¸°ì¡´ ì¶”ì¶œ ë°©ì‹ (pdfplumber) ê²°ê³¼
2. Upstage Document Parse ê²°ê³¼
ë¥¼ ë¹„êµí•˜ê³  í’ˆì§ˆ, ì‹œê°„, í•´ì„ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

Usage:
    python test_extraction_comparison.py
"""
import asyncio
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import re

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))

from app.core.database import get_db
from app.services.upstage_document_parser import UpstageDocumentParser
from app.services.streaming_pdf_processor import StreamingPDFProcessor
from loguru import logger


async def get_sample_documents(limit: int = 5) -> List[Dict[str, Any]]:
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•™ìŠµëœ ë¬¸ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°

    Args:
        limit: ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜

    Returns:
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"Fetching {limit} sample documents from database...")

    db = next(get_db())

    try:
        # crawler_documents í…Œì´ë¸”ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ì¡°íšŒ
        query = """
            SELECT
                id,
                insurer,
                title,
                pdf_url,
                category,
                product_type,
                status,
                metadata,
                created_at
            FROM crawler_documents
            WHERE status IN ('processed', 'completed')
            ORDER BY created_at DESC
            LIMIT %s
        """

        result = db.execute(query, (limit,))
        documents = []

        for row in result.fetchall():
            doc = {
                'id': str(row[0]),
                'insurer': row[1],
                'title': row[2],
                'pdf_url': row[3],
                'category': row[4],
                'product_type': row[5],
                'status': row[6],
                'metadata': row[7] or {},
                'created_at': row[8]
            }
            documents.append(doc)

        logger.info(f"âœ… Found {len(documents)} documents")
        return documents

    except Exception as e:
        logger.error(f"Failed to fetch documents: {e}")
        # Fallback: í•˜ë“œì½”ë”©ëœ ìƒ˜í”Œ URL ì‚¬ìš©
        logger.warning("Using hardcoded sample URLs as fallback")
        return get_fallback_sample_urls()
    finally:
        db.close()


def get_fallback_sample_urls() -> List[Dict[str, Any]]:
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ìƒ˜í”Œ URL
    """
    return [
        {
            'id': 'sample-1',
            'insurer': 'ì‚¼ì„±í™”ì¬',
            'title': '(ë¬´ë°°ë‹¹)ì‚¼ì„±í™”ì¬ìë™ì°¨ë³´í—˜ ì•½ê´€',
            'pdf_url': 'https://www.samsungfire.com/static/kr/down/terms/Auto_Insurance_Terms.pdf',
            'category': 'ì•½ê´€',
            'product_type': 'ìë™ì°¨ë³´í—˜',
            'status': 'processed',
            'metadata': {},
            'created_at': datetime.now()
        },
        {
            'id': 'sample-2',
            'insurer': 'KBì†í•´ë³´í—˜',
            'title': 'ë¬´ë°°ë‹¹ KB 5.10.10 ê±´ê°•ë³´í—˜ ì•½ê´€',
            'pdf_url': 'https://www.kbinsure.co.kr/CG302120N.ec',
            'category': 'ì•½ê´€',
            'product_type': 'ê±´ê°•ë³´í—˜',
            'status': 'processed',
            'metadata': {},
            'created_at': datetime.now()
        }
    ]


def calculate_text_quality_score(text: str) -> Dict[str, float]:
    """
    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°

    Returns:
        {
            'korean_ratio': í•œê¸€ ë¹„ìœ¨,
            'structure_score': êµ¬ì¡°í™” ì ìˆ˜,
            'readability_score': ê°€ë…ì„± ì ìˆ˜,
            'overall_score': ì¢…í•© ì ìˆ˜
        }
    """
    if not text:
        return {
            'korean_ratio': 0.0,
            'structure_score': 0.0,
            'readability_score': 0.0,
            'overall_score': 0.0
        }

    # 1. í•œê¸€ ë¹„ìœ¨ (ë³´í—˜ì•½ê´€ì€ í•œê¸€ì´ ë§ì•„ì•¼ í•¨)
    korean_chars = sum(1 for c in text if ord('ê°€') <= ord(c) <= ord('í£'))
    korean_ratio = korean_chars / len(text) if text else 0

    # 2. êµ¬ì¡°í™” ì ìˆ˜ (ì œ1ì¡°, ì œ1ì¥ ë“±ì˜ íŒ¨í„´ ì¸ì‹)
    chapter_pattern = re.findall(r'ì œ\d+ì¥', text)
    article_pattern = re.findall(r'ì œ\d+ì¡°', text)
    structure_score = min((len(chapter_pattern) + len(article_pattern)) / 50, 1.0)

    # 3. ê°€ë…ì„± ì ìˆ˜ (í‰ê·  ì¤„ ê¸¸ì´, íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨)
    lines = [line for line in text.split('\n') if line.strip()]
    avg_line_length = sum(len(line) for line in lines) / len(lines) if lines else 0

    special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
    special_ratio = special_chars / len(text) if text else 0

    readability_score = 0.0
    readability_score += min(avg_line_length / 50, 0.5)  # ì ì ˆí•œ ì¤„ ê¸¸ì´
    readability_score += max(0.5 - special_ratio, 0)  # íŠ¹ìˆ˜ë¬¸ìê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ

    # 4. ì¢…í•© ì ìˆ˜
    overall_score = (
        korean_ratio * 0.4 +
        structure_score * 0.3 +
        readability_score * 0.3
    )

    return {
        'korean_ratio': round(korean_ratio, 3),
        'structure_score': round(structure_score, 3),
        'readability_score': round(readability_score, 3),
        'overall_score': round(overall_score, 3)
    }


def calculate_uds_interpretation(
    text: str,
    sections: List[Dict] = None,
    tables: List[Dict] = None
) -> Dict[str, Any]:
    """
    UDS (Understanding, Detail, Structure) í•´ì„ë ¥ ê³„ì‚°

    Returns:
        {
            'understanding': ì´í•´ë„ ì ìˆ˜ (0-100),
            'detail': ìƒì„¸ë„ ì ìˆ˜ (0-100),
            'structure': êµ¬ì¡°í™” ì ìˆ˜ (0-100),
            'total': ì´ì  (0-100)
        }
    """
    # 1. Understanding (ì´í•´ë„): í…ìŠ¤íŠ¸ í’ˆì§ˆ ê¸°ë°˜
    quality = calculate_text_quality_score(text)
    understanding = quality['overall_score'] * 100

    # 2. Detail (ìƒì„¸ë„): í…ìŠ¤íŠ¸ ê¸¸ì´ ë° í‘œ ìˆ˜ ê¸°ë°˜
    text_length = len(text)
    detail_from_text = min(text_length / 50000, 1.0) * 60  # ìµœëŒ€ 60ì 
    detail_from_tables = min(len(tables) if tables else 0, 20) * 2  # ìµœëŒ€ 40ì 
    detail = detail_from_text + detail_from_tables

    # 3. Structure (êµ¬ì¡°í™”): ì„¹ì…˜ ë¶„ì„ ê¸°ë°˜
    if sections:
        # ì„¹ì…˜ì´ ìˆìœ¼ë©´ êµ¬ì¡°í™”ê°€ ì˜ ëœ ê²ƒ
        num_chapters = sum(1 for s in sections if s.get('type') == 'chapter')
        num_articles = sum(1 for s in sections if s.get('type') == 'article')

        structure = min((num_chapters * 5 + num_articles * 2) / 100 * 100, 100)
    else:
        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
        chapter_pattern = len(re.findall(r'ì œ\d+ì¥', text))
        article_pattern = len(re.findall(r'ì œ\d+ì¡°', text))

        structure = min((chapter_pattern * 5 + article_pattern * 2) / 100 * 100, 100)

    # 4. Total (ì´ì )
    total = (understanding * 0.3 + detail * 0.3 + structure * 0.4)

    return {
        'understanding': round(understanding, 1),
        'detail': round(detail, 1),
        'structure': round(structure, 1),
        'total': round(total, 1)
    }


async def compare_extraction_methods(document: Dict[str, Any]) -> Dict[str, Any]:
    """
    í•˜ë‚˜ì˜ ë¬¸ì„œì— ëŒ€í•´ ê¸°ì¡´ ë°©ì‹ê³¼ Upstage ë°©ì‹ì„ ë¹„êµ

    Args:
        document: ë¬¸ì„œ ì •ë³´

    Returns:
        ë¹„êµ ê²°ê³¼
    """
    pdf_url = document['pdf_url']

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“„ Document: {document['title']}")
    logger.info(f"   Insurer: {document['insurer']}")
    logger.info(f"   URL: {pdf_url}")
    logger.info(f"{'='*80}")

    processor = StreamingPDFProcessor()
    comparison = {
        'document': document,
        'pdfplumber': {},
        'upstage': {},
        'upstage_smart': {}
    }

    # ========================================
    # 1. pdfplumber (ê¸°ì¡´ ë°©ì‹)
    # ========================================
    logger.info("\n[1] Testing pdfplumber (current method)...")
    try:
        start_time = time.time()
        result_pdf = await processor.process_pdf_streaming(
            pdf_url,
            use_upstage=False
        )
        elapsed_pdf = time.time() - start_time

        quality_pdf = calculate_text_quality_score(result_pdf['text'])
        uds_pdf = calculate_uds_interpretation(result_pdf['text'])

        comparison['pdfplumber'] = {
            'success': True,
            'elapsed_time': round(elapsed_pdf, 2),
            'total_pages': result_pdf['total_pages'],
            'text_length': len(result_pdf['text']),
            'method': result_pdf['method'],
            'quality': quality_pdf,
            'uds': uds_pdf,
            'sections_count': 0,
            'tables_count': 0
        }

        logger.info(f"âœ… pdfplumber completed in {elapsed_pdf:.2f}s")
        logger.info(f"   Pages: {result_pdf['total_pages']}, Text: {len(result_pdf['text']):,} chars")
        logger.info(f"   Quality: {quality_pdf['overall_score']:.3f}, UDS Total: {uds_pdf['total']:.1f}")

    except Exception as e:
        logger.error(f"âŒ pdfplumber failed: {e}")
        comparison['pdfplumber'] = {'success': False, 'error': str(e)}

    # ========================================
    # 2. Upstage Document Parse (ì¼ë°˜)
    # ========================================
    logger.info("\n[2] Testing Upstage Document Parse...")
    try:
        start_time = time.time()
        result_upstage = await processor.process_pdf_streaming(
            pdf_url,
            use_upstage=True,
            extract_tables=True,
            smart_chunking=False
        )
        elapsed_upstage = time.time() - start_time

        quality_upstage = calculate_text_quality_score(result_upstage['text'])
        uds_upstage = calculate_uds_interpretation(
            result_upstage['text'],
            sections=result_upstage.get('sections', []),
            tables=result_upstage.get('tables', [])
        )

        comparison['upstage'] = {
            'success': True,
            'elapsed_time': round(elapsed_upstage, 2),
            'total_pages': result_upstage['total_pages'],
            'text_length': len(result_upstage['text']),
            'method': result_upstage['method'],
            'quality': quality_upstage,
            'quality_score': result_upstage.get('quality_score', 0),
            'uds': uds_upstage,
            'sections_count': len(result_upstage.get('sections', [])),
            'tables_count': len(result_upstage.get('tables', []))
        }

        logger.info(f"âœ… Upstage completed in {elapsed_upstage:.2f}s")
        logger.info(f"   Pages: {result_upstage['total_pages']}, Text: {len(result_upstage['text']):,} chars")
        logger.info(f"   Sections: {len(result_upstage.get('sections', []))}, Tables: {len(result_upstage.get('tables', []))}")
        logger.info(f"   Quality: {quality_upstage['overall_score']:.3f}, UDS Total: {uds_upstage['total']:.1f}")

    except Exception as e:
        logger.error(f"âŒ Upstage failed: {e}")
        comparison['upstage'] = {'success': False, 'error': str(e)}

    # ========================================
    # 3. Upstage Smart Chunking
    # ========================================
    logger.info("\n[3] Testing Upstage Smart Chunking...")
    try:
        start_time = time.time()
        result_smart = await processor.process_pdf_streaming(
            pdf_url,
            use_upstage=True,
            extract_tables=True,
            smart_chunking=True
        )
        elapsed_smart = time.time() - start_time

        quality_smart = calculate_text_quality_score(result_smart['text'])
        uds_smart = calculate_uds_interpretation(
            result_smart['text'],
            sections=result_smart.get('sections', []),
            tables=result_smart.get('tables', [])
        )

        comparison['upstage_smart'] = {
            'success': True,
            'elapsed_time': round(elapsed_smart, 2),
            'total_pages': result_smart['total_pages'],
            'text_length': len(result_smart['text']),
            'chunks_count': len(result_smart.get('chunks', [])),
            'method': result_smart['method'],
            'quality': quality_smart,
            'quality_score': result_smart.get('quality_score', 0),
            'uds': uds_smart,
            'sections_count': len(result_smart.get('sections', [])),
            'tables_count': len(result_smart.get('tables', []))
        }

        logger.info(f"âœ… Upstage Smart Chunking completed in {elapsed_smart:.2f}s")
        logger.info(f"   Pages: {result_smart['total_pages']}, Chunks: {len(result_smart.get('chunks', []))}")
        logger.info(f"   Quality: {quality_smart['overall_score']:.3f}, UDS Total: {uds_smart['total']:.1f}")

    except Exception as e:
        logger.error(f"âŒ Upstage Smart Chunking failed: {e}")
        comparison['upstage_smart'] = {'success': False, 'error': str(e)}

    return comparison


def print_summary_table(all_comparisons: List[Dict[str, Any]]):
    """
    ì „ì²´ ë¹„êµ ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    """
    print("\n" + "="*120)
    print("ğŸ“Š ì „ì²´ ë¹„êµ ê²°ê³¼ ìš”ì•½")
    print("="*120)

    # í—¤ë”
    print(f"\n{'ë¬¸ì„œëª…':30s} | {'ë°©ì‹':15s} | {'ì‹œê°„(s)':8s} | {'í…ìŠ¤íŠ¸':10s} | {'í’ˆì§ˆ':6s} | {'UDS':6s} | {'ì„¹ì…˜':6s} | {'í‘œ':6s}")
    print("-" * 120)

    # ê° ë¬¸ì„œë³„ ê²°ê³¼
    for comp in all_comparisons:
        doc = comp['document']
        doc_name = doc['title'][:28] + '..' if len(doc['title']) > 30 else doc['title']

        for method_name, method_data in [
            ('pdfplumber', comp['pdfplumber']),
            ('Upstage', comp['upstage']),
            ('Upstage Smart', comp['upstage_smart'])
        ]:
            if method_data.get('success'):
                print(
                    f"{doc_name:30s} | "
                    f"{method_name:15s} | "
                    f"{method_data['elapsed_time']:8.2f} | "
                    f"{method_data['text_length']:10,d} | "
                    f"{method_data['quality']['overall_score']:6.3f} | "
                    f"{method_data['uds']['total']:6.1f} | "
                    f"{method_data['sections_count']:6d} | "
                    f"{method_data['tables_count']:6d}"
                )
            else:
                print(
                    f"{doc_name:30s} | "
                    f"{method_name:15s} | "
                    f"{'FAILED':8s} | "
                    f"{'-':10s} | "
                    f"{'-':6s} | "
                    f"{'-':6s} | "
                    f"{'-':6s} | "
                    f"{'-':6s}"
                )
        print("-" * 120)

    # í‰ê·  ê³„ì‚°
    print("\nğŸ“ˆ í‰ê·  ì ìˆ˜:")

    for method_name, key in [
        ('pdfplumber', 'pdfplumber'),
        ('Upstage', 'upstage'),
        ('Upstage Smart', 'upstage_smart')
    ]:
        successful = [c[key] for c in all_comparisons if c[key].get('success')]
        if successful:
            avg_time = sum(d['elapsed_time'] for d in successful) / len(successful)
            avg_quality = sum(d['quality']['overall_score'] for d in successful) / len(successful)
            avg_uds = sum(d['uds']['total'] for d in successful) / len(successful)
            avg_sections = sum(d['sections_count'] for d in successful) / len(successful)
            avg_tables = sum(d['tables_count'] for d in successful) / len(successful)

            print(
                f"  {method_name:15s}: "
                f"ì‹œê°„={avg_time:6.2f}s, "
                f"í’ˆì§ˆ={avg_quality:6.3f}, "
                f"UDS={avg_uds:6.1f}, "
                f"ì„¹ì…˜={avg_sections:6.1f}, "
                f"í‘œ={avg_tables:6.1f}"
            )

    # ê°œì„ ìœ¨ ê³„ì‚°
    print("\nğŸ’¡ Upstage vs pdfplumber ê°œì„ ìœ¨:")

    pdf_successful = [c['pdfplumber'] for c in all_comparisons if c['pdfplumber'].get('success')]
    upstage_successful = [c['upstage'] for c in all_comparisons if c['upstage'].get('success')]

    if pdf_successful and upstage_successful:
        pdf_avg_quality = sum(d['quality']['overall_score'] for d in pdf_successful) / len(pdf_successful)
        upstage_avg_quality = sum(d['quality']['overall_score'] for d in upstage_successful) / len(upstage_successful)

        pdf_avg_uds = sum(d['uds']['total'] for d in pdf_successful) / len(pdf_successful)
        upstage_avg_uds = sum(d['uds']['total'] for d in upstage_successful) / len(upstage_successful)

        quality_improvement = ((upstage_avg_quality - pdf_avg_quality) / pdf_avg_quality * 100) if pdf_avg_quality > 0 else 0
        uds_improvement = ((upstage_avg_uds - pdf_avg_uds) / pdf_avg_uds * 100) if pdf_avg_uds > 0 else 0

        print(f"  í’ˆì§ˆ ì ìˆ˜: {quality_improvement:+.1f}%")
        print(f"  UDS í•´ì„ë ¥: {uds_improvement:+.1f}%")

        if quality_improvement > 10 or uds_improvement > 10:
            print(f"\n  âœ… Upstageê°€ pdfplumber ëŒ€ë¹„ ìœ ì˜ë¯¸í•œ ê°œì„ ì„ ë³´ì…ë‹ˆë‹¤!")
            print(f"  ğŸ¯ ë³´í—˜ì•½ê´€ í•™ìŠµì— Upstage ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            print(f"\n  âš ï¸  ê°œì„  íš¨ê³¼ê°€ ë¯¸ë¯¸í•©ë‹ˆë‹¤. ë¬¸ì„œ íŠ¹ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("ğŸ”¬ ë³´í—˜ì•½ê´€ ì¶”ì¶œ ë°©ì‹ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print("\nê¸°ì¡´ ë°©ì‹ (pdfplumber) vs Upstage Document Parse")
    print("í‰ê°€ í•­ëª©: ì²˜ë¦¬ ì‹œê°„, í…ìŠ¤íŠ¸ í’ˆì§ˆ, UDS í•´ì„ë ¥, ì„¹ì…˜ ë¶„ì„, í‘œ ì¶”ì¶œ\n")

    # 1. ìƒ˜í”Œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    documents = await get_sample_documents(limit=5)

    if not documents:
        print("âŒ í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nâœ… {len(documents)}ê°œ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n")

    # 2. ê° ë¬¸ì„œë³„ ë¹„êµ
    all_comparisons = []

    for i, doc in enumerate(documents, 1):
        print(f"\n\n{'#'*80}")
        print(f"í…ŒìŠ¤íŠ¸ ì§„í–‰: {i}/{len(documents)}")
        print(f"{'#'*80}")

        comparison = await compare_extraction_methods(doc)
        all_comparisons.append(comparison)

        # ì ì‹œ ëŒ€ê¸° (API rate limit ê³ ë ¤)
        if i < len(documents):
            await asyncio.sleep(2)

    # 3. ìš”ì•½ í…Œì´ë¸” ì¶œë ¥
    print_summary_table(all_comparisons)

    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    asyncio.run(main())
