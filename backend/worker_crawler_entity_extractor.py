"""
PDF Download, Text Extraction, and LLM-Based Entity Extraction Worker

71ê°œì˜ ì™„ë£Œëœ crawler_documentsì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ í›„,
Claude API ê¸°ë°˜ LLM ì—”í‹°í‹° ì¶”ì¶œê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í’ë¶€í•œ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""
import sys
import os
import asyncio
from typing import List, Dict, Optional
from pathlib import Path

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import requests
import pdfplumber
from loguru import logger
from sqlalchemy import text as sql_text

from app.core.database import AsyncSessionLocal
from app.services.llm_entity_extractor import LLMEntityExtractor

# PDF ì €ì¥ ê²½ë¡œ
PDF_DIR = Path(__file__).parent / "data" / "pdfs"
PDF_DIR.mkdir(parents=True, exist_ok=True)


class CrawlerDocumentEntityExtractor:
    """crawler_documents í…Œì´ë¸”ì˜ PDFì—ì„œ LLM ê¸°ë°˜ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ëŠ” ì›Œì»¤"""

    def __init__(self):
        """LLM ì¶”ì¶œê¸° ì´ˆê¸°í™”"""
        self.llm_extractor = LLMEntityExtractor()

    async def run(self, limit: Optional[int] = None):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        logger.info("=" * 80)
        logger.info("ğŸ“š Crawler Document LLM Entity Extractor Worker Started")
        logger.info("=" * 80)

        # 1. ì™„ë£Œëœ ë¬¸ì„œ ì¡°íšŒ (crawler_documents í…Œì´ë¸”)
        async with AsyncSessionLocal() as db:
            query_str = """
                SELECT id, title, insurer, category, product_type, pdf_url
                FROM crawler_documents
                WHERE status = 'completed'
                ORDER BY created_at DESC
            """
            if limit:
                query_str += f" LIMIT {limit}"

            result = await db.execute(sql_text(query_str))
            documents = result.fetchall()

        logger.info(f"ğŸ“„ Found {len(documents)} completed documents")

        # 2. ê° ë¬¸ì„œ ì²˜ë¦¬
        total_entities = 0
        total_relationships = 0
        processed_count = 0
        failed_count = 0

        for doc in documents:
            doc_id, title, insurer, category, product_type, pdf_url = doc
            doc_id = str(doc_id)

            try:
                logger.info(f"\n{'=' * 80}")
                logger.info(f"ğŸ“„ Processing: {title[:50]}...")
                logger.info(f"   ë³´í—˜ì‚¬: {insurer}, ì¹´í…Œê³ ë¦¬: {category}")

                # Step 1: PDF ë‹¤ìš´ë¡œë“œ
                pdf_path = await self.download_pdf(doc_id, pdf_url, title)
                if not pdf_path:
                    logger.error(f"Failed to download PDF for {doc_id}")
                    failed_count += 1
                    continue

                # Step 2: í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = await self.extract_text_from_pdf(pdf_path)
                if not text or len(text) < 500:
                    logger.warning(f"Insufficient text extracted: {len(text)} chars")
                    failed_count += 1
                    continue

                logger.info(f"âœ… Extracted {len(text):,} characters")

                # Step 3: LLM ê¸°ë°˜ ì—”í‹°í‹° ë° ê´€ê³„ ì¶”ì¶œ (ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬)
                chunks = self._chunk_text(text, chunk_size=4000)
                logger.info(f"   ğŸ“„ Split into {len(chunks)} chunks for LLM processing")

                entities, relationships = self.llm_extractor.extract_from_chunks(
                    chunks=chunks,
                    insurer=insurer,
                    product_type=product_type,
                    document_id=doc_id
                )

                # Step 5: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                saved_entities, saved_relationships = await self.save_entities(
                    doc_id, entities, relationships
                )

                total_entities += saved_entities
                total_relationships += saved_relationships
                processed_count += 1

                logger.info(f"âœ… Saved {saved_entities} entities, {saved_relationships} relationships")

            except Exception as e:
                logger.error(f"âŒ Error processing {doc_id}: {e}", exc_info=True)
                failed_count += 1

        # 3. ìµœì¢… í†µê³„
        logger.info(f"\n{'=' * 80}")
        logger.info(f"ğŸ“Š Final Statistics:")
        logger.info(f"   Total Documents: {len(documents)}")
        logger.info(f"   Processed: {processed_count}")
        logger.info(f"   Failed: {failed_count}")
        logger.info(f"   Total Entities: {total_entities}")
        logger.info(f"   Total Relationships: {total_relationships}")
        logger.info(f"{'=' * 80}")

    async def download_pdf(self, doc_id: str, pdf_url: str, title: str) -> Optional[Path]:
        """PDF ë‹¤ìš´ë¡œë“œ"""
        try:
            # íŒŒì¼ëª… ìƒì„±
            safe_filename = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in title[:50])
            pdf_filename = f"{doc_id}_{safe_filename}.pdf"
            pdf_path = PDF_DIR / pdf_filename

            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
            if pdf_path.exists():
                logger.info(f"   PDF already exists: {pdf_filename}")
                return pdf_path

            # PDF ë‹¤ìš´ë¡œë“œ
            logger.info(f"   Downloading PDF from {pdf_url[:60]}...")
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()

            # íŒŒì¼ ì €ì¥
            with open(pdf_path, 'wb') as f:
                f.write(response.content)

            logger.info(f"   âœ… Downloaded: {len(response.content):,} bytes")
            return pdf_path

        except Exception as e:
            logger.error(f"   âŒ Download failed: {e}")
            return None

    async def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text_parts = []

            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)

            full_text = "\n\n".join(text_parts)
            return full_text

        except Exception as e:
            logger.error(f"   âŒ Text extraction failed: {e}")
            return ""

    def _chunk_text(self, text: str, chunk_size: int = 4000) -> List[str]:
        """
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (LLM ì»¨í…ìŠ¤íŠ¸ ì œí•œ ëŒ€ì‘)

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)

        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        current_pos = 0

        while current_pos < len(text):
            # ì²­í¬ ì¶”ì¶œ
            chunk = text[current_pos:current_pos + chunk_size]
            chunks.append(chunk)
            current_pos += chunk_size

        return chunks

    async def save_entities(
        self,
        doc_id: str,
        entities: List[Dict],
        relationships: List[Dict]
    ) -> tuple[int, int]:
        """ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            async with AsyncSessionLocal() as db:
                # ê¸°ì¡´ ì—”í‹°í‹° ì‚­ì œ
                await db.execute(
                    sql_text("DELETE FROM knowledge_entities WHERE document_id = :doc_id"),
                    {"doc_id": doc_id}
                )

                # ì—”í‹°í‹° ì €ì¥
                entity_count = 0
                for entity in entities:
                    await db.execute(sql_text("""
                        INSERT INTO knowledge_entities (
                            entity_id, label, type, description, source_text,
                            document_id, insurer, product_type, created_at
                        ) VALUES (
                            :entity_id, :label, :type, :description, :source_text,
                            :document_id, :insurer, :product_type, :created_at
                        )
                    """), entity)
                    entity_count += 1

                # ê´€ê³„ ì €ì¥
                relationship_count = 0
                for rel in relationships:
                    try:
                        await db.execute(sql_text("""
                            INSERT INTO knowledge_relationships (
                                source_entity_id, target_entity_id, type, description, created_at
                            ) VALUES (
                                :source_entity_id, :target_entity_id, :type, :description, :created_at
                            )
                        """), rel)
                        relationship_count += 1
                    except Exception:
                        pass  # ì¤‘ë³µ ê´€ê³„ëŠ” ë¬´ì‹œ

                await db.commit()

                return entity_count, relationship_count

        except Exception as e:
            logger.error(f"   âŒ Save failed: {e}", exc_info=True)
            return 0, 0


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys

    # ëª…ë ¹ì¤„ ì¸ìë¡œ limit ë°›ê¸°
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 5  # ê¸°ë³¸ 5ê°œ

    extractor = CrawlerDocumentEntityExtractor()
    await extractor.run(limit=limit)


if __name__ == "__main__":
    asyncio.run(main())
