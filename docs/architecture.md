# Architecture Document: InsureGraph Pro

**Project**: InsureGraph Pro
**Document Type**: Technical Architecture (BMAD Method)
**Version**: 1.0
**Date**: 2025-11-25
**Author**: Architect
**Status**: Draft (Pending Review)

---

## ğŸ“‹ Executive Summary

This document defines the technical architecture for InsureGraph Pro, a GraphRAG-based insurance policy analysis platform. The architecture is designed to support:

- **High Accuracy**: 4-layer defense against LLM hallucination
- **Complex Reasoning**: Multi-hop graph traversal for policy comparison
- **Regulatory Compliance**: Financial sandbox requirements & data privacy
- **Scalability**: Support for 500+ policies and 10,000+ FP users in Phase 3

**Key Architectural Decisions**:
- Hybrid approach: Rule-based + LLM for critical data accuracy
- Neo4j as unified graph + vector database (Phase 1)
- FastAPI + LangGraph for multi-agent orchestration
- AWS EKS for logical network separation compliance

---

## ğŸ—ï¸ System Architecture Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Client Layer                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Web App (Next.js)  â”‚  Mobile PWA  â”‚  Kakao Integration (Phase2)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTPS/JWT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Gateway (Kong)                           â”‚
â”‚  - Authentication  - Rate Limiting  - Request Routing            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer (FastAPI)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Ingestion API  â”‚  â”‚   Query API     â”‚  â”‚ Compliance API  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                    â”‚                     â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           LangGraph Multi-Agent Orchestrator               â”‚ â”‚
â”‚  â”‚  Parser â†’ Extractor â†’ Validator â†’ Reasoner â†’ Formatter    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Layer   â”‚                             â”‚   LLM Layer    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Neo4j Graph   â”‚                             â”‚ Upstage Solar  â”‚
â”‚  + Vector Indexâ”‚                             â”‚    GPT-4o      â”‚
â”‚                â”‚                             â”‚   (Fallback)   â”‚
â”‚  PostgreSQL    â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  (Metadata)    â”‚
â”‚                â”‚
â”‚  S3 (Files)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

| Component | Responsibility | Technology |
|-----------|---------------|------------|
| **Web Client** | FP workspace, graph visualization | Next.js 14, Cytoscape.js |
| **API Gateway** | Auth, rate limiting, routing | Kong Gateway |
| **Ingestion Service** | PDF parsing, entity extraction | FastAPI, LangGraph |
| **Query Service** | GraphRAG query execution | FastAPI, LangGraph |
| **Graph Database** | Knowledge graph storage & traversal | Neo4j Enterprise |
| **Vector Search** | Semantic search for clauses | Neo4j Vector Index |
| **Metadata DB** | User, session, audit logs | PostgreSQL 15 |
| **Object Storage** | PDF files, generated reports | AWS S3 |
| **LLM Orchestration** | Model selection, fallback logic | LangChain + custom |

---

## ğŸ”Œ API Architecture

### API Design Principles

1. **RESTful** with resource-based URLs
2. **JSON API** standard for error handling
3. **Versioning**: `/api/v1/...` for backward compatibility
4. **Authentication**: JWT with refresh token
5. **Rate Limiting**: Per-user tier limits

### Core API Endpoints

#### 1. Ingestion API

**POST /api/v1/policies/ingest**

Upload and process insurance policy PDF.

```json
Request:
POST /api/v1/policies/ingest
Content-Type: multipart/form-data

{
  "file": <binary>,
  "metadata": {
    "insurer": "Samsung Life",
    "product_name": "Cancer Insurance Premium",
    "launch_date": "2020-03-15",
    "product_code": "SL-CI-001"
  }
}

Response (202 Accepted):
{
  "job_id": "job_12345",
  "status": "processing",
  "estimated_time": 180,  // seconds
  "webhook_url": "https://api.insuregraph.com/webhooks/job_12345"
}
```

**GET /api/v1/policies/ingest/{job_id}/status**

Check ingestion job status.

```json
Response:
{
  "job_id": "job_12345",
  "status": "completed",  // processing, completed, failed
  "progress": 100,
  "results": {
    "product_id": "prod_67890",
    "nodes_created": 1247,
    "edges_created": 3521,
    "clauses_parsed": 89,
    "errors": []
  },
  "created_at": "2025-11-25T10:30:00Z",
  "completed_at": "2025-11-25T10:33:15Z"
}
```

#### 2. Query API

**POST /api/v1/analysis/query**

Execute natural language query against knowledge graph.

```json
Request:
{
  "query": "ê°‘ìƒì„ ì•” ë³´ì¥ë¼ìš”?",
  "context": {
    "product_ids": ["prod_67890"],  // optional: specific products
    "customer_profile": {            // optional: for personalized analysis
      "age": 35,
      "gender": "F",
      "existing_policies": []
    }
  },
  "options": {
    "include_reasoning_path": true,
    "max_hops": 3,
    "confidence_threshold": 0.7
  }
}

Response:
{
  "query_id": "qry_98765",
  "answer": {
    "summary": "ê°‘ìƒì„ ì•”(C77)ì€ ë‹´ë³´ì— í¬í•¨ë˜ë‚˜, 90ì¼ ë©´ì±…ê¸°ê°„ì´ ì ìš©ë©ë‹ˆë‹¤.",
    "confidence": 0.92,
    "status": "high_confidence",  // high_confidence, medium, needs_review
    "details": [
      {
        "product": "Cancer Insurance Premium",
        "coverage": "ì•”ì§„ë‹¨íŠ¹ì•½",
        "is_covered": true,
        "conditions": [
          {
            "type": "waiting_period",
            "days": 90,
            "description": "ê³„ì•½ì¼ë¡œë¶€í„° 90ì¼ ì´í›„ ë°œìƒí•œ ê°‘ìƒì„ ì•”ì€ ë³´ì¥"
          }
        ],
        "payment_amount": 100000000,
        "exclusions": []
      }
    ]
  },
  "reasoning_path": {
    "graph_visualization": {
      "nodes": [...],
      "edges": [...]
    },
    "cypher_query": "MATCH (p:Product)...",
    "execution_time_ms": 342
  },
  "sources": [
    {
      "clause_id": "clause_123",
      "article": "ì œ10ì¡°",
      "paragraph": "â‘ í•­",
      "page": 15,
      "excerpt": "ë‹¤ë§Œ, ê°‘ìƒì„ ì˜ ì•…ì„±ì‹ ìƒë¬¼(C77)ì€ ê³„ì•½ì¼ë¡œë¶€í„° 90ì¼ ì´í›„..."
    }
  ],
  "warnings": [],
  "disclaimer": "ë³¸ ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©°, ìµœì¢… íŒë‹¨ì€ ë³´í—˜ì‚¬ê°€ í•©ë‹ˆë‹¤."
}
```

**POST /api/v1/analysis/gap-analysis**

Analyze customer's coverage gaps.

```json
Request:
{
  "customer_id": "cust_111",
  "current_policies": [
    {
      "product_id": "prod_67890",
      "purchase_date": "2015-06-01",
      "coverages": [...]
    }
  ],
  "target_profile": {
    "age": 35,
    "occupation": "office_worker",
    "health_concerns": ["cancer", "cardiovascular"]
  }
}

Response:
{
  "analysis_id": "gap_222",
  "gaps": [
    {
      "type": "coverage_gap",
      "severity": "high",
      "description": "ê°‘ìƒì„  ë¦¼í”„ì ˆ ì „ì´ì•”ì´ ì¼ë°˜ì•”ìœ¼ë¡œ ë¶„ë¥˜ë˜ì§€ ì•ŠëŠ” 2015ë…„ ì•½ê´€",
      "impact": {
        "potential_loss": 50000000,
        "probability": "medium"
      },
      "recommendations": [
        {
          "action": "upgrade_policy",
          "suggested_product_id": "prod_99999",
          "reasoning": "2020ë…„ ì´í›„ ì•½ê´€ì€ ë¦¼í”„ì ˆ ì „ì´ë¥¼ ì¼ë°˜ì•”ìœ¼ë¡œ ì¸ì •"
        }
      ]
    }
  ],
  "opportunities": [
    {
      "type": "claim_opportunity",
      "description": "ê¸°ì¡´ ë³´í—˜ì—ì„œ ì²­êµ¬ ê°€ëŠ¥í•œ í•­ëª© ë°œê²¬",
      "estimated_amount": 3000000,
      "required_actions": ["ì§„ë‹¨ì„œ ì œì¶œ"]
    }
  ],
  "score": {
    "overall": 65,  // 0-100
    "cancer": 70,
    "cardiovascular": 80,
    "disability": 45
  }
}
```

**POST /api/v1/analysis/compare**

Compare multiple insurance products.

```json
Request:
{
  "product_ids": ["prod_67890", "prod_88888"],
  "comparison_criteria": [
    "coverage_overlap",
    "cost_benefit",
    "claim_conditions"
  ]
}

Response:
{
  "comparison_id": "cmp_333",
  "products": [
    {
      "product_id": "prod_67890",
      "name": "Samsung Cancer Insurance",
      "strengths": ["ê´‘ë²”ìœ„í•œ ì†Œì•¡ì•” ë³´ì¥"],
      "weaknesses": ["ë¹„ë¡€ë³´ìƒ 50%"]
    },
    {
      "product_id": "prod_88888",
      "name": "Hanwha CI Insurance",
      "strengths": ["ë¹„ë¡€ë³´ìƒ 100%"],
      "weaknesses": ["ê°‘ìƒì„ ì•” ì œì™¸"]
    }
  ],
  "overlaps": [
    {
      "disease": "ê°‘ìƒì„ ì•”(C77)",
      "overlap_type": "proportional",
      "combined_payout": 75000000,
      "individual_payouts": {
        "prod_67890": 50000000,
        "prod_88888": 25000000
      }
    }
  ],
  "recommendations": {
    "best_for_customer": "prod_67890",
    "reasoning": "ê³ ê° í”„ë¡œí•„ ìƒ ì†Œì•¡ì•” ë¦¬ìŠ¤í¬ê°€ ë†’ìŒ"
  }
}
```

#### 3. Compliance & Audit API

**POST /api/v1/compliance/validate-script**

Validate sales script for compliance.

```json
Request:
{
  "script": "ì´ ìƒí’ˆì€ ê°‘ìƒì„ ì•”ì„ 100% ë³´ì¥í•©ë‹ˆë‹¤!",
  "context": {
    "product_id": "prod_67890",
    "fp_id": "fp_123"
  }
}

Response:
{
  "validation_id": "val_444",
  "is_compliant": false,
  "violations": [
    {
      "type": "forbidden_phrase",
      "severity": "critical",
      "found_phrase": "100% ë³´ì¥í•©ë‹ˆë‹¤",
      "reason": "ì ˆëŒ€ì  ë‹¨ì–¸ í‘œí˜„ ê¸ˆì§€",
      "suggestion": "'ì•½ê´€ì— ë”°ë¼ ë³´ì¥ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤'ë¡œ ìˆ˜ì •"
    },
    {
      "type": "missing_disclaimer",
      "severity": "high",
      "required_phrase": "ë©´ì±…ê¸°ê°„ 90ì¼",
      "reason": "í•„ìˆ˜ ì„¤ëª… ì˜ë¬´ ëˆ„ë½"
    }
  ],
  "corrected_script": "ì´ ìƒí’ˆì€ ì•½ê´€ ì œ10ì¡°ì— ë”°ë¼ ê°‘ìƒì„ ì•”ì„ ë³´ì¥í•˜ë©°, ê³„ì•½ì¼ë¡œë¶€í„° 90ì¼ ë©´ì±…ê¸°ê°„ì´ ì ìš©ë©ë‹ˆë‹¤. ìì„¸í•œ ì‚¬í•­ì€ ì•½ê´€ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
  "risk_score": 85  // 0-100 (higher = more risky)
}
```

**GET /api/v1/audit/logs**

Retrieve audit logs for compliance reporting.

```json
Request:
GET /api/v1/audit/logs?start_date=2025-11-01&end_date=2025-11-30&fp_id=fp_123

Response:
{
  "logs": [
    {
      "log_id": "log_555",
      "timestamp": "2025-11-25T10:45:23Z",
      "fp_id": "fp_123",
      "action": "query_executed",
      "details": {
        "query": "ê°‘ìƒì„ ì•” ë³´ì¥ë¼ìš”?",
        "products_accessed": ["prod_67890"],
        "customer_id": "cust_111"
      },
      "ip_address": "192.168.1.100",
      "user_agent": "Mozilla/5.0..."
    }
  ],
  "pagination": {
    "total": 1247,
    "page": 1,
    "per_page": 50
  }
}
```

#### 4. MyData Integration API (Phase 2)

**POST /api/v1/mydata/import**

Import customer's existing policies via MyData.

```json
Request:
{
  "customer_id": "cust_111",
  "mydata_token": "eyJhbGci...",  // OAuth token from MyData provider
  "consent_id": "consent_666"
}

Response:
{
  "import_id": "imp_777",
  "status": "completed",
  "policies_imported": 3,
  "policies": [
    {
      "external_id": "md_policy_001",
      "insurer": "Samsung Life",
      "product_name": "Cancer Insurance",
      "start_date": "2015-06-01",
      "matched_product_id": "prod_67890",  // matched to our knowledge base
      "confidence": 0.95
    }
  ],
  "warnings": [
    {
      "policy": "md_policy_002",
      "issue": "Product not found in knowledge base",
      "action": "Manual review required"
    }
  ]
}
```

### API Authentication & Authorization

**JWT Token Structure**:

```json
{
  "sub": "fp_123",
  "role": "financial_planner",
  "ga_id": "ga_456",
  "tier": "pro",  // free, pro, enterprise
  "permissions": [
    "query:execute",
    "policies:read",
    "customers:manage"
  ],
  "rate_limits": {
    "queries_per_day": 1000,
    "ingestion_per_month": 50
  },
  "iat": 1700900000,
  "exp": 1700986400
}
```

**Role-Based Access Control (RBAC)**:

| Role | Permissions |
|------|-------------|
| `financial_planner` | Query execution, customer management, basic analytics |
| `ga_manager` | All FP permissions + team analytics, compliance monitoring |
| `admin` | All permissions + ingestion, system configuration |
| `end_user` (Phase 3) | Self-service policy analysis (read-only) |

---

## ğŸ—„ï¸ Database Architecture

### Neo4j Graph Schema (Detailed)

#### Node Labels & Properties

```cypher
// ============================================
// Core Business Entities
// ============================================

(:Product {
  id: STRING (PRIMARY),
  name: STRING,
  insurer: STRING,
  product_code: STRING,
  launch_date: DATE,
  version: STRING,
  status: STRING,  // 'active', 'deprecated', 'replaced'
  pdf_url: STRING,
  created_at: DATETIME,
  updated_at: DATETIME
})
CREATE INDEX product_id FOR (p:Product) ON (p.id)
CREATE INDEX product_insurer FOR (p:Product) ON (p.insurer)

(:Coverage {
  id: STRING (PRIMARY),
  name: STRING,
  code: STRING,
  type: STRING,  // 'cancer', 'cardiovascular', 'disability', 'death'
  category: STRING,  // 'life', 'health', 'annuity'
  base_amount: INTEGER,
  max_amount: INTEGER,
  min_amount: INTEGER,
  payment_type: STRING,  // 'lump_sum', 'installment', 'proportional'
  created_at: DATETIME
})
CREATE INDEX coverage_id FOR (c:Coverage) ON (c.id)
CREATE INDEX coverage_type FOR (c:Coverage) ON (c.type)

(:Disease {
  id: STRING (PRIMARY),
  kcd_code: STRING,  // Korean Classification of Disease
  kcd_version: STRING,  // 'KCD-8', 'KCD-9'
  name_ko: STRING,
  name_en: STRING,
  severity_level: STRING,  // 'minor', 'general', 'critical'
  category: STRING,  // 'cancer', 'cardiovascular', 'neurological'
  synonyms: LIST<STRING>,
  created_at: DATETIME
})
CREATE INDEX disease_kcd FOR (d:Disease) ON (d.kcd_code)
CREATE FULLTEXT INDEX disease_search FOR (d:Disease) ON EACH [d.name_ko, d.name_en, d.synonyms]

(:Condition {
  id: STRING (PRIMARY),
  type: STRING,  // 'waiting_period', 'reduction_period', 'age_limit', 'diagnosis_requirement'
  days: INTEGER,
  percentage: FLOAT,
  min_age: INTEGER,
  max_age: INTEGER,
  description: STRING,
  trigger_event: STRING
})

(:Clause {
  id: STRING (PRIMARY),
  product_id: STRING (FK),
  article_num: STRING,  // "ì œ10ì¡°"
  paragraph: STRING,    // "â‘ í•­"
  subclause: STRING,    // "ê°€ëª©"
  raw_text: STRING,
  summary: STRING,  // LLM-generated
  page_num: INTEGER,
  parent_clause_id: STRING,  // For hierarchical structure
  created_at: DATETIME
})
CREATE INDEX clause_product FOR (c:Clause) ON (c.product_id)
CREATE FULLTEXT INDEX clause_text FOR (c:Clause) ON EACH [c.raw_text, c.summary]

(:Exclusion {
  id: STRING (PRIMARY),
  type: STRING,  // 'disease', 'activity', 'pre_existing', 'intentional'
  description: STRING,
  priority: INTEGER,  // For conflict resolution
  effective_date: DATE,
  expiry_date: DATE
})

(:PaymentRule {
  id: STRING (PRIMARY),
  condition_type: STRING,  // 'duplicate_coverage', 'multiple_claims'
  formula: STRING,  // "MIN(actual_cost, coverage_amount)"
  proportional_ratio: FLOAT,
  max_payout: INTEGER,
  description: STRING
})

// ============================================
// Metadata & Audit Entities
// ============================================

(:Entity {
  id: STRING (PRIMARY),
  text: STRING,
  standard_form: STRING,  // Ontology-mapped term
  entity_type: STRING,  // 'disease', 'treatment', 'condition'
  confidence: FLOAT,
  source_clause_id: STRING
})

// ============================================
// Vector Embeddings (Neo4j Vector Index)
// ============================================

// Embeddings stored as node properties, indexed by vector index
ALTER TABLE Clause ADD PROPERTY embedding VECTOR(1536);  // For OpenAI ada-002
CREATE VECTOR INDEX clause_embeddings FOR (c:Clause) ON (c.embedding)
  OPTIONS {indexConfig: {
    `vector.dimensions`: 1536,
    `vector.similarity_function`: 'cosine'
  }}
```

#### Relationship Types & Properties

```cypher
// ============================================
// Core Relationships
// ============================================

(Product)-[:HAS_COVERAGE {
  order: INTEGER,
  is_optional: BOOLEAN,
  premium_rate: FLOAT
}]->(Coverage)

(Coverage)-[:COVERS {
  confidence: FLOAT,  // LLM extraction confidence
  extraction_method: STRING,  // 'rule_based', 'llm_extracted'
  verified_by_expert: BOOLEAN,
  verified_date: DATE
}]->(Disease)

(Coverage)-[:EXCLUDES {
  priority: INTEGER,
  override_covers: BOOLEAN,  // True if exclusion overrides coverage
  effective_period: STRING
}]->(Disease)

(Coverage)-[:REQUIRES {
  order: INTEGER,  // Sequence of conditions
  is_mandatory: BOOLEAN
}]->(Condition)

(Coverage)-[:APPLIES_RULE]->(PaymentRule)

// ============================================
// Conflict & Overlap Detection (Key Differentiator)
// ============================================

(Coverage)-[:CONFLICTS_WITH {
  conflict_type: STRING,  // 'duplicate', 'proportional', 'exclusive'
  overlap_percentage: FLOAT,
  resolution_rule: STRING,
  detected_date: DATE
}]->(Coverage)

(Product)-[:COMPETES_WITH {
  similarity_score: FLOAT,
  comparison_criteria: LIST<STRING>
}]->(Product)

// ============================================
// Provenance & Traceability (Critical for Trust)
// ============================================

(Coverage)-[:DEFINED_IN {
  is_primary_definition: BOOLEAN
}]->(Clause)

(Condition)-[:REFERENCES]->(Clause)

(Exclusion)-[:BASED_ON]->(Clause)

(Disease)-[:MENTIONED_IN]->(Clause)

// ============================================
// Temporal Relationships (Version Control)
// ============================================

(Product)-[:REPLACES {
  replaced_date: DATE,
  reason: STRING,  // 'regulation_change', 'product_update'
  migration_path: STRING
}]->(Product)

(Clause)-[:AMENDED_BY {
  amendment_date: DATE,
  change_summary: STRING
}]->(Clause)

// ============================================
// Reasoning & Inference Relationships
// ============================================

(Disease)-[:SUBTYPE_OF]->(Disease)  // e.g., ê°‘ìƒì„  ë¦¼í”„ì ˆ ì „ì´ -> ê°‘ìƒì„ ì•”

(Condition)-[:DEPENDS_ON]->(Condition)  // Sequential conditions

(Clause)-[:RELATED_TO {
  relation_type: STRING,  // 'exception', 'clarification', 'cross_reference'
  strength: FLOAT
}]->(Clause)
```

### PostgreSQL Metadata Schema

```sql
-- ============================================
-- User Management
-- ============================================

CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  email VARCHAR(255) UNIQUE NOT NULL,
  password_hash VARCHAR(255) NOT NULL,
  role VARCHAR(50) NOT NULL,  -- 'fp', 'ga_manager', 'admin', 'end_user'
  tier VARCHAR(50) NOT NULL DEFAULT 'free',
  ga_id UUID REFERENCES gas(id),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  last_login TIMESTAMP,
  is_active BOOLEAN DEFAULT TRUE
);

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_ga_id ON users(ga_id);

-- ============================================
-- GA (General Agency) Organizations
-- ============================================

CREATE TABLE gas (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  business_number VARCHAR(50) UNIQUE,
  contract_type VARCHAR(50),  -- 'free', 'pro', 'enterprise'
  max_fps INTEGER,
  created_at TIMESTAMP DEFAULT NOW(),
  contract_start DATE,
  contract_end DATE
);

-- ============================================
-- Customers (PII Masked)
-- ============================================

CREATE TABLE customers (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  fp_id UUID REFERENCES users(id),
  name_encrypted BYTEA,  -- AES-256 encrypted
  birth_year INTEGER,  -- Only year, not full DOB
  gender CHAR(1),
  phone_hash VARCHAR(64),  -- SHA-256 hashed
  consent_date TIMESTAMP,
  consent_id VARCHAR(100),
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_customers_fp_id ON customers(fp_id);

-- ============================================
-- Query History & Analytics
-- ============================================

CREATE TABLE query_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  customer_id UUID REFERENCES customers(id),
  query_text TEXT,
  query_type VARCHAR(50),  -- 'simple', 'comparison', 'gap_analysis'
  graph_query TEXT,  -- Cypher query executed
  result_confidence FLOAT,
  execution_time_ms INTEGER,
  ip_address INET,
  user_agent TEXT,
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_query_logs_user_id ON query_logs(user_id);
CREATE INDEX idx_query_logs_created_at ON query_logs(created_at);

-- ============================================
-- Ingestion Jobs
-- ============================================

CREATE TABLE ingestion_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  file_name VARCHAR(255),
  file_size BIGINT,
  file_url TEXT,  -- S3 URL
  status VARCHAR(50),  -- 'pending', 'processing', 'completed', 'failed'
  progress INTEGER DEFAULT 0,
  error_message TEXT,
  metadata JSONB,
  results JSONB,  -- {nodes_created, edges_created, etc.}
  created_at TIMESTAMP DEFAULT NOW(),
  started_at TIMESTAMP,
  completed_at TIMESTAMP
);

CREATE INDEX idx_ingestion_jobs_user_id ON ingestion_jobs(user_id);
CREATE INDEX idx_ingestion_jobs_status ON ingestion_jobs(status);

-- ============================================
-- Expert Review Queue (Phase 1 MVP)
-- ============================================

CREATE TABLE expert_reviews (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  query_log_id UUID REFERENCES query_logs(id),
  llm_answer TEXT,
  graph_paths JSONB,
  confidence FLOAT,
  status VARCHAR(50),  -- 'pending', 'approved', 'rejected'
  reviewer_id UUID REFERENCES users(id),
  review_notes TEXT,
  correct_answer TEXT,
  created_at TIMESTAMP DEFAULT NOW(),
  reviewed_at TIMESTAMP
);

CREATE INDEX idx_expert_reviews_status ON expert_reviews(status);

-- ============================================
-- Audit Logs (Compliance)
-- ============================================

CREATE TABLE audit_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  action VARCHAR(100),
  resource_type VARCHAR(50),
  resource_id VARCHAR(255),
  details JSONB,
  ip_address INET,
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_created_at ON audit_logs(created_at);
CREATE INDEX idx_audit_logs_action ON audit_logs(action);
```

---

## ğŸ”„ Data Ingestion Pipeline Architecture

### Pipeline Stages (LangGraph Orchestration)

```python
# LangGraph State Definition
from typing import TypedDict, List, Annotated
from langgraph.graph import StateGraph, END

class IngestionState(TypedDict):
    job_id: str
    file_path: str
    metadata: dict

    # Stage 1: OCR
    ocr_text: str
    ocr_confidence: float

    # Stage 2: Structure Parsing
    parsed_chunks: List[dict]
    document_hierarchy: dict

    # Stage 3: Critical Data Extraction (Rule-based)
    critical_data: List[dict]

    # Stage 4: Relationship Extraction (LLM)
    extracted_relations: List[dict]

    # Stage 5: Entity Linking
    standardized_entities: List[dict]

    # Stage 6: Graph Construction
    neo4j_nodes: List[dict]
    neo4j_edges: List[dict]

    # Stage 7: Validation
    validation_results: dict
    errors: List[str]
    warnings: List[str]
```

### Stage 1: OCR & Document Preprocessing

```python
class OCRAgent:
    """
    Upstage Document Parse integration
    """
    def __init__(self):
        self.client = UpstageDocumentParse(api_key=UPSTAGE_API_KEY)

    async def process(self, state: IngestionState) -> IngestionState:
        """
        Extract text, tables, and structure from PDF
        """
        result = await self.client.parse(
            file_path=state['file_path'],
            options={
                'ocr_lang': 'ko',
                'extract_tables': True,
                'extract_images': True,
                'layout_analysis': True
            }
        )

        state['ocr_text'] = result.text
        state['ocr_confidence'] = result.confidence
        state['parsed_chunks'] = result.chunks  # Pre-chunked by layout

        return state
```

### Stage 2: Legal Structure Parsing (Rule-based)

```python
class LegalStructureParser:
    """
    Parse Korean legal document structure
    """
    PATTERNS = {
        'article': r'ì œ(\d+)ì¡°\s*\[([^\]]+)\]',  # ì œ10ì¡° [ë³´í—˜ê¸ˆ ì§€ê¸‰]
        'paragraph': r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',
        'subclause': r'(\d+)\.\s',
        'exception': r'(ë‹¤ë§Œ|ë‹¨ì„œ|ì œì™¸í•˜ê³ )',
    }

    def parse(self, chunks: List[dict]) -> dict:
        """
        Build hierarchical tree of clauses
        """
        hierarchy = {
            'articles': []
        }

        current_article = None
        current_paragraph = None

        for chunk in chunks:
            text = chunk['text']

            # Match article
            if match := re.search(self.PATTERNS['article'], text):
                article_num = match.group(1)
                article_title = match.group(2)

                current_article = {
                    'article_num': f'ì œ{article_num}ì¡°',
                    'title': article_title,
                    'paragraphs': [],
                    'page': chunk['page'],
                    'bbox': chunk['bbox']
                }
                hierarchy['articles'].append(current_article)

            # Match paragraph
            elif match := re.search(self.PATTERNS['paragraph'], text):
                if current_article:
                    paragraph = {
                        'paragraph_num': match.group(0),
                        'text': text[match.end():].strip(),
                        'subclauses': []
                    }
                    current_article['paragraphs'].append(paragraph)
                    current_paragraph = paragraph

        return hierarchy
```

### Stage 3: Critical Data Extraction (Rule-based)

```python
class CriticalDataExtractor:
    """
    Extract critical data with 100% accuracy requirement
    """
    def extract_amounts(self, text: str) -> List[int]:
        """
        Extract monetary amounts: 1ì–µì›, 100ë§Œì› -> normalized integers
        """
        patterns = [
            (r'(\d+(?:,\d+)?)\s*ì–µ\s*ì›', 100_000_000),
            (r'(\d+(?:,\d+)?)\s*ë§Œ\s*ì›', 10_000),
            (r'(\d+(?:,\d+)?)\s*ì²œ\s*ì›', 1_000),
            (r'(\d+(?:,\d+)?)\s*ì›', 1),
        ]

        amounts = []
        for pattern, multiplier in patterns:
            for match in re.finditer(pattern, text):
                num_str = match.group(1).replace(',', '')
                amount = int(num_str) * multiplier
                amounts.append({
                    'value': amount,
                    'original_text': match.group(0),
                    'position': match.span()
                })

        return amounts

    def extract_periods(self, text: str) -> List[dict]:
        """
        Extract time periods: 90ì¼, 3ê°œì›” -> normalized to days
        """
        patterns = [
            (r'(\d+)\s*ì¼', 1),
            (r'(\d+)\s*ê°œì›”', 30),  # Approximate
            (r'(\d+)\s*ë…„', 365),
        ]

        periods = []
        for pattern, multiplier in patterns:
            for match in re.finditer(pattern, text):
                num = int(match.group(1))
                days = num * multiplier
                periods.append({
                    'days': days,
                    'original_text': match.group(0),
                    'position': match.span()
                })

        return periods

    def extract_kcd_codes(self, text: str) -> List[str]:
        """
        Extract KCD disease codes: C77, I21-I25
        """
        pattern = r'\b([A-Z]\d{2}(?:-[A-Z]?\d{2})?)\b'
        return re.findall(pattern, text)
```

### Stage 4: Relationship Extraction (LLM)

```python
class RelationExtractionAgent:
    """
    LLM-based relationship extraction with validation
    """
    PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì•½ê´€ ì¡°í•­ì—ì„œ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

[ì•½ê´€ ì¡°í•­]
{clause_text}

[ì¶”ì¶œëœ Critical Data]
ê¸ˆì•¡: {amounts}
ê¸°ê°„: {periods}
ì§ˆë³‘ì½”ë“œ: {kcd_codes}

[ì§€ì¹¨]
1. ì£¼ì²´(Subject): ì–´ë–¤ ë‹´ë³´/ìƒí’ˆ?
2. í–‰ìœ„(Action): COVERS, EXCLUDES, REQUIRES, REDUCES ì¤‘ ì„ íƒ
3. ê°ì²´(Object): ì–´ë–¤ ì§ˆë³‘/ìƒí™©?
4. ì¡°ê±´(Conditions): ë©´ì±…ê¸°ê°„, ê°ì•¡ë¹„ìœ¨ ë“±

[ì¤‘ìš”] Critical Dataê°€ ì œê³µë˜ì—ˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ê°’ì„ ì‚¬ìš©í•˜ì„¸ìš”. ìƒˆë¡œìš´ ìˆ«ìë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ - JSON]
{{
  "relations": [
    {{
      "subject": "ì•”ì§„ë‹¨íŠ¹ì•½",
      "action": "EXCLUDES",
      "object": "ê°‘ìƒì„ ì•”(C77)",
      "conditions": [
        {{"type": "waiting_period", "days": 90}}
      ],
      "confidence": 0.95,
      "reasoning": "ì œ10ì¡° â‘ í•­ì—ì„œ ëª…ì‹œ"
    }}
  ]
}}
"""

    async def extract(self, chunk: dict, critical_data: dict) -> List[dict]:
        """
        Extract relationships with LLM + validation
        """
        prompt = self.PROMPT_TEMPLATE.format(
            clause_text=chunk['text'],
            amounts=critical_data.get('amounts', []),
            periods=critical_data.get('periods', []),
            kcd_codes=critical_data.get('kcd_codes', [])
        )

        # Try Solar Pro first (cost-effective)
        response = await self.solar_llm.generate(prompt)
        relations = json.loads(response)

        # Validate: Check if LLM's numbers match critical_data
        validated_relations = []
        for rel in relations['relations']:
            is_valid, corrected_rel = self.validate_relation(rel, critical_data)

            if not is_valid:
                # Low confidence, retry with GPT-4o
                if rel['confidence'] < 0.7:
                    gpt4_response = await self.gpt4_llm.generate(prompt)
                    rel = json.loads(gpt4_response)['relations'][0]

            validated_relations.append(corrected_rel)

        return validated_relations

    def validate_relation(self, relation: dict, critical_data: dict) -> tuple:
        """
        Validate LLM output against rule-based critical data
        """
        is_valid = True

        # Check if LLM's period matches critical_data
        for condition in relation.get('conditions', []):
            if condition['type'] == 'waiting_period':
                llm_days = condition['days']

                # Find matching period in critical_data
                extracted_periods = [p['days'] for p in critical_data.get('periods', [])]

                if llm_days not in extracted_periods:
                    # Override with rule-based value
                    if extracted_periods:
                        condition['days'] = extracted_periods[0]
                        is_valid = False

        return is_valid, relation
```

### Stage 5: Entity Linking & Ontology Mapping

```python
class EntityLinker:
    """
    Standardize entities to ontology
    """
    ONTOLOGY = {
        'diseases': {
            'ì•…ì„±ì‹ ìƒë¬¼': {'standard': 'Cancer', 'kcd_prefix': 'C'},
            'ì•”': {'standard': 'Cancer', 'kcd_prefix': 'C'},
            'ê°‘ìƒì„ ì•”': {'standard': 'ThyroidCancer', 'kcd_code': 'C77'},
            'ë‡Œì¶œí˜ˆ': {'standard': 'CerebralHemorrhage', 'kcd_code': 'I61'},
            # ...
        }
    }

    def link_entities(self, relations: List[dict]) -> List[dict]:
        """
        Map entities to standard ontology
        """
        for relation in relations:
            # Standardize disease object
            obj = relation['object']

            for disease_term, mapping in self.ONTOLOGY['diseases'].items():
                if disease_term in obj:
                    relation['object_standard'] = mapping['standard']
                    relation['kcd_code'] = mapping.get('kcd_code')
                    break

        return relations
```

### Stage 6: Neo4j Graph Construction

```python
class GraphConstructor:
    """
    Build Neo4j graph from extracted relations
    """
    def __init__(self):
        self.driver = neo4j.GraphDatabase.driver(
            NEO4J_URI,
            auth=(NEO4J_USER, NEO4J_PASSWORD)
        )

    async def construct_graph(self, state: IngestionState) -> IngestionState:
        """
        Create nodes and relationships in Neo4j
        """
        with self.driver.session() as session:
            # Create Product node
            product_id = self.create_product(session, state['metadata'])

            # Create Clause nodes
            clause_mapping = {}
            for article in state['document_hierarchy']['articles']:
                clause_id = self.create_clause(session, product_id, article)
                clause_mapping[article['article_num']] = clause_id

            # Create Coverage, Disease, Condition nodes + relationships
            for relation in state['standardized_entities']:
                self.create_relation_graph(session, relation, clause_mapping)

            state['neo4j_nodes'] = session.run("MATCH (n) RETURN count(n)").single()[0]
            state['neo4j_edges'] = session.run("MATCH ()-[r]->() RETURN count(r)").single()[0]

        return state

    def create_relation_graph(self, session, relation: dict, clause_mapping: dict):
        """
        Create coverage-disease relationship with provenance
        """
        query = """
        // Create or match Coverage
        MERGE (cov:Coverage {name: $coverage_name})
        ON CREATE SET cov.id = randomUUID(), cov.created_at = datetime()

        // Create or match Disease
        MERGE (dis:Disease {kcd_code: $kcd_code})
        ON CREATE SET dis.id = randomUUID(),
                      dis.name_ko = $disease_name,
                      dis.created_at = datetime()

        // Create relationship
        MERGE (cov)-[r:COVERS]->(dis)
        SET r.confidence = $confidence,
            r.extraction_method = 'llm_extracted'

        // Link to source Clause (provenance!)
        WITH cov
        MATCH (clause:Clause {id: $clause_id})
        MERGE (cov)-[:DEFINED_IN]->(clause)
        """

        session.run(
            query,
            coverage_name=relation['subject'],
            kcd_code=relation['kcd_code'],
            disease_name=relation['object'],
            confidence=relation['confidence'],
            clause_id=clause_mapping.get(relation['article_ref'])
        )
```

### LangGraph Pipeline Orchestration

```python
def create_ingestion_pipeline() -> StateGraph:
    """
    Orchestrate ingestion stages with LangGraph
    """
    workflow = StateGraph(IngestionState)

    # Add nodes
    workflow.add_node("ocr", OCRAgent().process)
    workflow.add_node("parse_structure", LegalStructureParser().parse)
    workflow.add_node("extract_critical", CriticalDataExtractor().extract)
    workflow.add_node("extract_relations", RelationExtractionAgent().extract)
    workflow.add_node("link_entities", EntityLinker().link_entities)
    workflow.add_node("construct_graph", GraphConstructor().construct_graph)
    workflow.add_node("validate", ValidationAgent().validate)

    # Define edges
    workflow.add_edge("ocr", "parse_structure")
    workflow.add_edge("parse_structure", "extract_critical")
    workflow.add_edge("extract_critical", "extract_relations")
    workflow.add_edge("extract_relations", "link_entities")
    workflow.add_edge("link_entities", "construct_graph")
    workflow.add_edge("construct_graph", "validate")
    workflow.add_edge("validate", END)

    # Set entry point
    workflow.set_entry_point("ocr")

    return workflow.compile()

# Usage
pipeline = create_ingestion_pipeline()
result = await pipeline.ainvoke({
    'job_id': 'job_123',
    'file_path': '/tmp/policy.pdf',
    'metadata': {...}
})
```

---

## ğŸ” GraphRAG Query Engine Architecture

### Query Processing Flow

```
User Query (NL)
    â†“
[Query Classification]
    â”œâ”€ Simple Fact â†’ Vector Search + 1-hop Graph
    â”œâ”€ Complex Reasoning â†’ Multi-hop Graph Traversal + LLM
    â””â”€ Comparison â†’ Pre-computed CONFLICTS_WITH + Analysis
    â†“
[Hybrid Retrieval]
    â”œâ”€ Vector Search (Neo4j Vector Index)
    â””â”€ Graph Traversal (Cypher)
    â†“
[LLM Reasoning Layer]
    â”œâ”€ Context: Graph paths + Source clauses
    â””â”€ Model: Solar Pro â†’ GPT-4o (cascade)
    â†“
[4-Stage Validation]
    â”œâ”€ Source attachment check
    â”œâ”€ Confidence thresholding
    â”œâ”€ Forbidden phrase filtering
    â””â”€ Expert review queue (if needed)
    â†“
[Response Formatting]
```

### Query Classification

```python
class QueryClassifier:
    """
    Classify query type for optimal strategy
    """
    PATTERNS = {
        'simple_coverage': [
            r'(ë³´ì¥|ë‹´ë³´).*ë¼',
            r'ë‚˜ì™€ìš”',
            r'ì§€ê¸‰.*ë˜ë‚˜ìš”'
        ],
        'comparison': [
            r'(ë¹„êµ|ì°¨ì´)',
            r'ì¤‘ë³µ.*ë³´ì¥',
            r'ì–´ëŠ.*ì¢‹ì•„ìš”'
        ],
        'temporal': [
            r'\d{4}ë…„.*ê°€ì…',
            r'ì˜ˆì „.*ì•½ê´€',
            r'ê°œì •.*ì „'
        ],
        'gap_analysis': [
            r'ë¶€ì¡±í•œ.*ë³´ì¥',
            r'ì¶”ê°€.*í•„ìš”',
            r'ê³µë°±'
        ]
    }

    def classify(self, query: str) -> str:
        """
        Classify query into strategy type
        """
        for query_type, patterns in self.PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, query):
                    return query_type

        return 'general'
```

### Hybrid Retrieval Strategy

```python
class HybridRetriever:
    """
    Combine vector search + graph traversal
    """
    async def retrieve(self, query: str, options: dict) -> dict:
        """
        Execute hybrid retrieval
        """
        # Step 1: Vector search for relevant clauses
        vector_results = await self.vector_search(query, top_k=10)

        # Step 2: Extract coverage/disease entities from top results
        relevant_entities = self.extract_entities(vector_results)

        # Step 3: Graph traversal from entities
        graph_paths = await self.graph_traversal(
            relevant_entities,
            max_hops=options.get('max_hops', 3)
        )

        # Step 4: Merge and rank results
        combined = self.merge_results(vector_results, graph_paths)

        return combined

    async def vector_search(self, query: str, top_k: int) -> List[dict]:
        """
        Neo4j vector search on clause embeddings
        """
        # Generate query embedding
        query_embedding = await self.embedder.embed(query)

        # Search Neo4j vector index
        cypher = """
        CALL db.index.vector.queryNodes('clause_embeddings', $top_k, $query_embedding)
        YIELD node, score
        RETURN node.id AS clause_id,
               node.raw_text AS text,
               node.article_num AS article,
               score
        ORDER BY score DESC
        """

        with self.driver.session() as session:
            results = session.run(cypher, top_k=top_k, query_embedding=query_embedding)
            return [dict(record) for record in results]

    async def graph_traversal(self, entities: dict, max_hops: int) -> List[dict]:
        """
        Multi-hop graph traversal
        """
        cypher = """
        // Start from Coverage entities
        MATCH (cov:Coverage)
        WHERE cov.name IN $coverage_names

        // Traverse to Disease
        MATCH path = (cov)-[r:COVERS|EXCLUDES*1..{max_hops}]->(d:Disease)
        WHERE d.kcd_code IN $kcd_codes OR d.name_ko IN $disease_names

        // Optional: Get Conditions
        OPTIONAL MATCH (cov)-[:REQUIRES]->(cond:Condition)

        // Optional: Get source Clause (provenance)
        OPTIONAL MATCH (cov)-[:DEFINED_IN]->(clause:Clause)

        RETURN path, cond, clause
        LIMIT 50
        """.format(max_hops=max_hops)

        with self.driver.session() as session:
            results = session.run(
                cypher,
                coverage_names=entities.get('coverages', []),
                kcd_codes=entities.get('kcd_codes', []),
                disease_names=entities.get('diseases', [])
            )
            return [dict(record) for record in results]
```

### LLM Reasoning Layer

```python
class ReasoningAgent:
    """
    LLM-based reasoning over graph results
    """
    REASONING_PROMPT = """
ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ê·¸ë˜í”„ ë¶„ì„ ê²°ê³¼]
{graph_context}

[ì›ë¬¸ ì¡°í•­]
{source_clauses}

[ì§€ì¹¨]
1. ë°˜ë“œì‹œ ì œê³µëœ ì›ë¬¸ ì¡°í•­ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì•½ê´€ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ "í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
3. ì ˆëŒ€ì  ë‹¨ì–¸("100% ë³´ì¥", "ë¬´ì¡°ê±´")ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
4. "ì•½ê´€ ì œXì¡°ì— ë”°ë¥´ë©´"ê³¼ ê°™ì€ í‘œí˜„ì„ í¬í•¨í•˜ì„¸ìš”

[ì¶œë ¥ í˜•ì‹]
{{
  "summary": "2-3ë¬¸ì¥ ìš”ì•½",
  "details": [
    {{
      "coverage": "ë‹´ë³´ëª…",
      "is_covered": true/false,
      "conditions": [],
      "reasoning": "íŒë‹¨ ê·¼ê±°"
    }}
  ],
  "confidence": 0.0-1.0,
  "sources": ["clause_id_1", "clause_id_2"]
}}
"""

    async def reason(self, query: str, graph_results: dict) -> dict:
        """
        Generate answer with reasoning
        """
        # Format graph results for LLM
        graph_context = self.format_graph_context(graph_results)
        source_clauses = self.format_source_clauses(graph_results)

        prompt = self.REASONING_PROMPT.format(
            query=query,
            graph_context=graph_context,
            source_clauses=source_clauses
        )

        # Cascade: Solar Pro â†’ GPT-4o if low confidence
        response = await self.solar_llm.generate(prompt)
        answer = json.loads(response)

        if answer['confidence'] < 0.7:
            response = await self.gpt4_llm.generate(prompt)
            answer = json.loads(response)

        return answer
```

### 4-Stage Validation Pipeline

```python
class AnswerValidator:
    """
    4-stage validation to prevent hallucination
    """
    def validate(self, answer: dict, graph_results: dict) -> dict:
        """
        Run all validation stages
        """
        # Stage 1: Source attachment check
        if not self.check_sources(answer, graph_results):
            return self.reject_no_source()

        # Stage 2: Confidence thresholding
        status = self.check_confidence(answer['confidence'])
        if status == 'reject':
            return self.reject_low_confidence()

        # Stage 3: Forbidden phrase filtering
        violations = self.check_forbidden_phrases(answer['summary'])
        if violations:
            return self.reject_forbidden_phrases(violations)

        # Stage 4: Expert review queue (if medium confidence)
        if status == 'expert_review':
            self.add_to_review_queue(answer, graph_results)

        return {
            'status': status,
            'answer': answer,
            'warnings': self.generate_warnings(status)
        }

    def check_sources(self, answer: dict, graph_results: dict) -> bool:
        """
        Ensure all claims have source clauses
        """
        referenced_clauses = set(answer.get('sources', []))
        available_clauses = set(c['clause_id'] for c in graph_results.get('source_clauses', []))

        return referenced_clauses.issubset(available_clauses)

    FORBIDDEN_PHRASES = [
        '100% ë³´ì¥',
        'ë¬´ì¡°ê±´',
        'ì ˆëŒ€',
        'í™•ì‹¤íˆ',
        'ë‹¹ì—°íˆ',
    ]

    def check_forbidden_phrases(self, text: str) -> List[str]:
        """
        Detect forbidden phrases
        """
        violations = []
        for phrase in self.FORBIDDEN_PHRASES:
            if phrase in text:
                violations.append(phrase)
        return violations
```

---

## ğŸ¢ Infrastructure & Deployment Architecture

### AWS Architecture (Financial Compliance)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Internet                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CloudFront (CDN) + WAF                          â”‚
â”‚  - DDoS protection  - Geo-blocking  - SSL/TLS termination   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  VPC (Logically Isolated)                    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚           Public Subnet (NAT Gateway)                    â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚           Private Subnet - Application Tier              â”‚â”‚
â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚
â”‚ â”‚  â”‚   EKS Pod    â”‚  â”‚   EKS Pod    â”‚  â”‚   EKS Pod    â”‚  â”‚â”‚
â”‚ â”‚  â”‚  (FastAPI)   â”‚  â”‚  (Worker)    â”‚  â”‚  (Worker)    â”‚  â”‚â”‚
â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚           Private Subnet - Data Tier                     â”‚â”‚
â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚
â”‚ â”‚  â”‚   Neo4j      â”‚  â”‚  PostgreSQL  â”‚  â”‚   Redis      â”‚  â”‚â”‚
â”‚ â”‚  â”‚   (RDS/EC2)  â”‚  â”‚   (RDS)      â”‚  â”‚ (ElastiCache)â”‚  â”‚â”‚
â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              External Services (Outside VPC)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚      S3      â”‚  â”‚   Upstage    â”‚  â”‚   OpenAI     â”‚      â”‚
â”‚  â”‚  (Policies)  â”‚  â”‚  (LLM/OCR)   â”‚  â”‚   (GPT-4o)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EKS (Kubernetes) Deployment

```yaml
# kubernetes/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: insuregraph-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: insuregraph-api
  template:
    metadata:
      labels:
        app: insuregraph-api
    spec:
      containers:
      - name: fastapi
        image: insuregraph/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: NEO4J_URI
          valueFrom:
            secretKeyRef:
              name: neo4j-credentials
              key: uri
        - name: POSTGRES_URI
          valueFrom:
            secretKeyRef:
              name: postgres-credentials
              key: uri
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: insuregraph-api-service
spec:
  type: LoadBalancer
  selector:
    app: insuregraph-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: insuregraph-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: insuregraph-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Background Workers (Celery)

```yaml
# kubernetes/worker-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: insuregraph-worker
spec:
  replicas: 5
  selector:
    matchLabels:
      app: insuregraph-worker
  template:
    metadata:
      labels:
        app: insuregraph-worker
    spec:
      containers:
      - name: celery-worker
        image: insuregraph/worker:latest
        command: ["celery", "-A", "app.celery_app", "worker", "--loglevel=info", "--concurrency=4"]
        env:
        - name: CELERY_BROKER_URL
          value: "redis://redis-service:6379/0"
        - name: CELERY_RESULT_BACKEND
          value: "redis://redis-service:6379/1"
        resources:
          requests:
            memory: "1Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "4000m"
```

---

## ğŸ”’ Security & Compliance Architecture

### Data Flow & Compliance Boundaries

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PUBLIC ZONE (ì™¸ë¶€ ì ‘ê·¼)                     â”‚
â”‚  - CloudFront CDN                                            â”‚
â”‚  - WAF (Web Application Firewall)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTPS Only
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DMZ (Kong API Gateway)                          â”‚
â”‚  - JWT Validation                                            â”‚
â”‚  - Rate Limiting                                             â”‚
â”‚  - Request Logging (Audit)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Internal TLS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          APPLICATION ZONE (ë…¼ë¦¬ì  ë§ë¶„ë¦¬)                    â”‚
â”‚  - FastAPI Application (Private Subnet)                      â”‚
â”‚  - No direct internet access                                 â”‚
â”‚  - All external API calls via NAT Gateway                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Encrypted Connection
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA ZONE (ìµœê³  ë³´ì•ˆ ìˆ˜ì¤€)                      â”‚
â”‚  - Neo4j (Encrypted at rest + in transit)                    â”‚
â”‚  - PostgreSQL (TDE enabled)                                  â”‚
â”‚  - PII Encryption (AES-256)                                  â”‚
â”‚  - No direct external access                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PII (Personal Identifiable Information) Protection

```python
# Security module for PII handling

from cryptography.fernet import Fernet
import hashlib

class PIIProtector:
    """
    Encrypt/decrypt PII data
    """
    def __init__(self, encryption_key: bytes):
        self.cipher = Fernet(encryption_key)

    def encrypt_name(self, name: str) -> bytes:
        """
        Encrypt customer name (AES-256)
        """
        return self.cipher.encrypt(name.encode('utf-8'))

    def decrypt_name(self, encrypted_name: bytes) -> str:
        """
        Decrypt customer name (only for authorized access)
        """
        return self.cipher.decrypt(encrypted_name).decode('utf-8')

    @staticmethod
    def hash_phone(phone: str) -> str:
        """
        One-way hash for phone number (for deduplication)
        """
        return hashlib.sha256(phone.encode('utf-8')).hexdigest()

    @staticmethod
    def mask_birth_date(birth_date: str) -> dict:
        """
        Extract only year, discard month/day
        """
        year = birth_date.split('-')[0]
        return {
            'birth_year': int(year),
            'original_masked': True
        }

# PostgreSQL PII storage
INSERT INTO customers (
    name_encrypted,  -- AES-256 encrypted
    birth_year,      -- Only year, not full date
    phone_hash       -- SHA-256 hashed
) VALUES (
    %s, %s, %s
);
```

### Audit Logging

```python
class AuditLogger:
    """
    Comprehensive audit logging for compliance
    """
    def log_query_access(self, user_id: str, query: str, customer_id: str):
        """
        Log all query accesses for audit trail
        """
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'user_id': user_id,
            'action': 'query_executed',
            'resource_type': 'customer_data',
            'resource_id': customer_id,
            'query_text': self.sanitize_query(query),
            'ip_address': request.remote_addr,
            'user_agent': request.headers.get('User-Agent')
        }

        # Write to PostgreSQL audit_logs table
        db.execute(
            "INSERT INTO audit_logs (...) VALUES (...)",
            log_entry
        )

        # Also send to CloudWatch for real-time monitoring
        cloudwatch.put_log_events(
            logGroupName='/insuregraph/audit',
            logStreamName=f'{user_id}/{datetime.now().date()}',
            logEvents=[{
                'timestamp': int(datetime.now().timestamp() * 1000),
                'message': json.dumps(log_entry)
            }]
        )
```

### Financial Sandbox Compliance Checklist

| Requirement | Implementation | Status |
|-------------|---------------|--------|
| **ë…¼ë¦¬ì  ë§ë¶„ë¦¬** | Private VPC subnets, no direct internet access | âœ… Phase 1 |
| **PII ì•”í˜¸í™”** | AES-256 encryption at rest, TLS in transit | âœ… Phase 1 |
| **ì ‘ê·¼ ë¡œê·¸ ê¸°ë¡** | All API calls logged to audit_logs table | âœ… Phase 1 |
| **ê¶Œí•œ ê´€ë¦¬** | RBAC with JWT, role-based access control | âœ… Phase 1 |
| **ë°ì´í„° ìµœì†Œí™”** | Only birth year stored, phone hashed | âœ… Phase 1 |
| **ë³´ì•ˆ ì·¨ì•½ì  ì ê²€** | Monthly penetration testing | ğŸ”„ Phase 2 |
| **ì‚¬ê³  ëŒ€ì‘ ê³„íš** | Incident response playbook | ğŸ”„ Phase 2 |

---

## ğŸ“Š Performance & Scalability

### Performance Targets

| Metric | Phase 1 (MVP) | Phase 2 (Commercial) | Phase 3 (Scale) |
|--------|---------------|----------------------|-----------------|
| **Simple Query Latency** | < 500ms | < 300ms | < 200ms |
| **Complex Query Latency** | < 3s | < 2s | < 1.5s |
| **Ingestion Speed** | 50 pages/min | 100 pages/min | 200 pages/min |
| **Concurrent Users** | 100 | 1,000 | 10,000 |
| **Policy Knowledge Base** | 50 products | 200 products | 500+ products |
| **Graph Size** | ~50K nodes | ~200K nodes | ~500K nodes |

### Caching Strategy

```python
class QueryCache:
    """
    Multi-layer caching for performance
    """
    def __init__(self):
        self.redis = Redis(host='redis-service', port=6379)
        self.local_cache = {}  # In-memory LRU cache

    async def get_or_compute(self, query: str, compute_fn: callable) -> dict:
        """
        3-tier cache: Memory â†’ Redis â†’ Compute
        """
        cache_key = self.generate_cache_key(query)

        # Layer 1: Local memory cache (fastest)
        if cache_key in self.local_cache:
            return self.local_cache[cache_key]

        # Layer 2: Redis cache (fast)
        cached = self.redis.get(cache_key)
        if cached:
            result = json.loads(cached)
            self.local_cache[cache_key] = result
            return result

        # Layer 3: Compute (slow)
        result = await compute_fn()

        # Store in both caches
        self.redis.setex(cache_key, 3600, json.dumps(result))  # 1 hour TTL
        self.local_cache[cache_key] = result

        return result

    def invalidate_product(self, product_id: str):
        """
        Invalidate all caches for a product (e.g., after update)
        """
        pattern = f"query:*:product:{product_id}:*"
        for key in self.redis.scan_iter(match=pattern):
            self.redis.delete(key)
```

### Database Indexing Strategy

```cypher
// Neo4j Indexes for optimal query performance

// Primary key indexes (already defined in schema)
CREATE INDEX product_id FOR (p:Product) ON (p.id);
CREATE INDEX coverage_id FOR (c:Coverage) ON (c.id);
CREATE INDEX disease_kcd FOR (d:Disease) ON (d.kcd_code);

// Composite indexes for common queries
CREATE INDEX coverage_product FOR (c:Coverage) ON (c.product_id, c.type);
CREATE INDEX clause_product_article FOR (c:Clause) ON (c.product_id, c.article_num);

// Full-text search indexes
CREATE FULLTEXT INDEX disease_search FOR (d:Disease) ON EACH [d.name_ko, d.name_en, d.synonyms];
CREATE FULLTEXT INDEX clause_text FOR (c:Clause) ON EACH [c.raw_text, c.summary];

// Vector index for semantic search
CREATE VECTOR INDEX clause_embeddings FOR (c:Clause) ON (c.embedding)
  OPTIONS {indexConfig: {
    `vector.dimensions`: 1536,
    `vector.similarity_function`: 'cosine'
  }};
```

### Monitoring & Observability

```python
# Prometheus metrics

from prometheus_client import Counter, Histogram, Gauge

# API Metrics
api_requests_total = Counter(
    'insuregraph_api_requests_total',
    'Total API requests',
    ['method', 'endpoint', 'status_code']
)

api_request_duration = Histogram(
    'insuregraph_api_request_duration_seconds',
    'API request duration',
    ['method', 'endpoint']
)

# Query Metrics
query_execution_time = Histogram(
    'insuregraph_query_execution_seconds',
    'GraphRAG query execution time',
    ['query_type']
)

graph_traversal_hops = Histogram(
    'insuregraph_graph_hops',
    'Number of graph hops per query',
    ['query_type']
)

# LLM Metrics
llm_token_usage = Counter(
    'insuregraph_llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'operation']
)

llm_confidence_score = Histogram(
    'insuregraph_llm_confidence',
    'LLM confidence scores',
    ['model']
)

# Database Metrics
neo4j_nodes_count = Gauge(
    'insuregraph_neo4j_nodes_total',
    'Total nodes in Neo4j'
)

neo4j_relationships_count = Gauge(
    'insuregraph_neo4j_relationships_total',
    'Total relationships in Neo4j'
)
```

---

## ğŸ¯ Technology Decisions & Rationale

### Decision Log

#### Decision 1: Neo4j Vector Index vs. Dedicated Vector DB

**Context**: Need both graph traversal and vector search.

**Options Considered**:
- A) Neo4j Vector Index (unified)
- B) Neo4j + Pinecone (separate)

**Decision**: **A) Neo4j Vector Index** for Phase 1, migrate to B if performance issues.

**Rationale**:
- âœ… Reduced latency (no inter-service calls)
- âœ… Simpler architecture
- âœ… Lower operational cost
- âš ï¸ Risk: Performance at scale (mitigated by benchmarking)

---

#### Decision 2: Upstage Solar Pro vs. GPT-4o

**Context**: LLM for relation extraction and reasoning.

**Decision**: **Cascade strategy** - Solar Pro primary, GPT-4o fallback.

**Rationale**:
- âœ… Cost-effective (Solar Pro ~30% cheaper)
- âœ… Korean language specialization
- âœ… Table/form recognition superior
- âœ… GPT-4o backup ensures quality

---

#### Decision 3: LangGraph vs. Custom Orchestration

**Context**: Multi-agent workflow orchestration.

**Decision**: **LangGraph**

**Rationale**:
- âœ… Built-in state management
- âœ… Easy to visualize and debug
- âœ… Active community and support
- âœ… Integrates well with LangChain ecosystem

---

#### Decision 4: AWS vs. GCP vs. Azure

**Context**: Cloud infrastructure provider.

**Decision**: **AWS**

**Rationale**:
- âœ… Best support for Neo4j (AWS Marketplace)
- âœ… Strong financial compliance tools (CloudHSM, KMS)
- âœ… Team expertise
- âœ… Korean region availability (ap-northeast-2)

---

## ğŸ“ Project Structure

```
insuregraph-pro/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ compliance.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mydata.py
â”‚   â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ocr_agent.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ extractor.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ graph_constructor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ classifier.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ reasoner.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ validator.py
â”‚   â”‚   â”‚   â””â”€â”€ compliance/
â”‚   â”‚   â”‚       â””â”€â”€ script_validator.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_models.py
â”‚   â”‚   â”‚   â””â”€â”€ db_models.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ vpc.tf
â”‚   â”‚   â”œâ”€â”€ eks.tf
â”‚   â”‚   â”œâ”€â”€ rds.tf
â”‚   â”‚   â””â”€â”€ s3.tf
â”‚   â””â”€â”€ kubernetes/
â”‚       â”œâ”€â”€ deployment.yaml
â”‚       â”œâ”€â”€ service.yaml
â”‚       â””â”€â”€ ingress.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ prd.md
â”‚   â”œâ”€â”€ graphrag-implementation-strategy.md
â”‚   â””â”€â”€ architecture.md  â† This document
â””â”€â”€ README.md
```

---

## ğŸš€ Next Steps

1. **Review & Approval**: Architect â†’ CTO â†’ PM
2. **Create Epic & Stories**: Break down into implementable user stories
3. **Setup Development Environment**:
   - Provision Neo4j instance
   - Setup FastAPI boilerplate
   - Configure LLM API keys
4. **Prototype Core Pipeline**: Implement one complete ingestion flow (PDF â†’ Graph)
5. **Benchmark Performance**: Validate query latency assumptions

---

**Document Status**: âœ… Draft Complete â†’ Pending Review
**Next Reviewer**: CTO / Tech Lead
**Estimated Review Time**: 2-3 days
