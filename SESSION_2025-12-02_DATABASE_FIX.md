# Session Summary: Database Connection Fix (2025-12-02)

## Problem Solved

Fixed the server startup error that was preventing the document crawler feature from working.

### Original Error
```
File "/Users/gangseungsig/Documents/02_GitHub/12_InsureGraph Pro/backend/app/services/document_crawler_service.py", line 11, in <module>
from app.core.database import get_pg_connection
```

Server would not start due to incorrect database connection usage pattern.

## Root Cause

The code was trying to use `get_pg_connection()` as if it returned a direct database connection, but it's actually a **generator function** designed for FastAPI dependency injection with synchronous psycopg2.

Additionally, the code was using `await conn.fetch()` which is an **asyncpg pattern**, but `get_pg_connection()` returns a **psycopg2 connection** (synchronous).

## Solution

Refactored the code to use **SQLAlchemy's AsyncSession** with proper FastAPI dependency injection pattern.

## Files Modified

### 1. `backend/app/services/document_crawler_service.py`

**Changes:**
- Added SQLAlchemy imports: `AsyncSession`, `text`
- Removed import of `get_pg_connection`
- Modified `__init__` to accept `db: AsyncSession` parameter
- Updated `_get_crawler_urls()` to use SQLAlchemy `text()` queries
- Updated `save_crawled_documents()` to use SQLAlchemy with commit

**Before:**
```python
from app.core.database import get_pg_connection

class DocumentCrawlerService:
    def __init__(self):
        self.pdf_extractor = AIPdfExtractor()

    async def _get_crawler_urls(self, insurer: str) -> List[str]:
        conn = get_pg_connection()
        rows = await conn.fetch(...)
```

**After:**
```python
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

class DocumentCrawlerService:
    def __init__(self, db: AsyncSession):
        self.pdf_extractor = AIPdfExtractor()
        self.db = db

    async def _get_crawler_urls(self, insurer: str) -> List[str]:
        query = text("""
            SELECT url
            FROM crawler_urls
            WHERE insurer = :insurer AND enabled = true
            ORDER BY created_at DESC
        """)
        result = await self.db.execute(query, {"insurer": insurer})
        rows = result.fetchall()
        urls = [row.url for row in rows]
```

### 2. `backend/app/api/v1/endpoints/crawler_documents.py`

**Changes:**
- Added imports: `Depends`, `AsyncSession`, `text`
- Added import of `get_db` from database module
- Updated POST `/crawl-documents` endpoint to inject `db: AsyncSession = Depends(get_db)`
- Pass db session to `DocumentCrawlerService(db=db)`
- Updated GET `/documents` endpoint to use SQLAlchemy queries

**Before:**
```python
@router.post("/crawl-documents", response_model=CrawlResultResponse)
async def crawl_documents(
    background_tasks: BackgroundTasks,
    insurer: str = Query(..., description="ë³´í—˜ì‚¬ëª…")
):
    crawler_service = DocumentCrawlerService()
    ...

@router.get("/documents", response_model=DocumentListResponse)
async def list_crawler_documents(...):
    from app.core.database import get_pg_connection
    conn = get_pg_connection()
    rows = await conn.fetch(query, ...)
```

**After:**
```python
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text
from app.core.database import get_db

@router.post("/crawl-documents", response_model=CrawlResultResponse)
async def crawl_documents(
    background_tasks: BackgroundTasks,
    insurer: str = Query(..., description="ë³´í—˜ì‚¬ëª…"),
    db: AsyncSession = Depends(get_db)
):
    crawler_service = DocumentCrawlerService(db=db)
    ...

@router.get("/documents", response_model=DocumentListResponse)
async def list_crawler_documents(
    ...,
    db: AsyncSession = Depends(get_db)
):
    query = text(f"""
        SELECT ...
        FROM crawler_documents
        WHERE {where_clause}
        LIMIT :limit OFFSET :offset
    """)
    result = await db.execute(query, params)
    rows = result.fetchall()
```

## Verification

### Server Status
âœ… Server starts successfully on port 3030
âœ… No import errors
âœ… All modules load correctly

### API Endpoints Registered
```bash
$ curl -s http://localhost:3030/openapi.json | python3 -m json.tool | grep "/api/v1/crawler"
```

Confirmed endpoints:
- âœ… `POST /api/v1/crawler/crawl-documents` - Trigger document crawling
- âœ… `GET /api/v1/crawler/documents` - List crawled documents
- âœ… `POST /api/v1/crawler/urls` - Manage crawler URLs
- âœ… `GET /api/v1/crawler/urls` - List crawler URLs
- âœ… `PUT /api/v1/crawler/urls/{url_id}` - Update crawler URL
- âœ… `DELETE /api/v1/crawler/urls/{url_id}` - Delete crawler URL

### Test Data
Added MetLife crawler URL for testing:
```sql
INSERT INTO crawler_urls (insurer, url, description, enabled) VALUES
('ë©”íŠ¸ë¼ì´í”„ìƒëª…', 'https://brand.metlife.co.kr/pn/mcvrgProd/retrieveMcvrgProdMain.do', 'ì•½ê´€ì •ë³´', true);
```

## Key Learnings

### FastAPI Database Patterns

**âŒ Incorrect Pattern:**
```python
# Module-level connection call
from app.core.database import get_pg_connection
conn = get_pg_connection()  # This is a generator!

# Then trying to use await
rows = await conn.fetch(...)  # Won't work!
```

**âœ… Correct Pattern:**
```python
# Use dependency injection
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession
from app.core.database import get_db

@router.post("/endpoint")
async def my_endpoint(db: AsyncSession = Depends(get_db)):
    result = await db.execute(text("SELECT ..."), params)
    rows = result.fetchall()
```

### SQLAlchemy vs AsyncPG Patterns

**AsyncPG style** (not used in this project):
```python
rows = await conn.fetch("SELECT * FROM table WHERE id = $1", id)
value = rows[0]['column_name']
```

**SQLAlchemy style** (used in this project):
```python
from sqlalchemy import text

query = text("SELECT * FROM table WHERE id = :id")
result = await db.execute(query, {"id": id})
rows = result.fetchall()
value = rows[0].column_name  # Access via attribute, not dict
```

## Next Steps

1. âœ… Server startup error - **FIXED**
2. â³ Test crawler functionality with frontend "ë¬¸ì„œ ì—…ë°ì´íŠ¸" button
3. â³ Verify PDF links are extracted and classified correctly
4. â³ Connect frontend document list to real crawler_documents data
5. â³ Implement learned/unlearned document sorting

## Architecture Overview

```
Frontend (Next.js)
    â†“
[ë¬¸ì„œ ì—…ë°ì´íŠ¸ ë²„íŠ¼] clicks
    â†“
POST /api/v1/crawler/crawl-documents?insurer=ë©”íŠ¸ë¼ì´í”„ìƒëª…
    â†“
DocumentCrawlerService (with SQLAlchemy AsyncSession)
    â†“
â”œâ”€â†’ Fetch crawler_urls from DB
â”œâ”€â†’ PlaywrightCrawler (crawl each URL)
â”‚   â””â”€â†’ Wait 3 seconds, extract HTML
â”œâ”€â†’ AIPdfExtractor (AI classification)
â”‚   â”œâ”€â†’ Parse HTML with BeautifulSoup
â”‚   â”œâ”€â†’ Filter PDF links
â”‚   â””â”€â†’ OpenAI GPT-4o-mini classification
â”‚       â””â”€â†’ Returns: {title, category, product_type, is_relevant}
â””â”€â†’ Save to crawler_documents table
    â””â”€â†’ ON CONFLICT (pdf_url) DO UPDATE

Frontend displays: "Nê°œ ë¬¸ì„œ ë°œê²¬"
```

## Database Schema

### `crawler_documents` table
- `id` (UUID PK)
- `insurer` (ë³´í—˜ì‚¬ëª…)
- `title` (ë¬¸ì„œ ì œëª©)
- `pdf_url` (UNIQUE - PDF URL)
- `category` (ì•½ê´€/íŠ¹ì•½)
- `product_type` (ì¢…ì‹ ë³´í—˜, ì •ê¸°ë³´í—˜, etc.)
- `source_url` (ì›ë³¸ í˜ì´ì§€ URL)
- `status` (pending/downloaded/processed/failed)
- `file_path` (ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ)
- `error_message` (ì—ëŸ¬ ë©”ì‹œì§€)
- `metadata` (JSONB)
- `created_at`, `updated_at`

**Indexes:**
- `insurer`, `status`, `category`, `product_type`, `created_at DESC`

## Status

âœ… **Server startup issue resolved**
âœ… **Database connection pattern fixed**
âœ… **All endpoints registered and accessible**
ğŸ¯ **Ready for functional testing**

---

**Session End Time:** 2025-12-02 10:46 KST
**Next Session:** Test crawler functionality and frontend integration
